import os
import copy
import json
import importlib

import numpy as np
import pandas as pd

from cvf_analysis import CVFAnalysis, logger


class LinearRegressionFullAnalysis(CVFAnalysis):

    @property
    def results_dir(self):
        return os.path.join("results", "linear_regression")

    @property
    def results_prefix(self):
        return f"linear_regression__{self.min_slope}_{self.max_slope}__{self.slope_step}__{self.ds_matrix_config_id}"

    def __init__(self, graph_name, graph) -> None:
        super().__init__(graph_name, graph)
        self.no_of_nodes = 8

        self.nodes = list(range(self.no_of_nodes))

        self.iterations = 100
        self.cache = {"p": {}, "q": {}, "r": {}}

        # self.learning_rate = 0.001
        # self.slope_step_decimals = 1
        # self.min_slope = np.float64(0.0)
        # self.max_slope = np.float64(1.1)
        # self.no_of_nodes = 3
        # self.df = pd.read_csv(
        #     "/home/agaru/research/cvf-python/linear_regression/random-data.csv"
        # )
        # self.doubly_stochastic_matrix_config = [
        #     [1 / 3, 1 / 3, 1 / 3],
        #     [1 / 3, 2 / 3, 0],
        #     [1 / 3, 0, 2 / 3],
        # ]
        # self.actual_m = 0.9
        # self.actual_b = -0.11847322643445737

        # importlib.import_module(f"lr_configs.{graph_name}")

        self.learning_rate = 0.0001
        self.stop_threshold = 0.0001
        self.slope_step = np.float64(0.025)
        self.slope_step_decimals = 3
        self.min_slope = np.float64(1.000)
        self.max_slope = np.float64(1.900)
        # self.no_of_nodes = 4
        self.df = pd.read_csv(
            os.path.join(
                os.getenv("CVF_CODE_ROOT", "/"),
                "linear_regression",
                "SOCR-HeightWeight.csv",
            )
        )  # /home/agaru/research/cvf-python/
        self.df.rename(
            columns={"Height(Inches)": "X", "Weight(Pounds)": "y"}, inplace=True
        )
        self.df.drop("Index", axis=1, inplace=True)
        # self.ds_matrix_config_id = 1
        # self.doubly_stochastic_matrix_config = [
        #     [1 / 2, 1 / 4, 1 / 8, 1 / 8],
        #     [1 / 4, 3 / 4, 0, 0],
        #     [1 / 8, 0, 7 / 8, 0],
        #     [1 / 8, 0, 0, 7 / 8],
        # ]

        # self.ds_matrix_config_id = 2
        # self.doubly_stochastic_matrix_config = [
        #     [5 / 8, 1 / 8, 1 / 8, 1 / 8],
        #     [1 / 8, 7 / 8, 0, 0],
        #     [1 / 8, 0, 7 / 8, 0],
        #     [1 / 8, 0, 0, 7 / 8],
        # ]

        # self.ds_matrix_config_id = 3
        # self.doubly_stochastic_matrix_config = [
        #     [1 / 4, 1 / 2, 1 / 8, 1 / 8],
        #     [1 / 2, 1 / 2, 0, 0],
        #     [1 / 8, 0, 7 / 8, 0],
        #     [1 / 8, 0, 0, 7 / 8],
        # ]

        self.ds_matrix_config_id = 4
        self.doubly_stochastic_matrix_config = [
            [1 / 8, 1 / 8, 1 / 8, 1 / 8, 1 / 8, 1 / 8, 1 / 8, 1 / 8],
            [1 / 8, 7 / 8, 0, 0, 0, 0, 0, 0],
            [1 / 8, 0, 7 / 8, 0, 0, 0, 0, 0],
            [1 / 8, 0, 0, 7 / 8, 0, 0, 0, 0],
            [1 / 8, 0, 0, 0, 7 / 8, 0, 0, 0],
            [1 / 8, 0, 0, 0, 0, 7 / 8, 0, 0],
            [1 / 8, 0, 0, 0, 0, 0, 7 / 8, 0],
            [1 / 8, 0, 0, 0, 0, 0, 0, 7 / 8],
        ]

        # self.slope_step = 1 / (10**self.slope_step_decimals)
        self.df = self.df.sample(frac=1, random_state=36).reset_index(drop=True)
        self.node_data_partitions = np.array_split(self.df, self.no_of_nodes)
        for i, node_data in enumerate(self.node_data_partitions):
            self.df.loc[node_data.index, "node"] = i

        # self.df["partition"] = -1
        # for i in range(self.no_of_nodes):
        #     node_filter = self.df["node"] == i
        #     node_df = self.df[node_filter]
        #     partitions = self.__gen_test_data_partition_frm_df(
        #         self.no_of_nodes, node_df
        #     )
        #     for i, p in enumerate(partitions):
        #         self.df.loc[self.df.index.isin(p.index.values), "partition"] = i

        self._preprocessing()

    def _preprocessing(self):
        self.df["X_2"] = self.df["X"].apply(lambda x: np.square(x))
        self.df["Xy"] = self.df[["X", "y"]].apply(lambda row: row.X * row.y, axis=1)

    # def __gen_test_data_partition_frm_df(self, partitions, df):
    #     shuffled = df.sample(frac=1)
    #     result = np.array_split(shuffled, partitions)
    #     return result

    def _start(self):
        self._gen_configurations()
        # self._find_invariants()
        self._find_program_transitions_n_cvfs()
        self._init_pts_rank()
        self.__save_pts_to_file()
        self._rank_all_states()
        self._gen_save_rank_count()
        self._calculate_pts_rank_effect()
        self._calculate_cvfs_rank_effect()
        self._gen_save_rank_effect_count()
        self._gen_save_rank_effect_by_node_count()

    def _gen_configurations(self):
        self.configurations = {tuple([self.min_slope for _ in range(self.no_of_nodes)])}
        # perturb each state at a time for all states in configurations and accumulate the same in the configurations for next state to perturb
        for node_pos in range(self.no_of_nodes):
            config_copy = copy.deepcopy(self.configurations)
            for i in np.round(
                np.arange(
                    self.min_slope + self.slope_step,
                    self.max_slope + self.slope_step,
                    self.slope_step,
                ),
                self.slope_step_decimals,
            ):
                for cc in config_copy:
                    cc = list(cc)
                    cc[node_pos] = i
                    self.configurations.add(tuple(cc))

        logger.info("No. of Configurations: %s", len(self.configurations))

    def _find_invariants(self):
        logger.info("No. of Invariants: %s", len(self.invariants))

    # def __forward(self, X, params):
    #     return params["m"] * X + params["c"]

    # def __loss_fn(self, y, y_pred):
    #     N = len(y)
    #     return (1 / N) * sum((y[i] - y_pred[i]) ** 2 for i in range(N))

    def __get_f(self, state, node_id):
        doubly_st_mt = self.doubly_stochastic_matrix_config[node_id]
        return sum(frac * state[j] for j, frac in enumerate(doubly_st_mt))

    def __get_p(self, node_id):
        if node_id in self.cache["p"]:
            return self.cache["p"][node_id]

        df = self.__get_node_data_df(node_id)
        N = len(df)
        result = -2 / N
        self.cache["p"][node_id] = result
        return result

    def __get_q(self, node_id):
        if node_id in self.cache["q"]:
            return self.cache["q"][node_id]

        df = self.__get_node_data_df(node_id)
        result = np.sum(df["Xy"])
        self.cache["q"][node_id] = result
        return result

    def __get_r(self, node_id):
        if node_id in self.cache["r"]:
            return self.cache["r"][node_id]

        df = self.__get_node_data_df(node_id)
        result = np.sum(df["X_2"])
        self.cache["r"][node_id] = result
        return result

    # def __gradient_m(self, X, y, y_pred):
    #     N = len(y)
    #     return (-2 / N) * np.sum(X * (y - y_pred))

    def __get_node_data_df(self, node_id):
        return self.df[self.df["node"] == node_id]
        # return self.df[self.df["node"] == 0]

    # def __get_node_test_data_df(self, node_id):
    #     return self.df[self.df["node"] == node_id]

    def __clean_float_to_step_size_single(self, slope):
        quotient = np.divide(slope, self.slope_step)
        if quotient == int(quotient):
            return np.round(slope, self.slope_step_decimals)
        return np.round(np.int64(quotient) * self.slope_step, self.slope_step_decimals)

    def __copy_replace_indx_value(self, lst, indx, value):
        lst_copy = lst.copy()
        lst_copy[indx] = value
        return lst_copy

    def _get_program_transitions(self, start_state):
        program_transitions = set()
        node_params = list(start_state)

        for node_id in range(self.no_of_nodes):
            for i in range(1, self.iterations + 1):
                prev_m = node_params[node_id]

                start_state_cpy = list(start_state)
                start_state_cpy[node_id] = prev_m

                # node_df = self.__get_node_data_df(node_id)
                # X_node = node_df["X"].array
                # y_node = node_df["y"].array

                # y_node_pred = self.__forward(X_node, {"m": prev_m, "c": 0})
                # grad_m = self.__gradient_m(X_node, y_node, y_node_pred)

                # doubly_st_mt = self.doubly_stochastic_matrix_config[node_id]

                # new_slope = (
                #     sum(
                #         frac * start_state_cpy[j]
                #         for j, frac in enumerate(doubly_st_mt)
                #     )
                #     - self.learning_rate * grad_m
                # )

                new_slope = self.__get_f(
                    start_state_cpy, node_id
                ) - self.learning_rate * self.__get_p(node_id) * (
                    self.__get_q(node_id) - prev_m * self.__get_r(node_id)
                )

                # if new_slope < self.min_slope:
                #     new_slope = self.min_slope

                if new_slope > self.max_slope:
                    new_slope = self.max_slope

                node_params[node_id] = new_slope

                if abs(prev_m - new_slope) <= self.stop_threshold:
                    break
            else:
                logger.debug(
                    "Couldn't converge node %s for the state %s", node_id, start_state
                )

        for node_id, new_slope in enumerate(node_params):
            new_slope_cleaned = self.__clean_float_to_step_size_single(new_slope)
            if new_slope_cleaned != start_state[node_id]:
                new_node_params = self.__copy_replace_indx_value(
                    list(start_state), node_id, new_slope_cleaned
                )
                new_node_params = tuple(new_node_params)
                program_transitions.add(new_node_params)

        if not program_transitions:
            self.invariants.add(start_state)
            logger.debug("No program transition found for %s !", start_state)

        return program_transitions

    def _get_cvfs(self, start_state):
        cvfs = dict()
        all_slope_values = set(
            np.round(
                np.arange(
                    self.min_slope,
                    self.max_slope + self.slope_step,
                    self.slope_step,
                ),
                self.slope_step_decimals,
            )
        )
        for position, val in enumerate(start_state):
            possible_slope_values = all_slope_values - {val}
            for perturb_val in possible_slope_values:
                perturb_state = list(start_state)
                perturb_state[position] = perturb_val
                perturb_state = tuple(perturb_state)
                cvfs[perturb_state] = (
                    position  # track the nodes to calculate its overall rank effect
                )

        return cvfs

    def __save_pts_to_file(self):
        def _map_key(state):
            return json.dumps([float(k) for k in state])

        pts = {
            _map_key(state): list(pts["program_transitions"])
            for state, pts in self.pts_n_cvfs.items()
        }

        json.dump(pts, open("output.json", "w"))
