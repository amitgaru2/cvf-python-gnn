{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import CVFConfigForGCNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_graph_6 = CVFConfigForGCNDataset(\n",
    "    device,\n",
    "    \"graph_6_config_rank_dataset.csv\",\n",
    "    \"graph_6_edge_index.json\",\n",
    ")\n",
    "\n",
    "dataset_graph_7 = CVFConfigForGCNDataset(\n",
    "    device,\n",
    "    \"graph_7_config_rank_dataset.csv\",\n",
    "    \"graph_7_edge_index.json\",\n",
    ")\n",
    "\n",
    "dataset_graph_8 = CVFConfigForGCNDataset(\n",
    "    device,\n",
    "    \"graph_8_config_rank_dataset.csv\",\n",
    "    \"graph_8_edge_index.json\",\n",
    ")\n",
    "\n",
    "dataset_graph_10 = CVFConfigForGCNDataset(\n",
    "    device,\n",
    "    \"graph_10_config_rank_dataset.csv\",\n",
    "    \"graph_10_edge_index.json\",\n",
    ")\n",
    "\n",
    "dataset_rr_n7 = CVFConfigForGCNDataset(\n",
    "    device,\n",
    "    \"graph_random_regular_graph_n7_d4_config_rank_dataset.csv\",\n",
    "    \"graph_random_regular_graph_n7_d4_edge_index.json\",\n",
    ")\n",
    "\n",
    "dataset_plc_n8 = CVFConfigForGCNDataset(\n",
    "    device,\n",
    "    \"graph_powerlaw_cluster_graph_n8_config_rank_dataset.csv\",\n",
    "    \"graph_powerlaw_cluster_graph_n8_edge_index.json\",\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dataset_coll = [\n",
    "    dataset_graph_6,\n",
    "    dataset_graph_7,\n",
    "    dataset_graph_8,\n",
    "    dataset_graph_10,\n",
    "    dataset_rr_n7,\n",
    "    dataset_plc_n8,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_coll = []\n",
    "test_dataloader_coll = []\n",
    "\n",
    "for dataset in dataset_coll:\n",
    "    train_size = int(0.95 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    train_dataloader_coll.append(train_loader)\n",
    "    test_dataloader_coll.append(test_loader)\n",
    "\n",
    "train_dataloader_coll_iter = [iter(i) for i in train_dataloader_coll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch():\n",
    "    end_loop = [False for _ in range(len(train_dataloader_coll))]\n",
    "    while not any(end_loop):\n",
    "        for di, data_loader in enumerate(train_dataloader_coll_iter):\n",
    "            if end_loop[di]:\n",
    "                continue\n",
    "            try:\n",
    "                batch = next(data_loader)\n",
    "            except StopIteration:\n",
    "                end_loop[di] = True\n",
    "                continue\n",
    "            yield batch, di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: [72, 171, 266, 475, 1160, 2128]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of batches:\", [len(i) for i in train_dataloader_coll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaGNN(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNConv(dim_in, dim_h)\n",
    "        self.gcn2 = GCNConv(dim_h, dim_h)\n",
    "        self.out = torch.nn.Linear(dim_h, dim_out)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.gcn1(x, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = self.gcn2(h, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = self.out(h)\n",
    "        h = torch.relu(h)\n",
    "        h = global_mean_pool(h, torch.zeros(h.size(1)).to(device).long())\n",
    "        return h\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "        # edge_index = dataset.edge_index.t().to(device)\n",
    "        dataloaders = itertools.tee(generate_batch(), epochs)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for batch, di in dataloaders[epoch - 1]:\n",
    "                x = batch[0]\n",
    "                y = batch[1]\n",
    "                y = y.unsqueeze(-1)\n",
    "                optimizer.zero_grad()\n",
    "                out = self(x, dataset_coll[di].edge_index)\n",
    "                # print(out.shape, y.shape)\n",
    "                loss = criterion(out, y)\n",
    "                total_loss += loss\n",
    "                count += 1\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if count > 0:\n",
    "                print(\n",
    "                    \"Training set | Epoch\",\n",
    "                    epoch,\n",
    "                    \"| Loss:\",\n",
    "                    (total_loss / count).item(),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanillaGNN(\n",
      "  (gcn1): GCNConv(1, 64)\n",
      "  (gcn2): GCNConv(64, 64)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Training set | Epoch 1 | Loss: 1.9057844877243042\n",
      "Training set | Epoch 2 | Loss: 1.6398823261260986\n",
      "Training set | Epoch 3 | Loss: 1.6259151697158813\n",
      "Training set | Epoch 4 | Loss: 1.6197623014450073\n",
      "Training set | Epoch 5 | Loss: 1.6159372329711914\n",
      "Training set | Epoch 6 | Loss: 1.6134103536605835\n",
      "Training set | Epoch 7 | Loss: 1.610600233078003\n",
      "Training set | Epoch 8 | Loss: 1.5993735790252686\n",
      "Training set | Epoch 9 | Loss: 1.565748929977417\n",
      "Training set | Epoch 10 | Loss: 1.4700793027877808\n",
      "Training set | Epoch 11 | Loss: 1.321799635887146\n",
      "Training set | Epoch 12 | Loss: 1.2337960004806519\n",
      "Training set | Epoch 13 | Loss: 1.1675610542297363\n",
      "Training set | Epoch 14 | Loss: 1.1079288721084595\n",
      "Training set | Epoch 15 | Loss: 1.049405574798584\n",
      "Training set | Epoch 16 | Loss: 0.9942586421966553\n",
      "Training set | Epoch 17 | Loss: 0.9591251611709595\n",
      "Training set | Epoch 18 | Loss: 0.9150562286376953\n",
      "Training set | Epoch 19 | Loss: 0.892737627029419\n",
      "Training set | Epoch 20 | Loss: 0.8854740262031555\n",
      "Training set | Epoch 21 | Loss: 0.8564903140068054\n",
      "Training set | Epoch 22 | Loss: 0.8492783904075623\n",
      "Training set | Epoch 23 | Loss: 0.8384944200515747\n",
      "Training set | Epoch 24 | Loss: 0.8414528369903564\n",
      "Training set | Epoch 25 | Loss: 0.8290221691131592\n",
      "Training set | Epoch 26 | Loss: 0.8282464146614075\n",
      "Training set | Epoch 27 | Loss: 0.8239895701408386\n",
      "Training set | Epoch 28 | Loss: 0.8216436505317688\n",
      "Training set | Epoch 29 | Loss: 0.8171790838241577\n",
      "Training set | Epoch 30 | Loss: 0.8152450919151306\n",
      "Training set | Epoch 31 | Loss: 0.8161688446998596\n",
      "Training set | Epoch 32 | Loss: 0.8111199736595154\n",
      "Training set | Epoch 33 | Loss: 0.8070629239082336\n",
      "Training set | Epoch 34 | Loss: 0.804975688457489\n",
      "Training set | Epoch 35 | Loss: 0.8042294979095459\n",
      "Training set | Epoch 36 | Loss: 0.8051860332489014\n",
      "Training set | Epoch 37 | Loss: 0.800588846206665\n",
      "Training set | Epoch 38 | Loss: 0.7964795827865601\n",
      "Training set | Epoch 39 | Loss: 0.7942544221878052\n",
      "Training set | Epoch 40 | Loss: 0.7900888323783875\n",
      "Training set | Epoch 41 | Loss: 0.789265513420105\n",
      "Training set | Epoch 42 | Loss: 0.7884995341300964\n",
      "Training set | Epoch 43 | Loss: 0.7858878970146179\n",
      "Training set | Epoch 44 | Loss: 0.7844648361206055\n",
      "Training set | Epoch 45 | Loss: 0.7820122838020325\n",
      "Training set | Epoch 46 | Loss: 0.7793975472450256\n",
      "Training set | Epoch 47 | Loss: 0.7802547812461853\n",
      "Training set | Epoch 48 | Loss: 0.7770061492919922\n",
      "Training set | Epoch 49 | Loss: 0.7774230241775513\n",
      "Training set | Epoch 50 | Loss: 0.7764883637428284\n",
      "Training set | Epoch 51 | Loss: 0.7741179466247559\n",
      "Training set | Epoch 52 | Loss: 0.7738087773323059\n",
      "Training set | Epoch 53 | Loss: 0.7718389630317688\n",
      "Training set | Epoch 54 | Loss: 0.7693763375282288\n",
      "Training set | Epoch 55 | Loss: 0.7668402194976807\n",
      "Training set | Epoch 56 | Loss: 0.7689963579177856\n",
      "Training set | Epoch 57 | Loss: 0.7685462236404419\n",
      "Training set | Epoch 58 | Loss: 0.7663530111312866\n",
      "Training set | Epoch 59 | Loss: 0.7652132511138916\n",
      "Training set | Epoch 60 | Loss: 0.7672997117042542\n",
      "Training set | Epoch 61 | Loss: 0.7632924914360046\n",
      "Training set | Epoch 62 | Loss: 0.763022243976593\n",
      "Training set | Epoch 63 | Loss: 0.7618796825408936\n",
      "Training set | Epoch 64 | Loss: 0.7609723806381226\n",
      "Training set | Epoch 65 | Loss: 0.7594301700592041\n",
      "Training set | Epoch 66 | Loss: 0.7568610906600952\n",
      "Training set | Epoch 67 | Loss: 0.7547076940536499\n",
      "Training set | Epoch 68 | Loss: 0.751499354839325\n",
      "Training set | Epoch 69 | Loss: 0.7462142705917358\n",
      "Training set | Epoch 70 | Loss: 0.7482908964157104\n",
      "Training set | Epoch 71 | Loss: 0.7464203834533691\n",
      "Training set | Epoch 72 | Loss: 0.742540717124939\n",
      "Training set | Epoch 73 | Loss: 0.7397584915161133\n",
      "Training set | Epoch 74 | Loss: 0.7390206456184387\n",
      "Training set | Epoch 75 | Loss: 0.7346298694610596\n",
      "Training set | Epoch 76 | Loss: 0.7309625744819641\n",
      "Training set | Epoch 77 | Loss: 0.7338417768478394\n",
      "Training set | Epoch 78 | Loss: 0.7293917536735535\n",
      "Training set | Epoch 79 | Loss: 0.7309589385986328\n",
      "Training set | Epoch 80 | Loss: 0.7281378507614136\n",
      "Training set | Epoch 81 | Loss: 0.7244328260421753\n",
      "Training set | Epoch 82 | Loss: 0.7268659472465515\n",
      "Training set | Epoch 83 | Loss: 0.7236121892929077\n",
      "Training set | Epoch 84 | Loss: 0.7211825251579285\n",
      "Training set | Epoch 85 | Loss: 0.7205194234848022\n",
      "Training set | Epoch 86 | Loss: 0.7207443118095398\n",
      "Training set | Epoch 87 | Loss: 0.7177976369857788\n",
      "Training set | Epoch 88 | Loss: 0.7176183462142944\n",
      "Training set | Epoch 89 | Loss: 0.7202320694923401\n",
      "Training set | Epoch 90 | Loss: 0.7160947322845459\n",
      "Training set | Epoch 91 | Loss: 0.7214629650115967\n",
      "Training set | Epoch 92 | Loss: 0.7151824235916138\n",
      "Training set | Epoch 93 | Loss: 0.7159969806671143\n",
      "Training set | Epoch 94 | Loss: 0.7170517444610596\n",
      "Training set | Epoch 95 | Loss: 0.7132225632667542\n",
      "Training set | Epoch 96 | Loss: 0.7113608717918396\n",
      "Training set | Epoch 97 | Loss: 0.7115628123283386\n",
      "Training set | Epoch 98 | Loss: 0.7124359607696533\n",
      "Training set | Epoch 99 | Loss: 0.7121869921684265\n",
      "Training set | Epoch 100 | Loss: 0.7088196873664856\n",
      "Training set | Epoch 101 | Loss: 0.7100228071212769\n",
      "Training set | Epoch 102 | Loss: 0.7105515003204346\n",
      "Training set | Epoch 103 | Loss: 0.7080122232437134\n",
      "Training set | Epoch 104 | Loss: 0.7057349681854248\n",
      "Training set | Epoch 105 | Loss: 0.7022717595100403\n",
      "Training set | Epoch 106 | Loss: 0.7069386839866638\n",
      "Training set | Epoch 107 | Loss: 0.7055422067642212\n",
      "Training set | Epoch 108 | Loss: 0.7006688714027405\n",
      "Training set | Epoch 109 | Loss: 0.7019134759902954\n",
      "Training set | Epoch 110 | Loss: 0.7037609219551086\n",
      "Training set | Epoch 111 | Loss: 0.7022553086280823\n",
      "Training set | Epoch 112 | Loss: 0.7030774354934692\n",
      "Training set | Epoch 113 | Loss: 0.7039008736610413\n",
      "Training set | Epoch 114 | Loss: 0.7008130550384521\n",
      "Training set | Epoch 115 | Loss: 0.6984515190124512\n",
      "Training set | Epoch 116 | Loss: 0.7001208662986755\n",
      "Training set | Epoch 117 | Loss: 0.699449360370636\n",
      "Training set | Epoch 118 | Loss: 0.7018492221832275\n",
      "Training set | Epoch 119 | Loss: 0.6972315311431885\n",
      "Training set | Epoch 120 | Loss: 0.699533998966217\n",
      "Training set | Epoch 121 | Loss: 0.6949301958084106\n",
      "Training set | Epoch 122 | Loss: 0.6949695944786072\n",
      "Training set | Epoch 123 | Loss: 0.6994971036911011\n",
      "Training set | Epoch 124 | Loss: 0.6963691711425781\n",
      "Training set | Epoch 125 | Loss: 0.6953539252281189\n",
      "Training set | Epoch 126 | Loss: 0.6955571174621582\n",
      "Training set | Epoch 127 | Loss: 0.6972305178642273\n",
      "Training set | Epoch 128 | Loss: 0.6926865577697754\n",
      "Training set | Epoch 129 | Loss: 0.6974050998687744\n",
      "Training set | Epoch 130 | Loss: 0.6935665607452393\n",
      "Training set | Epoch 131 | Loss: 0.6928772926330566\n",
      "Training set | Epoch 132 | Loss: 0.6919007897377014\n",
      "Training set | Epoch 133 | Loss: 0.6942262649536133\n",
      "Training set | Epoch 134 | Loss: 0.6921430826187134\n",
      "Training set | Epoch 135 | Loss: 0.691256046295166\n",
      "Training set | Epoch 136 | Loss: 0.6908976435661316\n",
      "Training set | Epoch 137 | Loss: 0.691555917263031\n",
      "Training set | Epoch 138 | Loss: 0.6933534145355225\n",
      "Training set | Epoch 139 | Loss: 0.6924522519111633\n",
      "Training set | Epoch 140 | Loss: 0.6931179761886597\n",
      "Training set | Epoch 141 | Loss: 0.6921536326408386\n",
      "Training set | Epoch 142 | Loss: 0.6932215094566345\n",
      "Training set | Epoch 143 | Loss: 0.6888613104820251\n",
      "Training set | Epoch 144 | Loss: 0.6906862854957581\n",
      "Training set | Epoch 145 | Loss: 0.6907532215118408\n",
      "Training set | Epoch 146 | Loss: 0.6897605061531067\n",
      "Training set | Epoch 147 | Loss: 0.6903846263885498\n",
      "Training set | Epoch 148 | Loss: 0.6878509521484375\n",
      "Training set | Epoch 149 | Loss: 0.6903339624404907\n",
      "Training set | Epoch 150 | Loss: 0.690802276134491\n",
      "Training set | Epoch 151 | Loss: 0.6922597885131836\n",
      "Training set | Epoch 152 | Loss: 0.691391110420227\n",
      "Training set | Epoch 153 | Loss: 0.6899150609970093\n",
      "Training set | Epoch 154 | Loss: 0.6894391775131226\n",
      "Training set | Epoch 155 | Loss: 0.692108690738678\n",
      "Training set | Epoch 156 | Loss: 0.6880988478660583\n",
      "Training set | Epoch 157 | Loss: 0.6897983551025391\n",
      "Training set | Epoch 158 | Loss: 0.6880261898040771\n",
      "Training set | Epoch 159 | Loss: 0.688770592212677\n",
      "Training set | Epoch 160 | Loss: 0.6937332153320312\n",
      "Training set | Epoch 161 | Loss: 0.6898989081382751\n",
      "Training set | Epoch 162 | Loss: 0.688345193862915\n",
      "Training set | Epoch 163 | Loss: 0.6904280781745911\n",
      "Training set | Epoch 164 | Loss: 0.6884168386459351\n",
      "Training set | Epoch 165 | Loss: 0.6913005709648132\n",
      "Training set | Epoch 166 | Loss: 0.6869874000549316\n",
      "Training set | Epoch 167 | Loss: 0.6916675567626953\n",
      "Training set | Epoch 168 | Loss: 0.6927800178527832\n",
      "Training set | Epoch 169 | Loss: 0.6897814273834229\n",
      "Training set | Epoch 170 | Loss: 0.6879278421401978\n",
      "Training set | Epoch 171 | Loss: 0.6899135708808899\n",
      "Training set | Epoch 172 | Loss: 0.6847138404846191\n",
      "Training set | Epoch 173 | Loss: 0.6908595561981201\n",
      "Training set | Epoch 174 | Loss: 0.6864303946495056\n",
      "Training set | Epoch 175 | Loss: 0.6914980411529541\n",
      "Training set | Epoch 176 | Loss: 0.6898370385169983\n",
      "Training set | Epoch 177 | Loss: 0.685648500919342\n",
      "Training set | Epoch 178 | Loss: 0.6908034682273865\n",
      "Training set | Epoch 179 | Loss: 0.6866777539253235\n",
      "Training set | Epoch 180 | Loss: 0.6874109506607056\n",
      "Training set | Epoch 181 | Loss: 0.6894866228103638\n",
      "Training set | Epoch 182 | Loss: 0.6867948770523071\n",
      "Training set | Epoch 183 | Loss: 0.6861178874969482\n",
      "Training set | Epoch 184 | Loss: 0.6880884170532227\n",
      "Training set | Epoch 185 | Loss: 0.688033938407898\n",
      "Training set | Epoch 186 | Loss: 0.6847370862960815\n",
      "Training set | Epoch 187 | Loss: 0.6888220310211182\n",
      "Training set | Epoch 188 | Loss: 0.6857123374938965\n",
      "Training set | Epoch 189 | Loss: 0.6893654465675354\n",
      "Training set | Epoch 190 | Loss: 0.6845968961715698\n",
      "Training set | Epoch 191 | Loss: 0.6880164742469788\n",
      "Training set | Epoch 192 | Loss: 0.689278781414032\n",
      "Training set | Epoch 193 | Loss: 0.6869582533836365\n",
      "Training set | Epoch 194 | Loss: 0.6828075647354126\n",
      "Training set | Epoch 195 | Loss: 0.6905522346496582\n",
      "Training set | Epoch 196 | Loss: 0.6825841665267944\n",
      "Training set | Epoch 197 | Loss: 0.683327853679657\n",
      "Training set | Epoch 198 | Loss: 0.6910322308540344\n",
      "Training set | Epoch 199 | Loss: 0.6882758140563965\n",
      "Training set | Epoch 200 | Loss: 0.6840184926986694\n"
     ]
    }
   ],
   "source": [
    "gnn = VanillaGNN(1, 64, 1).to(device)\n",
    "print(gnn)\n",
    "\n",
    "gnn.fit(epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indx 0 Test loss: tensor(0.5431, device='cuda:0') Total matched 130 out of 240 (54.17%)\n",
      "Indx 1 Test loss: tensor(0.3911, device='cuda:0') Total matched 350 out of 576 (60.76%)\n",
      "Indx 2 Test loss: tensor(0.5350, device='cuda:0') Total matched 436 out of 896 (48.66%)\n",
      "Indx 3 Test loss: tensor(0.4347, device='cuda:0') Total matched 923 out of 1600 (57.69%)\n",
      "Indx 4 Test loss: tensor(0.9890, device='cuda:0') Total matched 1501 out of 3907 (38.42%)\n",
      "Indx 5 Test loss: tensor(1.2696, device='cuda:0') Total matched 2458 out of 7168 (34.29%)\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "import csv\n",
    "\n",
    "torch.no_grad()\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "f = open(\"test_result.csv\", \"w\", newline=\"\")\n",
    "csv_writer = csv.writer(f)\n",
    "csv_writer.writerow([\"Actual\", \"Predicted\"])\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for indx in range(len(dataset_coll)):\n",
    "    # indx = 0\n",
    "    total_loss = 0\n",
    "    total_matched = 0\n",
    "    dataset = dataset_coll[indx]\n",
    "    test_loader = test_dataloader_coll[indx]\n",
    "    test_dataset = test_loader.dataset\n",
    "\n",
    "    count = 0\n",
    "    for batch in test_loader:\n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "        y = y.unsqueeze(-1)\n",
    "        out = gnn(x, dataset.edge_index)\n",
    "        csv_writer.writerows(zip(y.detach().cpu().numpy(), out.detach().cpu().numpy()))\n",
    "        loss = criterion(out, y)\n",
    "        # print(\"Loss: \", loss)\n",
    "        total_loss += loss\n",
    "        out = torch.round(out)\n",
    "        matched = (out == y).sum().item()\n",
    "        total_matched += matched\n",
    "        count += 1\n",
    "\n",
    "    print(\n",
    "        \"Indx\",\n",
    "        indx,\n",
    "        \"Test loss:\",\n",
    "        total_loss.detach() / count,\n",
    "        \"Total matched\",\n",
    "        total_matched,\n",
    "        \"out of\",\n",
    "        len(test_dataset),\n",
    "        f\"({round(total_matched/len(test_dataset) * 100, 2)}%)\",\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
