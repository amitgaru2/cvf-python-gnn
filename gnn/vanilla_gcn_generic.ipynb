{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import CVFConfigForGCNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapping_categories = 15\n",
    "\n",
    "# dataset = CVFConfigDataset(\n",
    "#     \"small_graph_test_config_rank_dataset.csv\", \"small_graph_edge_index.json\", 4\n",
    "# )\n",
    "\n",
    "# dataset_n1 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"graph_1_config_rank_dataset.csv\",\n",
    "#     \"graph_1_edge_index.json\",\n",
    "#     20\n",
    "# )\n",
    "\n",
    "# dataset_n4 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"graph_4_config_rank_dataset.csv\",\n",
    "#     \"graph_4_edge_index.json\",\n",
    "#     color_mapping_categories\n",
    "# )\n",
    "\n",
    "# dataset_n5 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"graph_5_config_rank_dataset.csv\",\n",
    "#     \"graph_5_edge_index.json\",\n",
    "#     color_mapping_categories\n",
    "# )\n",
    "\n",
    "# dataset_n6 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"graph_6_config_rank_dataset.csv\",\n",
    "#     \"graph_6_edge_index.json\",\n",
    "#     color_mapping_categories,\n",
    "# )\n",
    "\n",
    "# dataset_n6b = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"graph_6b_config_rank_dataset.csv\",\n",
    "#     \"graph_6b_edge_index.json\",\n",
    "#     color_mapping_categories,\n",
    "# )\n",
    "\n",
    "# dataset_n7 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"graph_7_config_rank_dataset.csv\",\n",
    "#     \"graph_7_edge_index.json\",\n",
    "#     color_mapping_categories,\n",
    "# )\n",
    "\n",
    "# # dataset_n8 = CVFConfigForGCNDataset(\n",
    "# #     device,\n",
    "# #     \"graph_8_config_rank_dataset.csv\",\n",
    "# #     \"graph_8_edge_index.json\",\n",
    "# #     color_mapping_categories,\n",
    "# # )\n",
    "\n",
    "# dataset_pl_n5 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"graph_powerlaw_cluster_graph_n5_config_rank_dataset.csv\",\n",
    "#     \"graph_powerlaw_cluster_graph_n5_edge_index.json\",\n",
    "#     color_mapping_categories\n",
    "# )\n",
    "\n",
    "# dataset_pl_n6 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"graph_powerlaw_cluster_graph_n6_config_rank_dataset.csv\",\n",
    "#     \"graph_powerlaw_cluster_graph_n6_edge_index.json\",\n",
    "#     color_mapping_categories\n",
    "# )\n",
    "\n",
    "# dataset_pl_n7 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"graph_powerlaw_cluster_graph_n7_config_rank_dataset.csv\",\n",
    "#     \"graph_powerlaw_cluster_graph_n7_edge_index.json\",\n",
    "#     color_mapping_categories\n",
    "# )\n",
    "\n",
    "# dataset_pl_n8 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"graph_powerlaw_cluster_graph_n8_config_rank_dataset.csv\",\n",
    "#     \"graph_powerlaw_cluster_graph_n8_edge_index.json\",\n",
    "#     color_mapping_categories\n",
    "# )\n",
    "\n",
    "# dataset_pl_n9 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"graph_powerlaw_cluster_graph_n9_config_rank_dataset.csv\",\n",
    "#     \"graph_powerlaw_cluster_graph_n9_edge_index.json\",\n",
    "#     color_mapping_categories,\n",
    "# )\n",
    "\n",
    "# dataset_pl_n12 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"graph_powerlaw_cluster_graph_n12_config_rank_dataset.csv\",\n",
    "#     \"graph_powerlaw_cluster_graph_n12_edge_index.json\",\n",
    "#     color_mapping_categories\n",
    "# )\n",
    "\n",
    "# dataset_implicit_n15 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"implicit_graph_n15_config_rank_dataset.csv\",\n",
    "#     \"implicit_graph_n15_edge_index.json\",\n",
    "#     color_mapping_categories,\n",
    "#     one_hot_encode=False\n",
    "# )\n",
    "\n",
    "# dataset_implicit_n10 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"implicit_graph_n10_config_rank_dataset.csv\",\n",
    "#     \"implicit_graph_n10_edge_index.json\",\n",
    "#     color_mapping_categories,\n",
    "#     one_hot_encode=True\n",
    "# )\n",
    "\n",
    "# dataset_implicit_n4 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"implicit_graph_n4_config_rank_dataset.csv\",\n",
    "#     \"implicit_graph_n4_edge_index.json\",\n",
    "#     color_mapping_categories,\n",
    "#     one_hot_encode=True\n",
    "# )\n",
    "\n",
    "# dataset_implicit_n5 = CVFConfigForGCNDataset(\n",
    "#     device,\n",
    "#     \"implicit_graph_n5_config_rank_dataset.csv\",\n",
    "#     \"implicit_graph_n5_edge_index.json\",\n",
    "# )\n",
    "\n",
    "dataset_rr_n4 = CVFConfigForGCNDataset(\n",
    "    device,\n",
    "    \"graph_random_regular_graph_n4_d3_config_rank_dataset.csv\",\n",
    "    \"graph_random_regular_graph_n4_d3_edge_index.json\",\n",
    ")\n",
    "\n",
    "dataset_rr_n5 = CVFConfigForGCNDataset(\n",
    "    device,\n",
    "    \"graph_random_regular_graph_n5_d4_config_rank_dataset.csv\",\n",
    "    \"graph_random_regular_graph_n5_d4_edge_index.json\",\n",
    ")\n",
    "\n",
    "dataset_rr_n6 = CVFConfigForGCNDataset(\n",
    "    device,\n",
    "    \"graph_random_regular_graph_n6_d3_config_rank_dataset.csv\",\n",
    "    \"graph_random_regular_graph_n6_d3_edge_index.json\",\n",
    ")\n",
    "\n",
    "dataset_rr_n7 = CVFConfigForGCNDataset(\n",
    "    device,\n",
    "    \"graph_random_regular_graph_n7_d4_config_rank_dataset.csv\",\n",
    "    \"graph_random_regular_graph_n7_d4_edge_index.json\",\n",
    ")\n",
    "\n",
    "dataset_rr_n8 = CVFConfigForGCNDataset(\n",
    "    device,\n",
    "    \"graph_random_regular_graph_n8_d4_config_rank_dataset.csv\",\n",
    "    \"graph_random_regular_graph_n8_d4_edge_index.json\",\n",
    ")\n",
    "\n",
    "\n",
    "dataset_tiny_test = CVFConfigForGCNDataset(\n",
    "    device,\n",
    "    \"tiny_graph_test_config_rank_dataset.csv\",\n",
    "    \"tiny_graph_edge_index.json\",\n",
    ")\n",
    "\n",
    "dataset_graph_1 = CVFConfigForGCNDataset(\n",
    "    device,\n",
    "    \"graph_1_config_rank_dataset.csv\",\n",
    "    \"graph_1_edge_index.json\",\n",
    ")\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "dataset_coll = [\n",
    "    dataset_rr_n8\n",
    "]\n",
    "\n",
    "train_dataloader_coll = []\n",
    "test_dataloader_coll = []\n",
    "\n",
    "for dataset in dataset_coll:\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    train_dataloader_coll.append(train_loader)\n",
    "    test_dataloader_coll.append(test_loader)\n",
    "\n",
    "train_dataloader_coll_iter = [iter(i) for i in train_dataloader_coll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch():\n",
    "    end_loop = [False for _ in range(len(train_dataloader_coll))]\n",
    "    while not any(end_loop):\n",
    "        for di, data_loader in enumerate(train_dataloader_coll_iter):\n",
    "            if end_loop[di]:\n",
    "                continue\n",
    "            try:\n",
    "                batch = next(data_loader)\n",
    "            except StopIteration:\n",
    "                end_loop[di] = True\n",
    "                continue\n",
    "            yield batch, di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: [687]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of batches:\", [len(i) for i in train_dataloader_coll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaGNN(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNConv(dim_in, dim_h)\n",
    "        self.gcn2 = GCNConv(dim_h, dim_h)\n",
    "        self.out = torch.nn.Linear(dim_h, dim_out)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.gcn1(x, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = self.gcn2(h, edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = self.out(h)\n",
    "        h = torch.relu(h)\n",
    "        h = global_mean_pool(h, torch.zeros(h.size(1)).to(device).long())\n",
    "        return h\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=0.01)\n",
    "        # edge_index = dataset.edge_index.t().to(device)\n",
    "        dataloaders = itertools.tee(generate_batch(), epochs)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for batch, di in dataloaders[epoch - 1]:\n",
    "                x = batch[0]\n",
    "                y = batch[1]\n",
    "                y = y.unsqueeze(-1)\n",
    "                optimizer.zero_grad()\n",
    "                out = self(x, dataset_coll[di].edge_index)\n",
    "                # print(out.shape, y.shape)\n",
    "                loss = criterion(out, y)\n",
    "                total_loss += loss\n",
    "                count += 1\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if count > 0:\n",
    "                print(\n",
    "                    \"Training set | Epoch\",\n",
    "                    epoch,\n",
    "                    \"| Loss:\",\n",
    "                    (total_loss / count).item(),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanillaGNN(\n",
      "  (gcn1): GCNConv(1, 64)\n",
      "  (gcn2): GCNConv(64, 64)\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Training set | Epoch 1 | Loss: 1.3277958631515503\n",
      "Training set | Epoch 2 | Loss: 1.2286453247070312\n",
      "Training set | Epoch 3 | Loss: 1.2268195152282715\n",
      "Training set | Epoch 4 | Loss: 1.2256735563278198\n",
      "Training set | Epoch 5 | Loss: 1.2250410318374634\n",
      "Training set | Epoch 6 | Loss: 1.2241405248641968\n",
      "Training set | Epoch 7 | Loss: 1.2238231897354126\n",
      "Training set | Epoch 8 | Loss: 1.2232012748718262\n",
      "Training set | Epoch 9 | Loss: 1.2228171825408936\n",
      "Training set | Epoch 10 | Loss: 1.2221161127090454\n",
      "Training set | Epoch 11 | Loss: 1.222428798675537\n",
      "Training set | Epoch 12 | Loss: 1.2217425107955933\n",
      "Training set | Epoch 13 | Loss: 1.221452236175537\n",
      "Training set | Epoch 14 | Loss: 1.2207239866256714\n",
      "Training set | Epoch 15 | Loss: 1.2206833362579346\n",
      "Training set | Epoch 16 | Loss: 1.2205160856246948\n",
      "Training set | Epoch 17 | Loss: 1.2203260660171509\n",
      "Training set | Epoch 18 | Loss: 1.219973087310791\n",
      "Training set | Epoch 19 | Loss: 1.2197684049606323\n",
      "Training set | Epoch 20 | Loss: 1.2189075946807861\n",
      "Training set | Epoch 21 | Loss: 1.2186610698699951\n",
      "Training set | Epoch 22 | Loss: 1.2185070514678955\n",
      "Training set | Epoch 23 | Loss: 1.2180156707763672\n",
      "Training set | Epoch 24 | Loss: 1.217753291130066\n",
      "Training set | Epoch 25 | Loss: 1.217637062072754\n",
      "Training set | Epoch 26 | Loss: 1.2172538042068481\n",
      "Training set | Epoch 27 | Loss: 1.2171998023986816\n",
      "Training set | Epoch 28 | Loss: 1.2160382270812988\n",
      "Training set | Epoch 29 | Loss: 1.2166857719421387\n",
      "Training set | Epoch 30 | Loss: 1.2166153192520142\n",
      "Training set | Epoch 31 | Loss: 1.2165781259536743\n",
      "Training set | Epoch 32 | Loss: 1.2166523933410645\n",
      "Training set | Epoch 33 | Loss: 1.2165100574493408\n",
      "Training set | Epoch 34 | Loss: 1.2164949178695679\n",
      "Training set | Epoch 35 | Loss: 1.2162024974822998\n",
      "Training set | Epoch 36 | Loss: 1.2162952423095703\n",
      "Training set | Epoch 37 | Loss: 1.216302514076233\n",
      "Training set | Epoch 38 | Loss: 1.2160733938217163\n",
      "Training set | Epoch 39 | Loss: 1.2160152196884155\n",
      "Training set | Epoch 40 | Loss: 1.2164490222930908\n",
      "Training set | Epoch 41 | Loss: 1.21614670753479\n",
      "Training set | Epoch 42 | Loss: 1.2165038585662842\n",
      "Training set | Epoch 43 | Loss: 1.2161468267440796\n",
      "Training set | Epoch 44 | Loss: 1.215940237045288\n",
      "Training set | Epoch 45 | Loss: 1.2157407999038696\n",
      "Training set | Epoch 46 | Loss: 1.2160766124725342\n",
      "Training set | Epoch 47 | Loss: 1.2158557176589966\n",
      "Training set | Epoch 48 | Loss: 1.2164181470870972\n",
      "Training set | Epoch 49 | Loss: 1.215843915939331\n",
      "Training set | Epoch 50 | Loss: 1.2159818410873413\n",
      "Training set | Epoch 51 | Loss: 1.2157542705535889\n",
      "Training set | Epoch 52 | Loss: 1.216023564338684\n",
      "Training set | Epoch 53 | Loss: 1.2161856889724731\n",
      "Training set | Epoch 54 | Loss: 1.2158788442611694\n",
      "Training set | Epoch 55 | Loss: 1.21591317653656\n",
      "Training set | Epoch 56 | Loss: 1.216227650642395\n",
      "Training set | Epoch 57 | Loss: 1.2157797813415527\n",
      "Training set | Epoch 58 | Loss: 1.2155476808547974\n",
      "Training set | Epoch 59 | Loss: 1.2157248258590698\n",
      "Training set | Epoch 60 | Loss: 1.2158836126327515\n",
      "Training set | Epoch 61 | Loss: 1.2158746719360352\n",
      "Training set | Epoch 62 | Loss: 1.2161383628845215\n",
      "Training set | Epoch 63 | Loss: 1.2160677909851074\n",
      "Training set | Epoch 64 | Loss: 1.2157870531082153\n",
      "Training set | Epoch 65 | Loss: 1.2153759002685547\n",
      "Training set | Epoch 66 | Loss: 1.2154024839401245\n",
      "Training set | Epoch 67 | Loss: 1.2153102159500122\n",
      "Training set | Epoch 68 | Loss: 1.2156028747558594\n",
      "Training set | Epoch 69 | Loss: 1.215709924697876\n",
      "Training set | Epoch 70 | Loss: 1.2155910730361938\n",
      "Training set | Epoch 71 | Loss: 1.2159185409545898\n",
      "Training set | Epoch 72 | Loss: 1.2157315015792847\n",
      "Training set | Epoch 73 | Loss: 1.2155286073684692\n",
      "Training set | Epoch 74 | Loss: 1.215701937675476\n",
      "Training set | Epoch 75 | Loss: 1.2155086994171143\n",
      "Training set | Epoch 76 | Loss: 1.2155097723007202\n",
      "Training set | Epoch 77 | Loss: 1.2155990600585938\n",
      "Training set | Epoch 78 | Loss: 1.2153695821762085\n",
      "Training set | Epoch 79 | Loss: 1.215582013130188\n",
      "Training set | Epoch 80 | Loss: 1.2153925895690918\n",
      "Training set | Epoch 81 | Loss: 1.215645432472229\n",
      "Training set | Epoch 82 | Loss: 1.2155436277389526\n",
      "Training set | Epoch 83 | Loss: 1.2154076099395752\n",
      "Training set | Epoch 84 | Loss: 1.2152444124221802\n",
      "Training set | Epoch 85 | Loss: 1.2151893377304077\n",
      "Training set | Epoch 86 | Loss: 1.2153419256210327\n",
      "Training set | Epoch 87 | Loss: 1.2154568433761597\n",
      "Training set | Epoch 88 | Loss: 1.215435266494751\n",
      "Training set | Epoch 89 | Loss: 1.2152507305145264\n",
      "Training set | Epoch 90 | Loss: 1.2152827978134155\n",
      "Training set | Epoch 91 | Loss: 1.215334415435791\n",
      "Training set | Epoch 92 | Loss: 1.2153797149658203\n",
      "Training set | Epoch 93 | Loss: 1.2153605222702026\n",
      "Training set | Epoch 94 | Loss: 1.215415596961975\n",
      "Training set | Epoch 95 | Loss: 1.2153747081756592\n",
      "Training set | Epoch 96 | Loss: 1.2153871059417725\n",
      "Training set | Epoch 97 | Loss: 1.2153277397155762\n",
      "Training set | Epoch 98 | Loss: 1.2153090238571167\n",
      "Training set | Epoch 99 | Loss: 1.2154532670974731\n",
      "Training set | Epoch 100 | Loss: 1.2156081199645996\n",
      "Training set | Epoch 101 | Loss: 1.21527099609375\n",
      "Training set | Epoch 102 | Loss: 1.2151401042938232\n",
      "Training set | Epoch 103 | Loss: 1.215218186378479\n",
      "Training set | Epoch 104 | Loss: 1.2153884172439575\n",
      "Training set | Epoch 105 | Loss: 1.2150918245315552\n",
      "Training set | Epoch 106 | Loss: 1.215436577796936\n",
      "Training set | Epoch 107 | Loss: 1.215653896331787\n",
      "Training set | Epoch 108 | Loss: 1.2152531147003174\n",
      "Training set | Epoch 109 | Loss: 1.215619683265686\n",
      "Training set | Epoch 110 | Loss: 1.2154912948608398\n",
      "Training set | Epoch 111 | Loss: 1.2152066230773926\n",
      "Training set | Epoch 112 | Loss: 1.215205430984497\n",
      "Training set | Epoch 113 | Loss: 1.2152131795883179\n",
      "Training set | Epoch 114 | Loss: 1.215518832206726\n",
      "Training set | Epoch 115 | Loss: 1.2152963876724243\n",
      "Training set | Epoch 116 | Loss: 1.21523916721344\n",
      "Training set | Epoch 117 | Loss: 1.2154968976974487\n",
      "Training set | Epoch 118 | Loss: 1.2150616645812988\n",
      "Training set | Epoch 119 | Loss: 1.2152248620986938\n",
      "Training set | Epoch 120 | Loss: 1.215197205543518\n",
      "Training set | Epoch 121 | Loss: 1.2154983282089233\n",
      "Training set | Epoch 122 | Loss: 1.2154101133346558\n",
      "Training set | Epoch 123 | Loss: 1.2153581380844116\n",
      "Training set | Epoch 124 | Loss: 1.2155680656433105\n",
      "Training set | Epoch 125 | Loss: 1.2152955532073975\n",
      "Training set | Epoch 126 | Loss: 1.2153595685958862\n",
      "Training set | Epoch 127 | Loss: 1.2152479887008667\n",
      "Training set | Epoch 128 | Loss: 1.2151477336883545\n",
      "Training set | Epoch 129 | Loss: 1.2154145240783691\n",
      "Training set | Epoch 130 | Loss: 1.2152601480484009\n",
      "Training set | Epoch 131 | Loss: 1.2153407335281372\n",
      "Training set | Epoch 132 | Loss: 1.2158514261245728\n",
      "Training set | Epoch 133 | Loss: 1.215362548828125\n",
      "Training set | Epoch 134 | Loss: 1.215074896812439\n",
      "Training set | Epoch 135 | Loss: 1.2153921127319336\n",
      "Training set | Epoch 136 | Loss: 1.2151356935501099\n",
      "Training set | Epoch 137 | Loss: 1.2155715227127075\n",
      "Training set | Epoch 138 | Loss: 1.215273380279541\n",
      "Training set | Epoch 139 | Loss: 1.2152178287506104\n",
      "Training set | Epoch 140 | Loss: 1.2153685092926025\n",
      "Training set | Epoch 141 | Loss: 1.215754747390747\n",
      "Training set | Epoch 142 | Loss: 1.2153503894805908\n",
      "Training set | Epoch 143 | Loss: 1.215356469154358\n",
      "Training set | Epoch 144 | Loss: 1.215232491493225\n",
      "Training set | Epoch 145 | Loss: 1.2150894403457642\n",
      "Training set | Epoch 146 | Loss: 1.215366244316101\n",
      "Training set | Epoch 147 | Loss: 1.215397596359253\n",
      "Training set | Epoch 148 | Loss: 1.2156603336334229\n",
      "Training set | Epoch 149 | Loss: 1.2152178287506104\n",
      "Training set | Epoch 150 | Loss: 1.2153116464614868\n",
      "Training set | Epoch 151 | Loss: 1.2155227661132812\n",
      "Training set | Epoch 152 | Loss: 1.215186595916748\n",
      "Training set | Epoch 153 | Loss: 1.215533971786499\n",
      "Training set | Epoch 154 | Loss: 1.2152516841888428\n",
      "Training set | Epoch 155 | Loss: 1.2152796983718872\n",
      "Training set | Epoch 156 | Loss: 1.2153494358062744\n",
      "Training set | Epoch 157 | Loss: 1.2151275873184204\n",
      "Training set | Epoch 158 | Loss: 1.2152855396270752\n",
      "Training set | Epoch 159 | Loss: 1.2152073383331299\n",
      "Training set | Epoch 160 | Loss: 1.215187430381775\n",
      "Training set | Epoch 161 | Loss: 1.215232491493225\n",
      "Training set | Epoch 162 | Loss: 1.2151362895965576\n",
      "Training set | Epoch 163 | Loss: 1.2152279615402222\n",
      "Training set | Epoch 164 | Loss: 1.2156562805175781\n",
      "Training set | Epoch 165 | Loss: 1.2151708602905273\n",
      "Training set | Epoch 166 | Loss: 1.215116262435913\n",
      "Training set | Epoch 167 | Loss: 1.2148668766021729\n",
      "Training set | Epoch 168 | Loss: 1.2150044441223145\n",
      "Training set | Epoch 169 | Loss: 1.2150301933288574\n",
      "Training set | Epoch 170 | Loss: 1.2155135869979858\n",
      "Training set | Epoch 171 | Loss: 1.2152410745620728\n",
      "Training set | Epoch 172 | Loss: 1.215128779411316\n",
      "Training set | Epoch 173 | Loss: 1.2151020765304565\n",
      "Training set | Epoch 174 | Loss: 1.215104579925537\n",
      "Training set | Epoch 175 | Loss: 1.2152400016784668\n",
      "Training set | Epoch 176 | Loss: 1.2151552438735962\n",
      "Training set | Epoch 177 | Loss: 1.2151832580566406\n",
      "Training set | Epoch 178 | Loss: 1.2151602506637573\n",
      "Training set | Epoch 179 | Loss: 1.2152578830718994\n",
      "Training set | Epoch 180 | Loss: 1.21499502658844\n",
      "Training set | Epoch 181 | Loss: 1.2153264284133911\n",
      "Training set | Epoch 182 | Loss: 1.2150294780731201\n",
      "Training set | Epoch 183 | Loss: 1.2152174711227417\n",
      "Training set | Epoch 184 | Loss: 1.2148433923721313\n",
      "Training set | Epoch 185 | Loss: 1.2149548530578613\n",
      "Training set | Epoch 186 | Loss: 1.2152822017669678\n",
      "Training set | Epoch 187 | Loss: 1.2151212692260742\n",
      "Training set | Epoch 188 | Loss: 1.2149066925048828\n",
      "Training set | Epoch 189 | Loss: 1.2150546312332153\n",
      "Training set | Epoch 190 | Loss: 1.2149426937103271\n",
      "Training set | Epoch 191 | Loss: 1.2151843309402466\n",
      "Training set | Epoch 192 | Loss: 1.2151018381118774\n",
      "Training set | Epoch 193 | Loss: 1.2153481245040894\n",
      "Training set | Epoch 194 | Loss: 1.2153398990631104\n",
      "Training set | Epoch 195 | Loss: 1.2148773670196533\n",
      "Training set | Epoch 196 | Loss: 1.214941382408142\n",
      "Training set | Epoch 197 | Loss: 1.2152851819992065\n",
      "Training set | Epoch 198 | Loss: 1.2151191234588623\n",
      "Training set | Epoch 199 | Loss: 1.2149890661239624\n",
      "Training set | Epoch 200 | Loss: 1.2152385711669922\n",
      "Training set | Epoch 201 | Loss: 1.215111255645752\n",
      "Training set | Epoch 202 | Loss: 1.2151576280593872\n",
      "Training set | Epoch 203 | Loss: 1.2150254249572754\n",
      "Training set | Epoch 204 | Loss: 1.21506667137146\n",
      "Training set | Epoch 205 | Loss: 1.2149707078933716\n",
      "Training set | Epoch 206 | Loss: 1.214987874031067\n",
      "Training set | Epoch 207 | Loss: 1.2150167226791382\n",
      "Training set | Epoch 208 | Loss: 1.2151795625686646\n",
      "Training set | Epoch 209 | Loss: 1.2151228189468384\n",
      "Training set | Epoch 210 | Loss: 1.2151391506195068\n",
      "Training set | Epoch 211 | Loss: 1.2151092290878296\n",
      "Training set | Epoch 212 | Loss: 1.2150474786758423\n",
      "Training set | Epoch 213 | Loss: 1.2153253555297852\n",
      "Training set | Epoch 214 | Loss: 1.215221643447876\n",
      "Training set | Epoch 215 | Loss: 1.2151899337768555\n",
      "Training set | Epoch 216 | Loss: 1.215111494064331\n",
      "Training set | Epoch 217 | Loss: 1.2151228189468384\n",
      "Training set | Epoch 218 | Loss: 1.2149943113327026\n",
      "Training set | Epoch 219 | Loss: 1.2151585817337036\n",
      "Training set | Epoch 220 | Loss: 1.2151551246643066\n",
      "Training set | Epoch 221 | Loss: 1.2151670455932617\n",
      "Training set | Epoch 222 | Loss: 1.215052604675293\n",
      "Training set | Epoch 223 | Loss: 1.2154738903045654\n",
      "Training set | Epoch 224 | Loss: 1.2150568962097168\n",
      "Training set | Epoch 225 | Loss: 1.2151340246200562\n",
      "Training set | Epoch 226 | Loss: 1.2149457931518555\n",
      "Training set | Epoch 227 | Loss: 1.2150996923446655\n",
      "Training set | Epoch 228 | Loss: 1.215022325515747\n",
      "Training set | Epoch 229 | Loss: 1.214948296546936\n",
      "Training set | Epoch 230 | Loss: 1.2149258852005005\n",
      "Training set | Epoch 231 | Loss: 1.2150567770004272\n",
      "Training set | Epoch 232 | Loss: 1.2150827646255493\n",
      "Training set | Epoch 233 | Loss: 1.2149966955184937\n",
      "Training set | Epoch 234 | Loss: 1.2151676416397095\n",
      "Training set | Epoch 235 | Loss: 1.2152031660079956\n",
      "Training set | Epoch 236 | Loss: 1.2151086330413818\n",
      "Training set | Epoch 237 | Loss: 1.215061068534851\n",
      "Training set | Epoch 238 | Loss: 1.2153488397598267\n",
      "Training set | Epoch 239 | Loss: 1.2153068780899048\n",
      "Training set | Epoch 240 | Loss: 1.2151010036468506\n",
      "Training set | Epoch 241 | Loss: 1.2151166200637817\n",
      "Training set | Epoch 242 | Loss: 1.2152372598648071\n",
      "Training set | Epoch 243 | Loss: 1.2152706384658813\n",
      "Training set | Epoch 244 | Loss: 1.214854121208191\n",
      "Training set | Epoch 245 | Loss: 1.2149873971939087\n",
      "Training set | Epoch 246 | Loss: 1.2154470682144165\n",
      "Training set | Epoch 247 | Loss: 1.2149581909179688\n",
      "Training set | Epoch 248 | Loss: 1.2150591611862183\n",
      "Training set | Epoch 249 | Loss: 1.2151174545288086\n",
      "Training set | Epoch 250 | Loss: 1.2148767709732056\n"
     ]
    }
   ],
   "source": [
    "gnn = VanillaGNN(1, 64, 1).to(device)\n",
    "print(gnn)\n",
    "\n",
    "gnn.fit(epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: tensor(1.2060, device='cuda:0') Total matched 13165 out of 39063 (33.7%)\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "import csv\n",
    "\n",
    "torch.no_grad()\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "f = open(\"test_result.csv\", \"w\", newline=\"\")\n",
    "csv_writer = csv.writer(f)\n",
    "csv_writer.writerow([\"Actual\", \"Predicted\"])\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "total_loss = 0\n",
    "total_matched = 0\n",
    "\n",
    "indx = 0\n",
    "dataset = dataset_coll[indx]\n",
    "test_loader = test_dataloader_coll[indx]\n",
    "test_dataset = test_loader.dataset\n",
    "\n",
    "count = 0\n",
    "for batch in test_loader:\n",
    "    x = batch[0]\n",
    "    y = batch[1]\n",
    "    y = y.unsqueeze(-1)\n",
    "    out = gnn(x, dataset.edge_index)\n",
    "    csv_writer.writerows(zip(y.detach().cpu().numpy(), out.detach().cpu().numpy()))\n",
    "    loss = criterion(out, y)\n",
    "    total_loss += loss\n",
    "    out = torch.round(out)\n",
    "    matched = (out == y).sum().item()\n",
    "    total_matched += matched\n",
    "    count += 1\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(\n",
    "    \"Test loss:\",\n",
    "    total_loss.detach() / count,\n",
    "    \"Total matched\",\n",
    "    total_matched,\n",
    "    \"out of\",\n",
    "    len(test_dataset),\n",
    "    f\"({round(total_matched/len(test_dataset) * 100, 2)}%)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
