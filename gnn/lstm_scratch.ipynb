{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "from torch.utils.data import ConcatDataset, DataLoader, random_split, Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import  CVFConfigForGCNWSuccLSTMDataset, CVFConfigForGCNWSuccLSTMWNormalizationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s_n7 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "    device,\n",
    "    \"star_graph_n7_config_rank_dataset.csv\",\n",
    "    \"star_graph_n7_edge_index.json\",\n",
    ")\n",
    "\n",
    "# dataset_s_n13 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "#     device,\n",
    "#     \"star_graph_n13_config_rank_dataset.csv\",\n",
    "#     \"star_graph_n13_edge_index.json\",\n",
    "# )\n",
    "\n",
    "# dataset_s_n15 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "#     device,\n",
    "#     \"star_graph_n15_config_rank_dataset.csv\",\n",
    "#     \"star_graph_n15_edge_index.json\",\n",
    "# )\n",
    "\n",
    "dataset_rr_n7 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "    device,\n",
    "    \"graph_random_regular_graph_n7_d4_config_rank_dataset.csv\",\n",
    "    \"graph_random_regular_graph_n7_d4_edge_index.json\",\n",
    ")\n",
    "\n",
    "# dataset_rr_n8 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "#     device,\n",
    "#     \"graph_random_regular_graph_n8_d4_config_rank_dataset.csv\",\n",
    "#     \"graph_random_regular_graph_n8_d4_edge_index.json\",\n",
    "# )\n",
    "\n",
    "dataset_plc_n7 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "    device,\n",
    "    \"graph_powerlaw_cluster_graph_n7_config_rank_dataset.csv\",\n",
    "    \"graph_powerlaw_cluster_graph_n7_edge_index.json\",\n",
    ")\n",
    "\n",
    "# dataset_plc_n9 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "#     device,\n",
    "#     \"graph_powerlaw_cluster_graph_n9_config_rank_dataset.csv\",\n",
    "#     \"graph_powerlaw_cluster_graph_n9_edge_index.json\",\n",
    "# )\n",
    "\n",
    "dataset_implict_n5 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "    device,\n",
    "    \"implicit_graph_n5_config_rank_dataset.csv\",\n",
    "    \"implicit_graph_n5_edge_index.json\",\n",
    "    \"dijkstra\",\n",
    ")\n",
    "\n",
    "# dataset_implict_n10 = CVFConfigForGCNWSuccWEIDataset(\n",
    "#     device,\n",
    "#     \"implicit_graph_n10_config_rank_dataset.csv\",\n",
    "#     \"implicit_graph_n10_edge_index.json\",\n",
    "#     \"dijkstra\",\n",
    "# )\n",
    "\n",
    "dataset_implict_n7 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "    device,\n",
    "    \"implicit_graph_n7_config_rank_dataset.csv\",\n",
    "    \"implicit_graph_n7_edge_index.json\",\n",
    "    \"dijkstra\",\n",
    ")\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "dataset_coll = [\n",
    "    # dataset_implict_n5,\n",
    "    # dataset_implict_n10,\n",
    "    dataset_implict_n7,\n",
    "    # dataset_s_n7,\n",
    "    # # dataset_s_n13,\n",
    "    # # dataset_s_n15,\n",
    "    # dataset_rr_n7,\n",
    "    # # dataset_rr_n8,\n",
    "    # dataset_plc_n7,\n",
    "    # # dataset_plc_n9,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes = [int(0.95 * len(ds)) for ds in dataset_coll]\n",
    "test_sizes = [len(ds) - trs for ds, trs in zip(dataset_coll, train_sizes)]\n",
    "\n",
    "train_test_datasets = [\n",
    "    random_split(ds, [tr_s, ts])\n",
    "    for ds, tr_s, ts in zip(dataset_coll, train_sizes, test_sizes)\n",
    "]\n",
    "\n",
    "train_datasets = [ds[0] for ds in train_test_datasets]\n",
    "test_datasets = [ds[1] for ds in train_test_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset size: 2,077\n"
     ]
    }
   ],
   "source": [
    "datasets = ConcatDataset(train_datasets)\n",
    "print(f\"Train Dataset size: {len(datasets):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBatchSampler(Sampler):\n",
    "    def __init__(self, datasets: ConcatDataset, batch_size: int):\n",
    "        self.datasets = datasets\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        last_accessed = [0] + self.datasets.cumulative_sizes[:]\n",
    "        end_loop = [False for _ in range(len(self.datasets.datasets))]\n",
    "\n",
    "        while not all(end_loop):\n",
    "            for turn in range(len(self.datasets.datasets)):\n",
    "                if end_loop[turn]:\n",
    "                    continue\n",
    "\n",
    "                batch_size = self.batch_size\n",
    "                if (\n",
    "                    last_accessed[turn] + batch_size\n",
    "                    >= self.datasets.cumulative_sizes[turn]\n",
    "                ):\n",
    "                    batch_size = (\n",
    "                        self.datasets.cumulative_sizes[turn] - last_accessed[turn]\n",
    "                    )\n",
    "                    end_loop[turn] = True\n",
    "\n",
    "                yield list(range(last_accessed[turn], last_accessed[turn] + batch_size))\n",
    "\n",
    "                last_accessed[turn] += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from normalization import compute_mean_std, NormalizeTransform\n",
    "\n",
    "# loader = DataLoader(datasets, batch_sampler=CustomBatchSampler(datasets, batch_size=1024))\n",
    "# mean, std = compute_mean_std(loader)\n",
    "# print(mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform = NormalizeTransform(mean, std)\n",
    "# for dataset in train_datasets:\n",
    "#     dataset.dataset.set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler = CustomBatchSampler(datasets, batch_size=batch_size)\n",
    "dataloader = DataLoader(datasets, batch_sampler=batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        output = self.norm(output)\n",
    "        output = self.h2o(output)\n",
    "        output = torch.relu(output)\n",
    "        output = global_mean_pool(output, torch.zeros(output.size(1)).to(device).long())\n",
    "        return output\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for batch in dataloader:\n",
    "                x = batch[0]\n",
    "                x = x[0]\n",
    "                y = batch[1]\n",
    "                y = y.unsqueeze(-1)\n",
    "                out = self(x)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(out, y)\n",
    "                total_loss += loss\n",
    "                count += 1\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(\n",
    "                \"Training set | Epoch\",\n",
    "                epoch,\n",
    "                \"| MSE Loss:\",\n",
    "                round((total_loss / count).item(), 4),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleLSTM(\n",
      "  (lstm): GRU(3, 64, batch_first=True)\n",
      "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (h2o): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 13,441\n",
      "\n",
      "Training set | Epoch 1 | MSE Loss: 453.8004\n",
      "Training set | Epoch 2 | MSE Loss: 245.9983\n",
      "Training set | Epoch 3 | MSE Loss: 130.6209\n",
      "Training set | Epoch 4 | MSE Loss: 87.7392\n",
      "Training set | Epoch 5 | MSE Loss: 73.4434\n",
      "Training set | Epoch 6 | MSE Loss: 64.9336\n",
      "Training set | Epoch 7 | MSE Loss: 59.6871\n",
      "Training set | Epoch 8 | MSE Loss: 55.7449\n",
      "Training set | Epoch 9 | MSE Loss: 53.5691\n",
      "Training set | Epoch 10 | MSE Loss: 52.0865\n",
      "Training set | Epoch 11 | MSE Loss: 51.0032\n",
      "Training set | Epoch 12 | MSE Loss: 50.4764\n",
      "Training set | Epoch 13 | MSE Loss: 49.7792\n",
      "Training set | Epoch 14 | MSE Loss: 48.1184\n",
      "Training set | Epoch 15 | MSE Loss: 45.6512\n",
      "Training set | Epoch 16 | MSE Loss: 44.2663\n",
      "Training set | Epoch 17 | MSE Loss: 42.9925\n",
      "Training set | Epoch 18 | MSE Loss: 42.055\n",
      "Training set | Epoch 19 | MSE Loss: 41.1111\n",
      "Training set | Epoch 20 | MSE Loss: 39.9943\n",
      "Training set | Epoch 21 | MSE Loss: 38.8445\n",
      "Training set | Epoch 22 | MSE Loss: 37.6916\n",
      "Training set | Epoch 23 | MSE Loss: 36.3936\n",
      "Training set | Epoch 24 | MSE Loss: 34.9319\n",
      "Training set | Epoch 25 | MSE Loss: 33.4507\n",
      "Training set | Epoch 26 | MSE Loss: 32.0339\n",
      "Training set | Epoch 27 | MSE Loss: 30.702\n",
      "Training set | Epoch 28 | MSE Loss: 29.4434\n",
      "Training set | Epoch 29 | MSE Loss: 28.0653\n",
      "Training set | Epoch 30 | MSE Loss: 26.856\n",
      "Training set | Epoch 31 | MSE Loss: 25.684\n",
      "Training set | Epoch 32 | MSE Loss: 24.6952\n",
      "Training set | Epoch 33 | MSE Loss: 23.4762\n",
      "Training set | Epoch 34 | MSE Loss: 22.4082\n",
      "Training set | Epoch 35 | MSE Loss: 21.6256\n",
      "Training set | Epoch 36 | MSE Loss: 20.9892\n",
      "Training set | Epoch 37 | MSE Loss: 20.382\n",
      "Training set | Epoch 38 | MSE Loss: 20.2575\n",
      "Training set | Epoch 39 | MSE Loss: 19.4675\n",
      "Training set | Epoch 40 | MSE Loss: 19.3914\n",
      "Training set | Epoch 41 | MSE Loss: 18.5038\n",
      "Training set | Epoch 42 | MSE Loss: 17.4603\n",
      "Training set | Epoch 43 | MSE Loss: 16.4651\n",
      "Training set | Epoch 44 | MSE Loss: 15.7577\n",
      "Training set | Epoch 45 | MSE Loss: 14.9336\n",
      "Training set | Epoch 46 | MSE Loss: 14.3975\n",
      "Training set | Epoch 47 | MSE Loss: 14.3244\n",
      "Training set | Epoch 48 | MSE Loss: 14.1599\n",
      "Training set | Epoch 49 | MSE Loss: 13.4779\n",
      "Training set | Epoch 50 | MSE Loss: 13.8568\n",
      "Training set | Epoch 51 | MSE Loss: 13.5403\n",
      "Training set | Epoch 52 | MSE Loss: 13.5413\n",
      "Training set | Epoch 53 | MSE Loss: 13.7449\n",
      "Training set | Epoch 54 | MSE Loss: 13.7996\n",
      "Training set | Epoch 55 | MSE Loss: 12.5503\n",
      "Training set | Epoch 56 | MSE Loss: 12.4302\n",
      "Training set | Epoch 57 | MSE Loss: 12.3559\n",
      "Training set | Epoch 58 | MSE Loss: 12.2602\n",
      "Training set | Epoch 59 | MSE Loss: 12.2262\n",
      "Training set | Epoch 60 | MSE Loss: 13.581\n",
      "Training set | Epoch 61 | MSE Loss: 10.4476\n",
      "Training set | Epoch 62 | MSE Loss: 9.7562\n",
      "Training set | Epoch 63 | MSE Loss: 9.0708\n",
      "Training set | Epoch 64 | MSE Loss: 8.7588\n",
      "Training set | Epoch 65 | MSE Loss: 8.4477\n",
      "Training set | Epoch 66 | MSE Loss: 8.1119\n",
      "Training set | Epoch 67 | MSE Loss: 7.7767\n",
      "Training set | Epoch 68 | MSE Loss: 7.533\n",
      "Training set | Epoch 69 | MSE Loss: 7.2954\n",
      "Training set | Epoch 70 | MSE Loss: 7.2322\n",
      "Training set | Epoch 71 | MSE Loss: 7.1751\n",
      "Training set | Epoch 72 | MSE Loss: 7.4471\n",
      "Training set | Epoch 73 | MSE Loss: 7.355\n",
      "Training set | Epoch 74 | MSE Loss: 7.226\n",
      "Training set | Epoch 75 | MSE Loss: 7.7343\n",
      "Training set | Epoch 76 | MSE Loss: 7.5221\n",
      "Training set | Epoch 77 | MSE Loss: 7.3598\n",
      "Training set | Epoch 78 | MSE Loss: 7.2017\n",
      "Training set | Epoch 79 | MSE Loss: 7.0813\n",
      "Training set | Epoch 80 | MSE Loss: 7.2347\n",
      "Training set | Epoch 81 | MSE Loss: 6.4587\n",
      "Training set | Epoch 82 | MSE Loss: 6.4561\n",
      "Training set | Epoch 83 | MSE Loss: 6.0836\n",
      "Training set | Epoch 84 | MSE Loss: 5.9249\n",
      "Training set | Epoch 85 | MSE Loss: 5.708\n",
      "Training set | Epoch 86 | MSE Loss: 5.6524\n",
      "Training set | Epoch 87 | MSE Loss: 5.3459\n",
      "Training set | Epoch 88 | MSE Loss: 5.4919\n",
      "Training set | Epoch 89 | MSE Loss: 5.0843\n",
      "Training set | Epoch 90 | MSE Loss: 5.062\n",
      "Training set | Epoch 91 | MSE Loss: 5.4358\n",
      "Training set | Epoch 92 | MSE Loss: 5.0851\n",
      "Training set | Epoch 93 | MSE Loss: 5.3759\n",
      "Training set | Epoch 94 | MSE Loss: 5.4203\n",
      "Training set | Epoch 95 | MSE Loss: 5.6262\n",
      "Training set | Epoch 96 | MSE Loss: 5.9493\n",
      "Training set | Epoch 97 | MSE Loss: 5.6874\n",
      "Training set | Epoch 98 | MSE Loss: 6.2518\n",
      "Training set | Epoch 99 | MSE Loss: 5.2446\n",
      "Training set | Epoch 100 | MSE Loss: 5.5449\n",
      "Training set | Epoch 101 | MSE Loss: 6.1031\n",
      "Training set | Epoch 102 | MSE Loss: 5.6778\n",
      "Training set | Epoch 103 | MSE Loss: 5.8013\n",
      "Training set | Epoch 104 | MSE Loss: 6.3234\n",
      "Training set | Epoch 105 | MSE Loss: 6.7784\n",
      "Training set | Epoch 106 | MSE Loss: 7.9418\n",
      "Training set | Epoch 107 | MSE Loss: 10.3506\n",
      "Training set | Epoch 108 | MSE Loss: 12.4534\n",
      "Training set | Epoch 109 | MSE Loss: 17.9093\n",
      "Training set | Epoch 110 | MSE Loss: 23.2024\n",
      "Training set | Epoch 111 | MSE Loss: 17.3963\n",
      "Training set | Epoch 112 | MSE Loss: 10.3467\n",
      "Training set | Epoch 113 | MSE Loss: 7.5935\n",
      "Training set | Epoch 114 | MSE Loss: 6.4583\n",
      "Training set | Epoch 115 | MSE Loss: 6.3942\n",
      "Training set | Epoch 116 | MSE Loss: 4.9518\n",
      "Training set | Epoch 117 | MSE Loss: 4.3623\n",
      "Training set | Epoch 118 | MSE Loss: 4.0803\n",
      "Training set | Epoch 119 | MSE Loss: 4.3847\n",
      "Training set | Epoch 120 | MSE Loss: 4.0984\n",
      "Training set | Epoch 121 | MSE Loss: 3.5689\n",
      "Training set | Epoch 122 | MSE Loss: 3.2416\n",
      "Training set | Epoch 123 | MSE Loss: 3.1874\n",
      "Training set | Epoch 124 | MSE Loss: 3.3649\n",
      "Training set | Epoch 125 | MSE Loss: 3.399\n",
      "Training set | Epoch 126 | MSE Loss: 3.1064\n",
      "Training set | Epoch 127 | MSE Loss: 2.8774\n",
      "Training set | Epoch 128 | MSE Loss: 2.9169\n",
      "Training set | Epoch 129 | MSE Loss: 2.8213\n",
      "Training set | Epoch 130 | MSE Loss: 3.224\n",
      "Training set | Epoch 131 | MSE Loss: 3.3769\n",
      "Training set | Epoch 132 | MSE Loss: 3.2748\n",
      "Training set | Epoch 133 | MSE Loss: 3.0736\n",
      "Training set | Epoch 134 | MSE Loss: 2.9162\n",
      "Training set | Epoch 135 | MSE Loss: 3.0687\n",
      "Training set | Epoch 136 | MSE Loss: 2.8309\n",
      "Training set | Epoch 137 | MSE Loss: 3.4552\n",
      "Training set | Epoch 138 | MSE Loss: 4.1696\n",
      "Training set | Epoch 139 | MSE Loss: 4.7003\n",
      "Training set | Epoch 140 | MSE Loss: 3.897\n",
      "Training set | Epoch 141 | MSE Loss: 4.528\n",
      "Training set | Epoch 142 | MSE Loss: 3.7945\n",
      "Training set | Epoch 143 | MSE Loss: 3.3004\n",
      "Training set | Epoch 144 | MSE Loss: 3.0374\n",
      "Training set | Epoch 145 | MSE Loss: 3.5511\n",
      "Training set | Epoch 146 | MSE Loss: 3.4843\n",
      "Training set | Epoch 147 | MSE Loss: 2.9927\n",
      "Training set | Epoch 148 | MSE Loss: 2.8263\n",
      "Training set | Epoch 149 | MSE Loss: 3.2495\n",
      "Training set | Epoch 150 | MSE Loss: 3.2187\n",
      "Training set | Epoch 151 | MSE Loss: 3.1683\n",
      "Training set | Epoch 152 | MSE Loss: 4.477\n",
      "Training set | Epoch 153 | MSE Loss: 4.8005\n",
      "Training set | Epoch 154 | MSE Loss: 3.725\n",
      "Training set | Epoch 155 | MSE Loss: 4.3409\n",
      "Training set | Epoch 156 | MSE Loss: 4.2919\n",
      "Training set | Epoch 157 | MSE Loss: 3.6104\n",
      "Training set | Epoch 158 | MSE Loss: 4.5849\n",
      "Training set | Epoch 159 | MSE Loss: 3.7463\n",
      "Training set | Epoch 160 | MSE Loss: 3.0924\n",
      "Training set | Epoch 161 | MSE Loss: 3.5235\n",
      "Training set | Epoch 162 | MSE Loss: 3.0657\n",
      "Training set | Epoch 163 | MSE Loss: 3.8848\n",
      "Training set | Epoch 164 | MSE Loss: 3.3224\n",
      "Training set | Epoch 165 | MSE Loss: 2.8538\n",
      "Training set | Epoch 166 | MSE Loss: 2.9415\n",
      "Training set | Epoch 167 | MSE Loss: 2.5049\n",
      "Training set | Epoch 168 | MSE Loss: 2.8806\n",
      "Training set | Epoch 169 | MSE Loss: 2.6176\n",
      "Training set | Epoch 170 | MSE Loss: 2.1709\n",
      "Training set | Epoch 171 | MSE Loss: 2.3556\n",
      "Training set | Epoch 172 | MSE Loss: 2.0575\n",
      "Training set | Epoch 173 | MSE Loss: 2.2759\n",
      "Training set | Epoch 174 | MSE Loss: 2.1938\n",
      "Training set | Epoch 175 | MSE Loss: 2.0532\n",
      "Training set | Epoch 176 | MSE Loss: 2.2144\n",
      "Training set | Epoch 177 | MSE Loss: 1.9534\n",
      "Training set | Epoch 178 | MSE Loss: 2.4681\n",
      "Training set | Epoch 179 | MSE Loss: 2.1793\n",
      "Training set | Epoch 180 | MSE Loss: 1.8956\n",
      "Training set | Epoch 181 | MSE Loss: 2.1387\n",
      "Training set | Epoch 182 | MSE Loss: 1.8216\n",
      "Training set | Epoch 183 | MSE Loss: 2.0109\n",
      "Training set | Epoch 184 | MSE Loss: 1.933\n",
      "Training set | Epoch 185 | MSE Loss: 1.6602\n",
      "Training set | Epoch 186 | MSE Loss: 1.8767\n",
      "Training set | Epoch 187 | MSE Loss: 1.7874\n",
      "Training set | Epoch 188 | MSE Loss: 1.8098\n",
      "Training set | Epoch 189 | MSE Loss: 2.0843\n",
      "Training set | Epoch 190 | MSE Loss: 1.7839\n",
      "Training set | Epoch 191 | MSE Loss: 2.0095\n",
      "Training set | Epoch 192 | MSE Loss: 2.0896\n",
      "Training set | Epoch 193 | MSE Loss: 2.0077\n",
      "Training set | Epoch 194 | MSE Loss: 2.2582\n",
      "Training set | Epoch 195 | MSE Loss: 2.1715\n",
      "Training set | Epoch 196 | MSE Loss: 1.834\n",
      "Training set | Epoch 197 | MSE Loss: 2.203\n",
      "Training set | Epoch 198 | MSE Loss: 1.9259\n",
      "Training set | Epoch 199 | MSE Loss: 1.9764\n",
      "Training set | Epoch 200 | MSE Loss: 2.0082\n",
      "Training set | Epoch 201 | MSE Loss: 1.8031\n",
      "Training set | Epoch 202 | MSE Loss: 1.8912\n",
      "Training set | Epoch 203 | MSE Loss: 2.1914\n",
      "Training set | Epoch 204 | MSE Loss: 1.7422\n",
      "Training set | Epoch 205 | MSE Loss: 2.2751\n",
      "Training set | Epoch 206 | MSE Loss: 2.4734\n",
      "Training set | Epoch 207 | MSE Loss: 1.9073\n",
      "Training set | Epoch 208 | MSE Loss: 2.8655\n",
      "Training set | Epoch 209 | MSE Loss: 2.5163\n",
      "Training set | Epoch 210 | MSE Loss: 2.4486\n",
      "Training set | Epoch 211 | MSE Loss: 2.5613\n",
      "Training set | Epoch 212 | MSE Loss: 2.1424\n",
      "Training set | Epoch 213 | MSE Loss: 2.1927\n",
      "Training set | Epoch 214 | MSE Loss: 2.3199\n",
      "Training set | Epoch 215 | MSE Loss: 1.7888\n",
      "Training set | Epoch 216 | MSE Loss: 2.0316\n",
      "Training set | Epoch 217 | MSE Loss: 1.812\n",
      "Training set | Epoch 218 | MSE Loss: 1.5544\n",
      "Training set | Epoch 219 | MSE Loss: 1.7851\n",
      "Training set | Epoch 220 | MSE Loss: 1.5119\n",
      "Training set | Epoch 221 | MSE Loss: 1.9166\n",
      "Training set | Epoch 222 | MSE Loss: 1.6727\n",
      "Training set | Epoch 223 | MSE Loss: 1.5731\n",
      "Training set | Epoch 224 | MSE Loss: 2.0013\n",
      "Training set | Epoch 225 | MSE Loss: 1.6491\n",
      "Training set | Epoch 226 | MSE Loss: 2.2919\n",
      "Training set | Epoch 227 | MSE Loss: 2.0179\n",
      "Training set | Epoch 228 | MSE Loss: 1.7309\n",
      "Training set | Epoch 229 | MSE Loss: 2.2609\n",
      "Training set | Epoch 230 | MSE Loss: 1.7341\n",
      "Training set | Epoch 231 | MSE Loss: 2.4179\n",
      "Training set | Epoch 232 | MSE Loss: 1.9784\n",
      "Training set | Epoch 233 | MSE Loss: 1.7168\n",
      "Training set | Epoch 234 | MSE Loss: 1.8563\n",
      "Training set | Epoch 235 | MSE Loss: 1.5046\n",
      "Training set | Epoch 236 | MSE Loss: 1.8814\n",
      "Training set | Epoch 237 | MSE Loss: 1.591\n",
      "Training set | Epoch 238 | MSE Loss: 1.8746\n",
      "Training set | Epoch 239 | MSE Loss: 2.0487\n",
      "Training set | Epoch 240 | MSE Loss: 2.0765\n",
      "Training set | Epoch 241 | MSE Loss: 2.803\n",
      "Training set | Epoch 242 | MSE Loss: 2.0841\n",
      "Training set | Epoch 243 | MSE Loss: 2.3351\n",
      "Training set | Epoch 244 | MSE Loss: 2.0027\n",
      "Training set | Epoch 245 | MSE Loss: 1.7304\n",
      "Training set | Epoch 246 | MSE Loss: 1.8303\n",
      "Training set | Epoch 247 | MSE Loss: 1.487\n",
      "Training set | Epoch 248 | MSE Loss: 1.4253\n",
      "Training set | Epoch 249 | MSE Loss: 1.3405\n",
      "Training set | Epoch 250 | MSE Loss: 1.3096\n",
      "Training set | Epoch 251 | MSE Loss: 1.4054\n",
      "Training set | Epoch 252 | MSE Loss: 1.1599\n",
      "Training set | Epoch 253 | MSE Loss: 1.403\n",
      "Training set | Epoch 254 | MSE Loss: 1.2285\n",
      "Training set | Epoch 255 | MSE Loss: 1.3504\n",
      "Training set | Epoch 256 | MSE Loss: 1.3875\n",
      "Training set | Epoch 257 | MSE Loss: 1.1203\n",
      "Training set | Epoch 258 | MSE Loss: 1.3585\n",
      "Training set | Epoch 259 | MSE Loss: 1.1369\n",
      "Training set | Epoch 260 | MSE Loss: 1.26\n",
      "Training set | Epoch 261 | MSE Loss: 1.3996\n",
      "Training set | Epoch 262 | MSE Loss: 1.0753\n",
      "Training set | Epoch 263 | MSE Loss: 1.3686\n",
      "Training set | Epoch 264 | MSE Loss: 1.2342\n",
      "Training set | Epoch 265 | MSE Loss: 1.2883\n",
      "Training set | Epoch 266 | MSE Loss: 1.4823\n",
      "Training set | Epoch 267 | MSE Loss: 1.0894\n",
      "Training set | Epoch 268 | MSE Loss: 1.319\n",
      "Training set | Epoch 269 | MSE Loss: 1.1556\n",
      "Training set | Epoch 270 | MSE Loss: 1.1652\n",
      "Training set | Epoch 271 | MSE Loss: 1.339\n",
      "Training set | Epoch 272 | MSE Loss: 1.0199\n",
      "Training set | Epoch 273 | MSE Loss: 1.3247\n",
      "Training set | Epoch 274 | MSE Loss: 1.1878\n",
      "Training set | Epoch 275 | MSE Loss: 1.2589\n",
      "Training set | Epoch 276 | MSE Loss: 1.59\n",
      "Training set | Epoch 277 | MSE Loss: 1.2506\n",
      "Training set | Epoch 278 | MSE Loss: 1.8258\n",
      "Training set | Epoch 279 | MSE Loss: 1.5354\n",
      "Training set | Epoch 280 | MSE Loss: 1.6991\n",
      "Training set | Epoch 281 | MSE Loss: 1.8784\n",
      "Training set | Epoch 282 | MSE Loss: 1.4005\n",
      "Training set | Epoch 283 | MSE Loss: 1.7684\n",
      "Training set | Epoch 284 | MSE Loss: 1.4783\n",
      "Training set | Epoch 285 | MSE Loss: 1.6225\n",
      "Training set | Epoch 286 | MSE Loss: 1.8237\n",
      "Training set | Epoch 287 | MSE Loss: 1.4183\n",
      "Training set | Epoch 288 | MSE Loss: 1.8631\n",
      "Training set | Epoch 289 | MSE Loss: 1.5378\n",
      "Training set | Epoch 290 | MSE Loss: 1.8454\n",
      "Training set | Epoch 291 | MSE Loss: 1.5964\n",
      "Training set | Epoch 292 | MSE Loss: 1.4501\n",
      "Training set | Epoch 293 | MSE Loss: 1.6977\n",
      "Training set | Epoch 294 | MSE Loss: 1.3414\n",
      "Training set | Epoch 295 | MSE Loss: 1.4665\n",
      "Training set | Epoch 296 | MSE Loss: 1.4845\n",
      "Training set | Epoch 297 | MSE Loss: 1.7116\n",
      "Training set | Epoch 298 | MSE Loss: 1.6797\n",
      "Training set | Epoch 299 | MSE Loss: 1.4843\n",
      "Training set | Epoch 300 | MSE Loss: 1.4364\n",
      "Training set | Epoch 301 | MSE Loss: 1.2781\n",
      "Training set | Epoch 302 | MSE Loss: 1.18\n",
      "Training set | Epoch 303 | MSE Loss: 1.0827\n",
      "Training set | Epoch 304 | MSE Loss: 0.8551\n",
      "Training set | Epoch 305 | MSE Loss: 0.773\n",
      "Training set | Epoch 306 | MSE Loss: 0.8516\n",
      "Training set | Epoch 307 | MSE Loss: 0.7296\n",
      "Training set | Epoch 308 | MSE Loss: 0.8572\n",
      "Training set | Epoch 309 | MSE Loss: 0.9962\n",
      "Training set | Epoch 310 | MSE Loss: 0.8415\n",
      "Training set | Epoch 311 | MSE Loss: 1.2009\n",
      "Training set | Epoch 312 | MSE Loss: 1.0629\n",
      "Training set | Epoch 313 | MSE Loss: 1.1678\n",
      "Training set | Epoch 314 | MSE Loss: 1.5806\n",
      "Training set | Epoch 315 | MSE Loss: 1.3176\n",
      "Training set | Epoch 316 | MSE Loss: 1.8481\n",
      "Training set | Epoch 317 | MSE Loss: 1.6981\n",
      "Training set | Epoch 318 | MSE Loss: 1.6236\n",
      "Training set | Epoch 319 | MSE Loss: 1.8661\n",
      "Training set | Epoch 320 | MSE Loss: 1.507\n",
      "Training set | Epoch 321 | MSE Loss: 1.6832\n",
      "Training set | Epoch 322 | MSE Loss: 1.9164\n",
      "Training set | Epoch 323 | MSE Loss: 1.5457\n",
      "Training set | Epoch 324 | MSE Loss: 2.1195\n",
      "Training set | Epoch 325 | MSE Loss: 1.9162\n",
      "Training set | Epoch 326 | MSE Loss: 2.3096\n",
      "Training set | Epoch 327 | MSE Loss: 2.0368\n",
      "Training set | Epoch 328 | MSE Loss: 2.0029\n",
      "Training set | Epoch 329 | MSE Loss: 1.8438\n",
      "Training set | Epoch 330 | MSE Loss: 2.2258\n",
      "Training set | Epoch 331 | MSE Loss: 1.6258\n",
      "Training set | Epoch 332 | MSE Loss: 1.6401\n",
      "Training set | Epoch 333 | MSE Loss: 1.6568\n",
      "Training set | Epoch 334 | MSE Loss: 1.2303\n",
      "Training set | Epoch 335 | MSE Loss: 1.4866\n",
      "Training set | Epoch 336 | MSE Loss: 1.4184\n",
      "Training set | Epoch 337 | MSE Loss: 1.3353\n",
      "Training set | Epoch 338 | MSE Loss: 1.7237\n",
      "Training set | Epoch 339 | MSE Loss: 1.2607\n",
      "Training set | Epoch 340 | MSE Loss: 1.5068\n",
      "Training set | Epoch 341 | MSE Loss: 1.44\n",
      "Training set | Epoch 342 | MSE Loss: 1.3782\n",
      "Training set | Epoch 343 | MSE Loss: 1.8256\n",
      "Training set | Epoch 344 | MSE Loss: 1.2745\n",
      "Training set | Epoch 345 | MSE Loss: 1.7015\n",
      "Training set | Epoch 346 | MSE Loss: 1.4002\n",
      "Training set | Epoch 347 | MSE Loss: 1.6995\n",
      "Training set | Epoch 348 | MSE Loss: 1.7956\n",
      "Training set | Epoch 349 | MSE Loss: 1.5898\n",
      "Training set | Epoch 350 | MSE Loss: 1.7862\n",
      "Training set | Epoch 351 | MSE Loss: 1.4551\n",
      "Training set | Epoch 352 | MSE Loss: 1.8254\n",
      "Training set | Epoch 353 | MSE Loss: 1.4145\n",
      "Training set | Epoch 354 | MSE Loss: 1.4967\n",
      "Training set | Epoch 355 | MSE Loss: 1.573\n",
      "Training set | Epoch 356 | MSE Loss: 1.2632\n",
      "Training set | Epoch 357 | MSE Loss: 1.8499\n",
      "Training set | Epoch 358 | MSE Loss: 1.2517\n",
      "Training set | Epoch 359 | MSE Loss: 1.607\n",
      "Training set | Epoch 360 | MSE Loss: 1.1458\n",
      "Training set | Epoch 361 | MSE Loss: 1.3775\n",
      "Training set | Epoch 362 | MSE Loss: 1.2294\n",
      "Training set | Epoch 363 | MSE Loss: 1.3397\n",
      "Training set | Epoch 364 | MSE Loss: 1.3907\n",
      "Training set | Epoch 365 | MSE Loss: 1.3241\n",
      "Training set | Epoch 366 | MSE Loss: 1.5893\n",
      "Training set | Epoch 367 | MSE Loss: 1.1114\n",
      "Training set | Epoch 368 | MSE Loss: 1.5074\n",
      "Training set | Epoch 369 | MSE Loss: 1.1247\n",
      "Training set | Epoch 370 | MSE Loss: 1.5186\n",
      "Training set | Epoch 371 | MSE Loss: 1.0728\n",
      "Training set | Epoch 372 | MSE Loss: 1.408\n",
      "Training set | Epoch 373 | MSE Loss: 1.1371\n",
      "Training set | Epoch 374 | MSE Loss: 1.3565\n",
      "Training set | Epoch 375 | MSE Loss: 1.1608\n",
      "Training set | Epoch 376 | MSE Loss: 1.1666\n",
      "Training set | Epoch 377 | MSE Loss: 1.1305\n",
      "Training set | Epoch 378 | MSE Loss: 0.9912\n",
      "Training set | Epoch 379 | MSE Loss: 1.0156\n",
      "Training set | Epoch 380 | MSE Loss: 0.8596\n",
      "Training set | Epoch 381 | MSE Loss: 1.1706\n",
      "Training set | Epoch 382 | MSE Loss: 0.793\n",
      "Training set | Epoch 383 | MSE Loss: 1.0993\n",
      "Training set | Epoch 384 | MSE Loss: 0.7676\n",
      "Training set | Epoch 385 | MSE Loss: 1.0701\n",
      "Training set | Epoch 386 | MSE Loss: 0.8875\n",
      "Training set | Epoch 387 | MSE Loss: 1.1276\n",
      "Training set | Epoch 388 | MSE Loss: 1.1686\n",
      "Training set | Epoch 389 | MSE Loss: 1.0494\n",
      "Training set | Epoch 390 | MSE Loss: 1.127\n",
      "Training set | Epoch 391 | MSE Loss: 0.8028\n",
      "Training set | Epoch 392 | MSE Loss: 1.1514\n",
      "Training set | Epoch 393 | MSE Loss: 0.8456\n",
      "Training set | Epoch 394 | MSE Loss: 1.1963\n",
      "Training set | Epoch 395 | MSE Loss: 1.116\n",
      "Training set | Epoch 396 | MSE Loss: 1.2244\n",
      "Training set | Epoch 397 | MSE Loss: 1.2954\n",
      "Training set | Epoch 398 | MSE Loss: 1.0264\n",
      "Training set | Epoch 399 | MSE Loss: 1.2527\n",
      "Training set | Epoch 400 | MSE Loss: 0.8942\n",
      "Training set | Epoch 401 | MSE Loss: 1.325\n",
      "Training set | Epoch 402 | MSE Loss: 1.3533\n",
      "Training set | Epoch 403 | MSE Loss: 1.6176\n",
      "Training set | Epoch 404 | MSE Loss: 1.8178\n",
      "Training set | Epoch 405 | MSE Loss: 1.4897\n",
      "Training set | Epoch 406 | MSE Loss: 1.5802\n",
      "Training set | Epoch 407 | MSE Loss: 1.2473\n",
      "Training set | Epoch 408 | MSE Loss: 0.9206\n",
      "Training set | Epoch 409 | MSE Loss: 0.9996\n",
      "Training set | Epoch 410 | MSE Loss: 0.7826\n",
      "Training set | Epoch 411 | MSE Loss: 0.6418\n",
      "Training set | Epoch 412 | MSE Loss: 0.6161\n",
      "Training set | Epoch 413 | MSE Loss: 0.5234\n",
      "Training set | Epoch 414 | MSE Loss: 0.66\n",
      "Training set | Epoch 415 | MSE Loss: 0.5512\n",
      "Training set | Epoch 416 | MSE Loss: 0.7345\n",
      "Training set | Epoch 417 | MSE Loss: 0.7486\n",
      "Training set | Epoch 418 | MSE Loss: 0.7016\n",
      "Training set | Epoch 419 | MSE Loss: 1.1752\n",
      "Training set | Epoch 420 | MSE Loss: 0.8763\n",
      "Training set | Epoch 421 | MSE Loss: 1.3715\n",
      "Training set | Epoch 422 | MSE Loss: 1.2035\n",
      "Training set | Epoch 423 | MSE Loss: 1.201\n",
      "Training set | Epoch 424 | MSE Loss: 1.5291\n",
      "Training set | Epoch 425 | MSE Loss: 1.1895\n",
      "Training set | Epoch 426 | MSE Loss: 1.8646\n",
      "Training set | Epoch 427 | MSE Loss: 1.2136\n",
      "Training set | Epoch 428 | MSE Loss: 1.6763\n",
      "Training set | Epoch 429 | MSE Loss: 1.6205\n",
      "Training set | Epoch 430 | MSE Loss: 1.5259\n",
      "Training set | Epoch 431 | MSE Loss: 1.6011\n",
      "Training set | Epoch 432 | MSE Loss: 1.3486\n",
      "Training set | Epoch 433 | MSE Loss: 1.3305\n",
      "Training set | Epoch 434 | MSE Loss: 1.2335\n",
      "Training set | Epoch 435 | MSE Loss: 1.099\n",
      "Training set | Epoch 436 | MSE Loss: 1.0601\n",
      "Training set | Epoch 437 | MSE Loss: 0.7558\n",
      "Training set | Epoch 438 | MSE Loss: 0.7676\n",
      "Training set | Epoch 439 | MSE Loss: 0.587\n",
      "Training set | Epoch 440 | MSE Loss: 0.5095\n",
      "Training set | Epoch 441 | MSE Loss: 0.5998\n",
      "Training set | Epoch 442 | MSE Loss: 0.5094\n",
      "Training set | Epoch 443 | MSE Loss: 0.6627\n",
      "Training set | Epoch 444 | MSE Loss: 0.7261\n",
      "Training set | Epoch 445 | MSE Loss: 0.7262\n",
      "Training set | Epoch 446 | MSE Loss: 1.1076\n",
      "Training set | Epoch 447 | MSE Loss: 0.8215\n",
      "Training set | Epoch 448 | MSE Loss: 1.3693\n",
      "Training set | Epoch 449 | MSE Loss: 1.1406\n",
      "Training set | Epoch 450 | MSE Loss: 1.3341\n",
      "Training set | Epoch 451 | MSE Loss: 1.5765\n",
      "Training set | Epoch 452 | MSE Loss: 1.2631\n",
      "Training set | Epoch 453 | MSE Loss: 1.3811\n",
      "Training set | Epoch 454 | MSE Loss: 1.2874\n",
      "Training set | Epoch 455 | MSE Loss: 1.5767\n",
      "Training set | Epoch 456 | MSE Loss: 1.9308\n",
      "Training set | Epoch 457 | MSE Loss: 1.5161\n",
      "Training set | Epoch 458 | MSE Loss: 1.5612\n",
      "Training set | Epoch 459 | MSE Loss: 1.2639\n",
      "Training set | Epoch 460 | MSE Loss: 1.0797\n",
      "Training set | Epoch 461 | MSE Loss: 1.1138\n",
      "Training set | Epoch 462 | MSE Loss: 0.879\n",
      "Training set | Epoch 463 | MSE Loss: 1.1789\n",
      "Training set | Epoch 464 | MSE Loss: 0.9909\n",
      "Training set | Epoch 465 | MSE Loss: 1.0826\n",
      "Training set | Epoch 466 | MSE Loss: 1.3338\n",
      "Training set | Epoch 467 | MSE Loss: 1.0027\n",
      "Training set | Epoch 468 | MSE Loss: 1.4226\n",
      "Training set | Epoch 469 | MSE Loss: 1.1072\n",
      "Training set | Epoch 470 | MSE Loss: 1.2915\n",
      "Training set | Epoch 471 | MSE Loss: 1.1299\n",
      "Training set | Epoch 472 | MSE Loss: 0.9019\n",
      "Training set | Epoch 473 | MSE Loss: 1.1513\n",
      "Training set | Epoch 474 | MSE Loss: 0.909\n",
      "Training set | Epoch 475 | MSE Loss: 1.2441\n",
      "Training set | Epoch 476 | MSE Loss: 1.0679\n",
      "Training set | Epoch 477 | MSE Loss: 1.2556\n",
      "Training set | Epoch 478 | MSE Loss: 1.4389\n",
      "Training set | Epoch 479 | MSE Loss: 1.0874\n",
      "Training set | Epoch 480 | MSE Loss: 1.5654\n",
      "Training set | Epoch 481 | MSE Loss: 1.7073\n",
      "Training set | Epoch 482 | MSE Loss: 1.292\n",
      "Training set | Epoch 483 | MSE Loss: 1.5314\n",
      "Training set | Epoch 484 | MSE Loss: 1.7837\n",
      "Training set | Epoch 485 | MSE Loss: 2.4005\n",
      "Training set | Epoch 486 | MSE Loss: 2.082\n",
      "Training set | Epoch 487 | MSE Loss: 2.2118\n",
      "Training set | Epoch 488 | MSE Loss: 1.5957\n",
      "Training set | Epoch 489 | MSE Loss: 1.3266\n",
      "Training set | Epoch 490 | MSE Loss: 1.0498\n",
      "Training set | Epoch 491 | MSE Loss: 0.7089\n",
      "Training set | Epoch 492 | MSE Loss: 0.569\n",
      "Training set | Epoch 493 | MSE Loss: 0.4567\n",
      "Training set | Epoch 494 | MSE Loss: 0.3665\n",
      "Training set | Epoch 495 | MSE Loss: 0.314\n",
      "Training set | Epoch 496 | MSE Loss: 0.281\n",
      "Training set | Epoch 497 | MSE Loss: 0.2596\n",
      "Training set | Epoch 498 | MSE Loss: 0.2531\n",
      "Training set | Epoch 499 | MSE Loss: 0.2403\n",
      "Training set | Epoch 500 | MSE Loss: 0.2962\n",
      "Training set | Epoch 501 | MSE Loss: 0.3285\n",
      "Training set | Epoch 502 | MSE Loss: 0.3092\n",
      "Training set | Epoch 503 | MSE Loss: 0.5255\n",
      "Training set | Epoch 504 | MSE Loss: 0.6453\n",
      "Training set | Epoch 505 | MSE Loss: 0.6374\n",
      "Training set | Epoch 506 | MSE Loss: 1.1652\n",
      "Training set | Epoch 507 | MSE Loss: 1.4967\n",
      "Training set | Epoch 508 | MSE Loss: 1.2643\n",
      "Training set | Epoch 509 | MSE Loss: 2.4966\n",
      "Training set | Epoch 510 | MSE Loss: 2.3534\n",
      "Training set | Epoch 511 | MSE Loss: 2.0658\n",
      "Training set | Epoch 512 | MSE Loss: 2.0703\n",
      "Training set | Epoch 513 | MSE Loss: 2.1027\n",
      "Training set | Epoch 514 | MSE Loss: 1.6156\n",
      "Training set | Epoch 515 | MSE Loss: 2.5146\n",
      "Training set | Epoch 516 | MSE Loss: 1.571\n",
      "Training set | Epoch 517 | MSE Loss: 1.7733\n",
      "Training set | Epoch 518 | MSE Loss: 1.4402\n",
      "Training set | Epoch 519 | MSE Loss: 1.1798\n",
      "Training set | Epoch 520 | MSE Loss: 1.5772\n",
      "Training set | Epoch 521 | MSE Loss: 1.1376\n",
      "Training set | Epoch 522 | MSE Loss: 1.6131\n",
      "Training set | Epoch 523 | MSE Loss: 1.3051\n",
      "Training set | Epoch 524 | MSE Loss: 1.5318\n",
      "Training set | Epoch 525 | MSE Loss: 1.5788\n",
      "Training set | Epoch 526 | MSE Loss: 1.2356\n",
      "Training set | Epoch 527 | MSE Loss: 1.7736\n",
      "Training set | Epoch 528 | MSE Loss: 1.2844\n",
      "Training set | Epoch 529 | MSE Loss: 1.8676\n",
      "Training set | Epoch 530 | MSE Loss: 1.4003\n",
      "Training set | Epoch 531 | MSE Loss: 1.5014\n",
      "Training set | Epoch 532 | MSE Loss: 1.6799\n",
      "Training set | Epoch 533 | MSE Loss: 1.6617\n",
      "Training set | Epoch 534 | MSE Loss: 1.9271\n",
      "Training set | Epoch 535 | MSE Loss: 1.4048\n",
      "Training set | Epoch 536 | MSE Loss: 1.3724\n",
      "Training set | Epoch 537 | MSE Loss: 1.0274\n",
      "Training set | Epoch 538 | MSE Loss: 0.7422\n",
      "Training set | Epoch 539 | MSE Loss: 0.7892\n",
      "Training set | Epoch 540 | MSE Loss: 0.7565\n",
      "Training set | Epoch 541 | MSE Loss: 0.8287\n",
      "Training set | Epoch 542 | MSE Loss: 0.5693\n",
      "Training set | Epoch 543 | MSE Loss: 0.6163\n",
      "Training set | Epoch 544 | MSE Loss: 0.5702\n",
      "Training set | Epoch 545 | MSE Loss: 0.5388\n",
      "Training set | Epoch 546 | MSE Loss: 0.7521\n",
      "Training set | Epoch 547 | MSE Loss: 0.6167\n",
      "Training set | Epoch 548 | MSE Loss: 1.0351\n",
      "Training set | Epoch 549 | MSE Loss: 0.7542\n",
      "Training set | Epoch 550 | MSE Loss: 1.1423\n",
      "Training set | Epoch 551 | MSE Loss: 0.9084\n",
      "Training set | Epoch 552 | MSE Loss: 1.1486\n",
      "Training set | Epoch 553 | MSE Loss: 1.0977\n",
      "Training set | Epoch 554 | MSE Loss: 1.0041\n",
      "Training set | Epoch 555 | MSE Loss: 1.3745\n",
      "Training set | Epoch 556 | MSE Loss: 0.878\n",
      "Training set | Epoch 557 | MSE Loss: 0.9487\n",
      "Training set | Epoch 558 | MSE Loss: 0.6993\n",
      "Training set | Epoch 559 | MSE Loss: 0.8504\n",
      "Training set | Epoch 560 | MSE Loss: 0.6194\n",
      "Training set | Epoch 561 | MSE Loss: 0.9302\n",
      "Training set | Epoch 562 | MSE Loss: 0.6989\n",
      "Training set | Epoch 563 | MSE Loss: 0.8912\n",
      "Training set | Epoch 564 | MSE Loss: 0.6839\n",
      "Training set | Epoch 565 | MSE Loss: 0.7604\n",
      "Training set | Epoch 566 | MSE Loss: 0.5965\n",
      "Training set | Epoch 567 | MSE Loss: 0.7576\n",
      "Training set | Epoch 568 | MSE Loss: 0.6689\n",
      "Training set | Epoch 569 | MSE Loss: 0.8856\n",
      "Training set | Epoch 570 | MSE Loss: 0.7772\n",
      "Training set | Epoch 571 | MSE Loss: 0.8807\n",
      "Training set | Epoch 572 | MSE Loss: 0.8461\n",
      "Training set | Epoch 573 | MSE Loss: 0.8223\n",
      "Training set | Epoch 574 | MSE Loss: 0.9542\n",
      "Training set | Epoch 575 | MSE Loss: 0.7574\n",
      "Training set | Epoch 576 | MSE Loss: 0.9437\n",
      "Training set | Epoch 577 | MSE Loss: 0.7367\n",
      "Training set | Epoch 578 | MSE Loss: 1.0582\n",
      "Training set | Epoch 579 | MSE Loss: 0.8405\n",
      "Training set | Epoch 580 | MSE Loss: 1.0793\n",
      "Training set | Epoch 581 | MSE Loss: 0.7788\n",
      "Training set | Epoch 582 | MSE Loss: 1.1516\n",
      "Training set | Epoch 583 | MSE Loss: 1.0148\n",
      "Training set | Epoch 584 | MSE Loss: 1.0588\n",
      "Training set | Epoch 585 | MSE Loss: 0.9193\n",
      "Training set | Epoch 586 | MSE Loss: 0.92\n",
      "Training set | Epoch 587 | MSE Loss: 0.7414\n",
      "Training set | Epoch 588 | MSE Loss: 0.7265\n",
      "Training set | Epoch 589 | MSE Loss: 0.5677\n",
      "Training set | Epoch 590 | MSE Loss: 0.4769\n",
      "Training set | Epoch 591 | MSE Loss: 0.5279\n",
      "Training set | Epoch 592 | MSE Loss: 0.4008\n",
      "Training set | Epoch 593 | MSE Loss: 0.5055\n",
      "Training set | Epoch 594 | MSE Loss: 0.3763\n",
      "Training set | Epoch 595 | MSE Loss: 0.5533\n",
      "Training set | Epoch 596 | MSE Loss: 0.6113\n",
      "Training set | Epoch 597 | MSE Loss: 0.6966\n",
      "Training set | Epoch 598 | MSE Loss: 0.9788\n",
      "Training set | Epoch 599 | MSE Loss: 0.7348\n",
      "Training set | Epoch 600 | MSE Loss: 1.1615\n",
      "Training set | Epoch 601 | MSE Loss: 0.7465\n",
      "Training set | Epoch 602 | MSE Loss: 1.0298\n",
      "Training set | Epoch 603 | MSE Loss: 0.8548\n",
      "Training set | Epoch 604 | MSE Loss: 1.1252\n",
      "Training set | Epoch 605 | MSE Loss: 0.9249\n",
      "Training set | Epoch 606 | MSE Loss: 1.2633\n",
      "Training set | Epoch 607 | MSE Loss: 1.2565\n",
      "Training set | Epoch 608 | MSE Loss: 1.1854\n",
      "Training set | Epoch 609 | MSE Loss: 1.3183\n",
      "Training set | Epoch 610 | MSE Loss: 1.1063\n",
      "Training set | Epoch 611 | MSE Loss: 1.1245\n",
      "Training set | Epoch 612 | MSE Loss: 0.922\n",
      "Training set | Epoch 613 | MSE Loss: 0.8402\n",
      "Training set | Epoch 614 | MSE Loss: 0.9025\n",
      "Training set | Epoch 615 | MSE Loss: 0.8676\n",
      "Training set | Epoch 616 | MSE Loss: 1.0652\n",
      "Training set | Epoch 617 | MSE Loss: 0.7397\n",
      "Training set | Epoch 618 | MSE Loss: 1.1462\n",
      "Training set | Epoch 619 | MSE Loss: 0.9\n",
      "Training set | Epoch 620 | MSE Loss: 1.0425\n",
      "Training set | Epoch 621 | MSE Loss: 1.0231\n",
      "Training set | Epoch 622 | MSE Loss: 0.9431\n",
      "Training set | Epoch 623 | MSE Loss: 1.3349\n",
      "Training set | Epoch 624 | MSE Loss: 0.8759\n",
      "Training set | Epoch 625 | MSE Loss: 1.2044\n",
      "Training set | Epoch 626 | MSE Loss: 2.6344\n",
      "Training set | Epoch 627 | MSE Loss: 2.3185\n",
      "Training set | Epoch 628 | MSE Loss: 1.9426\n",
      "Training set | Epoch 629 | MSE Loss: 1.4738\n",
      "Training set | Epoch 630 | MSE Loss: 1.1602\n",
      "Training set | Epoch 631 | MSE Loss: 1.1637\n",
      "Training set | Epoch 632 | MSE Loss: 1.2482\n",
      "Training set | Epoch 633 | MSE Loss: 1.102\n",
      "Training set | Epoch 634 | MSE Loss: 1.0852\n",
      "Training set | Epoch 635 | MSE Loss: 0.851\n",
      "Training set | Epoch 636 | MSE Loss: 0.7497\n",
      "Training set | Epoch 637 | MSE Loss: 1.0611\n",
      "Training set | Epoch 638 | MSE Loss: 0.878\n",
      "Training set | Epoch 639 | MSE Loss: 1.2977\n",
      "Training set | Epoch 640 | MSE Loss: 1.3791\n",
      "Training set | Epoch 641 | MSE Loss: 1.2201\n",
      "Training set | Epoch 642 | MSE Loss: 1.5857\n",
      "Training set | Epoch 643 | MSE Loss: 1.1318\n",
      "Training set | Epoch 644 | MSE Loss: 1.5278\n",
      "Training set | Epoch 645 | MSE Loss: 1.0994\n",
      "Training set | Epoch 646 | MSE Loss: 1.2387\n",
      "Training set | Epoch 647 | MSE Loss: 1.3265\n",
      "Training set | Epoch 648 | MSE Loss: 1.0006\n",
      "Training set | Epoch 649 | MSE Loss: 1.5685\n",
      "Training set | Epoch 650 | MSE Loss: 1.0769\n",
      "Training set | Epoch 651 | MSE Loss: 1.3814\n",
      "Training set | Epoch 652 | MSE Loss: 1.3252\n",
      "Training set | Epoch 653 | MSE Loss: 1.4434\n",
      "Training set | Epoch 654 | MSE Loss: 1.3132\n",
      "Training set | Epoch 655 | MSE Loss: 1.1391\n",
      "Training set | Epoch 656 | MSE Loss: 1.2913\n",
      "Training set | Epoch 657 | MSE Loss: 1.0423\n",
      "Training set | Epoch 658 | MSE Loss: 1.4372\n",
      "Training set | Epoch 659 | MSE Loss: 1.1012\n",
      "Training set | Epoch 660 | MSE Loss: 1.0707\n",
      "Training set | Epoch 661 | MSE Loss: 0.8712\n",
      "Training set | Epoch 662 | MSE Loss: 0.8225\n",
      "Training set | Epoch 663 | MSE Loss: 0.8477\n",
      "Training set | Epoch 664 | MSE Loss: 0.7548\n",
      "Training set | Epoch 665 | MSE Loss: 0.9649\n",
      "Training set | Epoch 666 | MSE Loss: 0.7522\n",
      "Training set | Epoch 667 | MSE Loss: 1.1817\n",
      "Training set | Epoch 668 | MSE Loss: 1.0235\n",
      "Training set | Epoch 669 | MSE Loss: 1.4741\n",
      "Training set | Epoch 670 | MSE Loss: 1.1835\n",
      "Training set | Epoch 671 | MSE Loss: 1.4413\n",
      "Training set | Epoch 672 | MSE Loss: 1.1811\n",
      "Training set | Epoch 673 | MSE Loss: 1.1419\n",
      "Training set | Epoch 674 | MSE Loss: 1.4631\n",
      "Training set | Epoch 675 | MSE Loss: 1.4183\n",
      "Training set | Epoch 676 | MSE Loss: 1.6063\n",
      "Training set | Epoch 677 | MSE Loss: 1.3337\n",
      "Training set | Epoch 678 | MSE Loss: 0.9605\n",
      "Training set | Epoch 679 | MSE Loss: 0.8874\n",
      "Training set | Epoch 680 | MSE Loss: 0.7925\n",
      "Training set | Epoch 681 | MSE Loss: 0.6983\n",
      "Training set | Epoch 682 | MSE Loss: 0.8977\n",
      "Training set | Epoch 683 | MSE Loss: 0.7207\n",
      "Training set | Epoch 684 | MSE Loss: 0.8978\n",
      "Training set | Epoch 685 | MSE Loss: 0.7467\n",
      "Training set | Epoch 686 | MSE Loss: 0.8715\n",
      "Training set | Epoch 687 | MSE Loss: 0.8793\n",
      "Training set | Epoch 688 | MSE Loss: 0.9312\n",
      "Training set | Epoch 689 | MSE Loss: 1.0314\n",
      "Training set | Epoch 690 | MSE Loss: 0.8149\n",
      "Training set | Epoch 691 | MSE Loss: 1.1372\n",
      "Training set | Epoch 692 | MSE Loss: 0.8757\n",
      "Training set | Epoch 693 | MSE Loss: 1.1242\n",
      "Training set | Epoch 694 | MSE Loss: 0.8082\n",
      "Training set | Epoch 695 | MSE Loss: 0.994\n",
      "Training set | Epoch 696 | MSE Loss: 0.7912\n",
      "Training set | Epoch 697 | MSE Loss: 0.9445\n",
      "Training set | Epoch 698 | MSE Loss: 1.1717\n",
      "Training set | Epoch 699 | MSE Loss: 1.6298\n",
      "Training set | Epoch 700 | MSE Loss: 1.3924\n",
      "Training set | Epoch 701 | MSE Loss: 1.1832\n",
      "Training set | Epoch 702 | MSE Loss: 1.0943\n",
      "Training set | Epoch 703 | MSE Loss: 0.7977\n",
      "Training set | Epoch 704 | MSE Loss: 0.8622\n",
      "Training set | Epoch 705 | MSE Loss: 0.6679\n",
      "Training set | Epoch 706 | MSE Loss: 0.7292\n",
      "Training set | Epoch 707 | MSE Loss: 0.4884\n",
      "Training set | Epoch 708 | MSE Loss: 0.6741\n",
      "Training set | Epoch 709 | MSE Loss: 0.6198\n",
      "Training set | Epoch 710 | MSE Loss: 1.0868\n",
      "Training set | Epoch 711 | MSE Loss: 0.9028\n",
      "Training set | Epoch 712 | MSE Loss: 1.086\n",
      "Training set | Epoch 713 | MSE Loss: 1.0587\n",
      "Training set | Epoch 714 | MSE Loss: 1.0725\n",
      "Training set | Epoch 715 | MSE Loss: 0.9876\n",
      "Training set | Epoch 716 | MSE Loss: 0.8105\n",
      "Training set | Epoch 717 | MSE Loss: 0.8946\n",
      "Training set | Epoch 718 | MSE Loss: 1.0123\n",
      "Training set | Epoch 719 | MSE Loss: 0.8698\n",
      "Training set | Epoch 720 | MSE Loss: 0.8739\n",
      "Training set | Epoch 721 | MSE Loss: 0.8099\n",
      "Training set | Epoch 722 | MSE Loss: 0.9103\n",
      "Training set | Epoch 723 | MSE Loss: 0.8473\n",
      "Training set | Epoch 724 | MSE Loss: 0.8373\n",
      "Training set | Epoch 725 | MSE Loss: 0.8837\n",
      "Training set | Epoch 726 | MSE Loss: 0.7607\n",
      "Training set | Epoch 727 | MSE Loss: 1.0934\n",
      "Training set | Epoch 728 | MSE Loss: 1.3343\n",
      "Training set | Epoch 729 | MSE Loss: 1.4949\n",
      "Training set | Epoch 730 | MSE Loss: 1.3773\n",
      "Training set | Epoch 731 | MSE Loss: 1.0797\n",
      "Training set | Epoch 732 | MSE Loss: 0.937\n",
      "Training set | Epoch 733 | MSE Loss: 0.7419\n",
      "Training set | Epoch 734 | MSE Loss: 0.6671\n",
      "Training set | Epoch 735 | MSE Loss: 0.5942\n",
      "Training set | Epoch 736 | MSE Loss: 0.4458\n",
      "Training set | Epoch 737 | MSE Loss: 0.6421\n",
      "Training set | Epoch 738 | MSE Loss: 0.4554\n",
      "Training set | Epoch 739 | MSE Loss: 0.6542\n",
      "Training set | Epoch 740 | MSE Loss: 0.6204\n",
      "Training set | Epoch 741 | MSE Loss: 0.7985\n",
      "Training set | Epoch 742 | MSE Loss: 0.7088\n",
      "Training set | Epoch 743 | MSE Loss: 0.8015\n",
      "Training set | Epoch 744 | MSE Loss: 0.8335\n",
      "Training set | Epoch 745 | MSE Loss: 0.8345\n",
      "Training set | Epoch 746 | MSE Loss: 0.8762\n",
      "Training set | Epoch 747 | MSE Loss: 0.6786\n",
      "Training set | Epoch 748 | MSE Loss: 0.911\n",
      "Training set | Epoch 749 | MSE Loss: 0.6991\n",
      "Training set | Epoch 750 | MSE Loss: 0.9141\n",
      "Training set | Epoch 751 | MSE Loss: 0.6108\n",
      "Training set | Epoch 752 | MSE Loss: 0.9139\n",
      "Training set | Epoch 753 | MSE Loss: 0.7177\n",
      "Training set | Epoch 754 | MSE Loss: 1.0449\n",
      "Training set | Epoch 755 | MSE Loss: 0.6912\n",
      "Training set | Epoch 756 | MSE Loss: 0.9702\n",
      "Training set | Epoch 757 | MSE Loss: 0.655\n",
      "Training set | Epoch 758 | MSE Loss: 0.8801\n",
      "Training set | Epoch 759 | MSE Loss: 0.5852\n",
      "Training set | Epoch 760 | MSE Loss: 0.8519\n",
      "Training set | Epoch 761 | MSE Loss: 0.6134\n",
      "Training set | Epoch 762 | MSE Loss: 0.8138\n",
      "Training set | Epoch 763 | MSE Loss: 0.5848\n",
      "Training set | Epoch 764 | MSE Loss: 0.6002\n",
      "Training set | Epoch 765 | MSE Loss: 0.58\n",
      "Training set | Epoch 766 | MSE Loss: 0.6725\n",
      "Training set | Epoch 767 | MSE Loss: 0.5723\n",
      "Training set | Epoch 768 | MSE Loss: 0.5827\n",
      "Training set | Epoch 769 | MSE Loss: 0.5687\n",
      "Training set | Epoch 770 | MSE Loss: 0.6601\n",
      "Training set | Epoch 771 | MSE Loss: 0.6732\n",
      "Training set | Epoch 772 | MSE Loss: 0.6215\n",
      "Training set | Epoch 773 | MSE Loss: 0.6874\n",
      "Training set | Epoch 774 | MSE Loss: 0.6286\n",
      "Training set | Epoch 775 | MSE Loss: 0.7133\n",
      "Training set | Epoch 776 | MSE Loss: 0.5665\n",
      "Training set | Epoch 777 | MSE Loss: 0.6739\n",
      "Training set | Epoch 778 | MSE Loss: 0.542\n",
      "Training set | Epoch 779 | MSE Loss: 0.6205\n",
      "Training set | Epoch 780 | MSE Loss: 0.5191\n",
      "Training set | Epoch 781 | MSE Loss: 0.6193\n",
      "Training set | Epoch 782 | MSE Loss: 0.6199\n",
      "Training set | Epoch 783 | MSE Loss: 0.7078\n",
      "Training set | Epoch 784 | MSE Loss: 0.5746\n",
      "Training set | Epoch 785 | MSE Loss: 0.6249\n",
      "Training set | Epoch 786 | MSE Loss: 0.6227\n",
      "Training set | Epoch 787 | MSE Loss: 0.6544\n",
      "Training set | Epoch 788 | MSE Loss: 0.6398\n",
      "Training set | Epoch 789 | MSE Loss: 0.6832\n",
      "Training set | Epoch 790 | MSE Loss: 0.6221\n",
      "Training set | Epoch 791 | MSE Loss: 0.6403\n",
      "Training set | Epoch 792 | MSE Loss: 0.4787\n",
      "Training set | Epoch 793 | MSE Loss: 0.5058\n",
      "Training set | Epoch 794 | MSE Loss: 0.4055\n",
      "Training set | Epoch 795 | MSE Loss: 0.5545\n",
      "Training set | Epoch 796 | MSE Loss: 0.4488\n",
      "Training set | Epoch 797 | MSE Loss: 0.5447\n",
      "Training set | Epoch 798 | MSE Loss: 0.5348\n",
      "Training set | Epoch 799 | MSE Loss: 0.756\n",
      "Training set | Epoch 800 | MSE Loss: 0.8808\n",
      "Training set | Epoch 801 | MSE Loss: 0.673\n",
      "Training set | Epoch 802 | MSE Loss: 0.8049\n",
      "Training set | Epoch 803 | MSE Loss: 0.6406\n",
      "Training set | Epoch 804 | MSE Loss: 0.8114\n",
      "Training set | Epoch 805 | MSE Loss: 0.6257\n",
      "Training set | Epoch 806 | MSE Loss: 0.6519\n",
      "Training set | Epoch 807 | MSE Loss: 0.5826\n",
      "Training set | Epoch 808 | MSE Loss: 0.6029\n",
      "Training set | Epoch 809 | MSE Loss: 0.721\n",
      "Training set | Epoch 810 | MSE Loss: 0.5805\n",
      "Training set | Epoch 811 | MSE Loss: 0.8154\n",
      "Training set | Epoch 812 | MSE Loss: 0.497\n",
      "Training set | Epoch 813 | MSE Loss: 0.6713\n",
      "Training set | Epoch 814 | MSE Loss: 0.5685\n",
      "Training set | Epoch 815 | MSE Loss: 0.5274\n",
      "Training set | Epoch 816 | MSE Loss: 0.6995\n",
      "Training set | Epoch 817 | MSE Loss: 0.5139\n",
      "Training set | Epoch 818 | MSE Loss: 0.8295\n",
      "Training set | Epoch 819 | MSE Loss: 0.5798\n",
      "Training set | Epoch 820 | MSE Loss: 0.7158\n",
      "Training set | Epoch 821 | MSE Loss: 0.7418\n",
      "Training set | Epoch 822 | MSE Loss: 0.7393\n",
      "Training set | Epoch 823 | MSE Loss: 0.9965\n",
      "Training set | Epoch 824 | MSE Loss: 0.7502\n",
      "Training set | Epoch 825 | MSE Loss: 0.854\n",
      "Training set | Epoch 826 | MSE Loss: 0.6703\n",
      "Training set | Epoch 827 | MSE Loss: 0.5784\n",
      "Training set | Epoch 828 | MSE Loss: 0.58\n",
      "Training set | Epoch 829 | MSE Loss: 0.448\n",
      "Training set | Epoch 830 | MSE Loss: 0.6143\n",
      "Training set | Epoch 831 | MSE Loss: 0.4687\n",
      "Training set | Epoch 832 | MSE Loss: 0.7473\n",
      "Training set | Epoch 833 | MSE Loss: 0.5645\n",
      "Training set | Epoch 834 | MSE Loss: 0.8492\n",
      "Training set | Epoch 835 | MSE Loss: 0.7006\n",
      "Training set | Epoch 836 | MSE Loss: 0.8217\n",
      "Training set | Epoch 837 | MSE Loss: 0.823\n",
      "Training set | Epoch 838 | MSE Loss: 0.6176\n",
      "Training set | Epoch 839 | MSE Loss: 0.8335\n",
      "Training set | Epoch 840 | MSE Loss: 0.8261\n",
      "Training set | Epoch 841 | MSE Loss: 0.9992\n",
      "Training set | Epoch 842 | MSE Loss: 1.8888\n",
      "Training set | Epoch 843 | MSE Loss: 2.4881\n",
      "Training set | Epoch 844 | MSE Loss: 3.0318\n",
      "Training set | Epoch 845 | MSE Loss: 3.3989\n",
      "Training set | Epoch 846 | MSE Loss: 4.0594\n",
      "Training set | Epoch 847 | MSE Loss: 3.8227\n",
      "Training set | Epoch 848 | MSE Loss: 3.7055\n",
      "Training set | Epoch 849 | MSE Loss: 3.6401\n",
      "Training set | Epoch 850 | MSE Loss: 3.6796\n",
      "Training set | Epoch 851 | MSE Loss: 3.5982\n",
      "Training set | Epoch 852 | MSE Loss: 3.5868\n",
      "Training set | Epoch 853 | MSE Loss: 3.1783\n",
      "Training set | Epoch 854 | MSE Loss: 2.6828\n",
      "Training set | Epoch 855 | MSE Loss: 3.0071\n",
      "Training set | Epoch 856 | MSE Loss: 2.6208\n",
      "Training set | Epoch 857 | MSE Loss: 2.7813\n",
      "Training set | Epoch 858 | MSE Loss: 2.6947\n",
      "Training set | Epoch 859 | MSE Loss: 2.8143\n",
      "Training set | Epoch 860 | MSE Loss: 2.8482\n",
      "Training set | Epoch 861 | MSE Loss: 2.6721\n",
      "Training set | Epoch 862 | MSE Loss: 2.9727\n",
      "Training set | Epoch 863 | MSE Loss: 3.1505\n",
      "Training set | Epoch 864 | MSE Loss: 2.2955\n",
      "Training set | Epoch 865 | MSE Loss: 2.6587\n",
      "Training set | Epoch 866 | MSE Loss: 3.2557\n",
      "Training set | Epoch 867 | MSE Loss: 2.3245\n",
      "Training set | Epoch 868 | MSE Loss: 3.1023\n",
      "Training set | Epoch 869 | MSE Loss: 2.7465\n",
      "Training set | Epoch 870 | MSE Loss: 3.1382\n",
      "Training set | Epoch 871 | MSE Loss: 2.9304\n",
      "Training set | Epoch 872 | MSE Loss: 2.4556\n",
      "Training set | Epoch 873 | MSE Loss: 2.1087\n",
      "Training set | Epoch 874 | MSE Loss: 1.5853\n",
      "Training set | Epoch 875 | MSE Loss: 1.7473\n",
      "Training set | Epoch 876 | MSE Loss: 1.0758\n",
      "Training set | Epoch 877 | MSE Loss: 1.1293\n",
      "Training set | Epoch 878 | MSE Loss: 0.7871\n",
      "Training set | Epoch 879 | MSE Loss: 0.8532\n",
      "Training set | Epoch 880 | MSE Loss: 0.5851\n",
      "Training set | Epoch 881 | MSE Loss: 0.5701\n",
      "Training set | Epoch 882 | MSE Loss: 0.4788\n",
      "Training set | Epoch 883 | MSE Loss: 0.4576\n",
      "Training set | Epoch 884 | MSE Loss: 0.4184\n",
      "Training set | Epoch 885 | MSE Loss: 0.3669\n",
      "Training set | Epoch 886 | MSE Loss: 0.3427\n",
      "Training set | Epoch 887 | MSE Loss: 0.3165\n",
      "Training set | Epoch 888 | MSE Loss: 0.2824\n",
      "Training set | Epoch 889 | MSE Loss: 0.2456\n",
      "Training set | Epoch 890 | MSE Loss: 0.2414\n",
      "Training set | Epoch 891 | MSE Loss: 0.2234\n",
      "Training set | Epoch 892 | MSE Loss: 0.2153\n",
      "Training set | Epoch 893 | MSE Loss: 0.196\n",
      "Training set | Epoch 894 | MSE Loss: 0.2075\n",
      "Training set | Epoch 895 | MSE Loss: 0.1947\n",
      "Training set | Epoch 896 | MSE Loss: 0.1859\n",
      "Training set | Epoch 897 | MSE Loss: 0.1821\n",
      "Training set | Epoch 898 | MSE Loss: 0.1708\n",
      "Training set | Epoch 899 | MSE Loss: 0.1862\n",
      "Training set | Epoch 900 | MSE Loss: 0.1603\n",
      "Training set | Epoch 901 | MSE Loss: 0.1765\n",
      "Training set | Epoch 902 | MSE Loss: 0.1465\n",
      "Training set | Epoch 903 | MSE Loss: 0.1947\n",
      "Training set | Epoch 904 | MSE Loss: 0.1481\n",
      "Training set | Epoch 905 | MSE Loss: 0.1968\n",
      "Training set | Epoch 906 | MSE Loss: 0.1643\n",
      "Training set | Epoch 907 | MSE Loss: 0.2184\n",
      "Training set | Epoch 908 | MSE Loss: 0.2005\n",
      "Training set | Epoch 909 | MSE Loss: 0.2069\n",
      "Training set | Epoch 910 | MSE Loss: 0.2362\n",
      "Training set | Epoch 911 | MSE Loss: 0.2083\n",
      "Training set | Epoch 912 | MSE Loss: 0.2945\n",
      "Training set | Epoch 913 | MSE Loss: 0.221\n",
      "Training set | Epoch 914 | MSE Loss: 0.294\n",
      "Training set | Epoch 915 | MSE Loss: 0.2303\n",
      "Training set | Epoch 916 | MSE Loss: 0.3157\n",
      "Training set | Epoch 917 | MSE Loss: 0.2849\n",
      "Training set | Epoch 918 | MSE Loss: 0.303\n",
      "Training set | Epoch 919 | MSE Loss: 0.3496\n",
      "Training set | Epoch 920 | MSE Loss: 0.2715\n",
      "Training set | Epoch 921 | MSE Loss: 0.408\n",
      "Training set | Epoch 922 | MSE Loss: 0.292\n",
      "Training set | Epoch 923 | MSE Loss: 0.3577\n",
      "Training set | Epoch 924 | MSE Loss: 0.3478\n",
      "Training set | Epoch 925 | MSE Loss: 0.3509\n",
      "Training set | Epoch 926 | MSE Loss: 0.4366\n",
      "Training set | Epoch 927 | MSE Loss: 0.3792\n",
      "Training set | Epoch 928 | MSE Loss: 0.4531\n",
      "Training set | Epoch 929 | MSE Loss: 0.4172\n",
      "Training set | Epoch 930 | MSE Loss: 0.7024\n",
      "Training set | Epoch 931 | MSE Loss: 0.6179\n",
      "Training set | Epoch 932 | MSE Loss: 0.4957\n",
      "Training set | Epoch 933 | MSE Loss: 0.5509\n",
      "Training set | Epoch 934 | MSE Loss: 0.4636\n",
      "Training set | Epoch 935 | MSE Loss: 0.7111\n",
      "Training set | Epoch 936 | MSE Loss: 0.5892\n",
      "Training set | Epoch 937 | MSE Loss: 0.7454\n",
      "Training set | Epoch 938 | MSE Loss: 0.6637\n",
      "Training set | Epoch 939 | MSE Loss: 0.659\n",
      "Training set | Epoch 940 | MSE Loss: 0.8118\n",
      "Training set | Epoch 941 | MSE Loss: 0.6143\n",
      "Training set | Epoch 942 | MSE Loss: 0.764\n",
      "Training set | Epoch 943 | MSE Loss: 0.6648\n",
      "Training set | Epoch 944 | MSE Loss: 0.6248\n",
      "Training set | Epoch 945 | MSE Loss: 0.5777\n",
      "Training set | Epoch 946 | MSE Loss: 0.4457\n",
      "Training set | Epoch 947 | MSE Loss: 0.5309\n",
      "Training set | Epoch 948 | MSE Loss: 0.4543\n",
      "Training set | Epoch 949 | MSE Loss: 0.6288\n",
      "Training set | Epoch 950 | MSE Loss: 0.506\n",
      "Training set | Epoch 951 | MSE Loss: 0.4606\n",
      "Training set | Epoch 952 | MSE Loss: 0.5366\n",
      "Training set | Epoch 953 | MSE Loss: 0.4779\n",
      "Training set | Epoch 954 | MSE Loss: 0.4984\n",
      "Training set | Epoch 955 | MSE Loss: 0.3897\n",
      "Training set | Epoch 956 | MSE Loss: 0.4025\n",
      "Training set | Epoch 957 | MSE Loss: 0.3236\n",
      "Training set | Epoch 958 | MSE Loss: 0.346\n",
      "Training set | Epoch 959 | MSE Loss: 0.448\n",
      "Training set | Epoch 960 | MSE Loss: 0.3665\n",
      "Training set | Epoch 961 | MSE Loss: 0.5392\n",
      "Training set | Epoch 962 | MSE Loss: 0.4325\n",
      "Training set | Epoch 963 | MSE Loss: 0.6482\n",
      "Training set | Epoch 964 | MSE Loss: 0.4715\n",
      "Training set | Epoch 965 | MSE Loss: 0.6218\n",
      "Training set | Epoch 966 | MSE Loss: 0.5765\n",
      "Training set | Epoch 967 | MSE Loss: 0.5818\n",
      "Training set | Epoch 968 | MSE Loss: 0.6905\n",
      "Training set | Epoch 969 | MSE Loss: 0.525\n",
      "Training set | Epoch 970 | MSE Loss: 0.734\n",
      "Training set | Epoch 971 | MSE Loss: 0.5626\n",
      "Training set | Epoch 972 | MSE Loss: 0.7711\n",
      "Training set | Epoch 973 | MSE Loss: 0.5453\n",
      "Training set | Epoch 974 | MSE Loss: 0.7119\n",
      "Training set | Epoch 975 | MSE Loss: 0.5202\n",
      "Training set | Epoch 976 | MSE Loss: 0.6432\n",
      "Training set | Epoch 977 | MSE Loss: 0.5409\n",
      "Training set | Epoch 978 | MSE Loss: 0.5882\n",
      "Training set | Epoch 979 | MSE Loss: 0.5099\n",
      "Training set | Epoch 980 | MSE Loss: 0.553\n",
      "Training set | Epoch 981 | MSE Loss: 0.4442\n",
      "Training set | Epoch 982 | MSE Loss: 0.4906\n",
      "Training set | Epoch 983 | MSE Loss: 0.4345\n",
      "Training set | Epoch 984 | MSE Loss: 0.419\n",
      "Training set | Epoch 985 | MSE Loss: 0.4717\n",
      "Training set | Epoch 986 | MSE Loss: 0.3163\n",
      "Training set | Epoch 987 | MSE Loss: 0.3706\n",
      "Training set | Epoch 988 | MSE Loss: 0.2966\n",
      "Training set | Epoch 989 | MSE Loss: 0.3725\n",
      "Training set | Epoch 990 | MSE Loss: 0.27\n",
      "Training set | Epoch 991 | MSE Loss: 0.3493\n",
      "Training set | Epoch 992 | MSE Loss: 0.3182\n",
      "Training set | Epoch 993 | MSE Loss: 0.3265\n",
      "Training set | Epoch 994 | MSE Loss: 0.3468\n",
      "Training set | Epoch 995 | MSE Loss: 0.318\n",
      "Training set | Epoch 996 | MSE Loss: 0.3699\n",
      "Training set | Epoch 997 | MSE Loss: 0.3302\n",
      "Training set | Epoch 998 | MSE Loss: 0.4502\n",
      "Training set | Epoch 999 | MSE Loss: 0.2982\n",
      "Training set | Epoch 1000 | MSE Loss: 0.4459\n"
     ]
    }
   ],
   "source": [
    "D = 3\n",
    "H = 64\n",
    "\n",
    "model = SimpleLSTM(D, H, 1).to(device)\n",
    "print(model)\n",
    "print()\n",
    "print(\"Total parameters:\", f\"{sum(p.numel() for p in model.parameters()):,}\")\n",
    "print()\n",
    "model.fit(epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | MSE loss: 12.7218 | Total matched 22 out of 110 (Accuracy: 20.0%)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "\n",
    "f = open(f\"test_results/test_result_w_succ_diff_nodes_lstm_{datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")}.csv\", \"w\", newline=\"\")\n",
    "csv_writer = csv.writer(f)\n",
    "csv_writer.writerow([\"Dataset\", \"Actual\", \"Predicted\"])\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_concat_datasets = ConcatDataset(test_datasets)\n",
    "    test_batch_sampler = CustomBatchSampler(test_concat_datasets, batch_size=10240)\n",
    "    test_dataloader = DataLoader(test_concat_datasets, batch_sampler=test_batch_sampler)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_matched = 0\n",
    "    count = 0\n",
    "    for batch in test_dataloader:\n",
    "        x = batch[0]\n",
    "        x = x[0]\n",
    "        y = batch[1]\n",
    "        y = y.unsqueeze(-1)\n",
    "        out = model(x)\n",
    "        csv_writer.writerows(\n",
    "            (i, j.item(), k.item())\n",
    "            for (i, j, k) in zip(\n",
    "                 batch[0][1], y.detach().cpu().numpy(), out.detach().cpu().numpy()\n",
    "            )\n",
    "        )\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss\n",
    "        out = torch.round(out)\n",
    "        matched = (out == y).sum().item()\n",
    "        total_matched += matched\n",
    "        count += 1\n",
    "\n",
    "    print(\n",
    "        \"Test set\",\n",
    "        \"| MSE loss:\",\n",
    "        round((total_loss / count).item(), 4),\n",
    "        \"| Total matched\",\n",
    "        total_matched,\n",
    "        \"out of\",\n",
    "        len(test_concat_datasets),\n",
    "        f\"(Accuracy: {round(total_matched/len(test_concat_datasets) * 100, 2)}%)\",\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Untrained Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | star_graph_n13 | MSE loss: 119.5661 | Total matched 2,628 out of 53,248 (Accuracy: 4.94%)\n"
     ]
    }
   ],
   "source": [
    "dataset_s_n13 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "    device,\n",
    "    \"star_graph_n13_config_rank_dataset.csv\",\n",
    "    \"star_graph_n13_edge_index.json\",\n",
    ")\n",
    "\n",
    "\n",
    "dataset = dataset_s_n13\n",
    "\n",
    "# dataset.set_transform(transform)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_dataloader = DataLoader(dataset, batch_size=10240)\n",
    "    total_loss = 0\n",
    "    total_matched = 0\n",
    "    count = 0\n",
    "    for batch in test_dataloader:\n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "        y = y.unsqueeze(-1)\n",
    "        out = model(x[0])\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss\n",
    "        out = torch.round(out)\n",
    "        matched = (out == y).sum().item()\n",
    "        total_matched += matched\n",
    "        count += 1\n",
    "\n",
    "    print(\n",
    "        \"Test set\",\n",
    "        f\"| {dataset.dataset_name}\",\n",
    "        \"| MSE loss:\",\n",
    "        round((total_loss / count).item(), 4),\n",
    "        \"| Total matched\",\n",
    "        f\"{total_matched:,}\",\n",
    "        \"out of\",\n",
    "        f\"{len(dataset):,}\",\n",
    "        f\"(Accuracy: {round(total_matched/len(dataset) * 100, 2)}%)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
