{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "from torch.utils.data import ConcatDataset, DataLoader, random_split, Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import  CVFConfigForGCNWSuccLSTMDataset, CVFConfigForGCNWSuccLSTMWNormalizationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s_n7 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "    device,\n",
    "    \"star_graph_n7_config_rank_dataset.csv\",\n",
    "    \"star_graph_n7_edge_index.json\",\n",
    ")\n",
    "\n",
    "# dataset_s_n13 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "#     device,\n",
    "#     \"star_graph_n13_config_rank_dataset.csv\",\n",
    "#     \"star_graph_n13_edge_index.json\",\n",
    "# )\n",
    "\n",
    "# dataset_s_n15 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "#     device,\n",
    "#     \"star_graph_n15_config_rank_dataset.csv\",\n",
    "#     \"star_graph_n15_edge_index.json\",\n",
    "# )\n",
    "\n",
    "dataset_rr_n7 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "    device,\n",
    "    \"graph_random_regular_graph_n7_d4_config_rank_dataset.csv\",\n",
    "    \"graph_random_regular_graph_n7_d4_edge_index.json\",\n",
    ")\n",
    "\n",
    "# dataset_rr_n8 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "#     device,\n",
    "#     \"graph_random_regular_graph_n8_d4_config_rank_dataset.csv\",\n",
    "#     \"graph_random_regular_graph_n8_d4_edge_index.json\",\n",
    "# )\n",
    "\n",
    "dataset_plc_n7 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "    device,\n",
    "    \"graph_powerlaw_cluster_graph_n7_config_rank_dataset.csv\",\n",
    "    \"graph_powerlaw_cluster_graph_n7_edge_index.json\",\n",
    ")\n",
    "\n",
    "# dataset_plc_n9 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "#     device,\n",
    "#     \"graph_powerlaw_cluster_graph_n9_config_rank_dataset.csv\",\n",
    "#     \"graph_powerlaw_cluster_graph_n9_edge_index.json\",\n",
    "# )\n",
    "\n",
    "dataset_implict_n5 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "    device,\n",
    "    \"implicit_graph_n5_config_rank_dataset.csv\",\n",
    "    \"implicit_graph_n5_edge_index.json\",\n",
    "    \"dijkstra\",\n",
    ")\n",
    "\n",
    "# dataset_implict_n10 = CVFConfigForGCNWSuccWEIDataset(\n",
    "#     device,\n",
    "#     \"implicit_graph_n10_config_rank_dataset.csv\",\n",
    "#     \"implicit_graph_n10_edge_index.json\",\n",
    "#     \"dijkstra\",\n",
    "# )\n",
    "\n",
    "dataset_implict_n7 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "    device,\n",
    "    \"implicit_graph_n7_config_rank_dataset.csv\",\n",
    "    \"implicit_graph_n7_edge_index.json\",\n",
    "    \"dijkstra\",\n",
    ")\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "dataset_coll = [\n",
    "    dataset_implict_n5,\n",
    "    # dataset_implict_n10,\n",
    "    # dataset_implict_n7,\n",
    "    # dataset_s_n7,\n",
    "    # # dataset_s_n13,\n",
    "    # # dataset_s_n15,\n",
    "    # dataset_rr_n7,\n",
    "    # # dataset_rr_n8,\n",
    "    # dataset_plc_n7,\n",
    "    # # dataset_plc_n9,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes = [int(0.95 * len(ds)) for ds in dataset_coll]\n",
    "test_sizes = [len(ds) - trs for ds, trs in zip(dataset_coll, train_sizes)]\n",
    "\n",
    "train_test_datasets = [\n",
    "    random_split(ds, [tr_s, ts])\n",
    "    for ds, tr_s, ts in zip(dataset_coll, train_sizes, test_sizes)\n",
    "]\n",
    "\n",
    "train_datasets = [ds[0] for ds in train_test_datasets]\n",
    "test_datasets = [ds[1] for ds in train_test_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset size: 230\n"
     ]
    }
   ],
   "source": [
    "datasets = ConcatDataset(train_datasets)\n",
    "print(f\"Train Dataset size: {len(datasets):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBatchSampler(Sampler):\n",
    "    def __init__(self, datasets: ConcatDataset, batch_size: int):\n",
    "        self.datasets = datasets\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        last_accessed = [0] + self.datasets.cumulative_sizes[:]\n",
    "        end_loop = [False for _ in range(len(self.datasets.datasets))]\n",
    "\n",
    "        while not all(end_loop):\n",
    "            for turn in range(len(self.datasets.datasets)):\n",
    "                if end_loop[turn]:\n",
    "                    continue\n",
    "\n",
    "                batch_size = self.batch_size\n",
    "                if (\n",
    "                    last_accessed[turn] + batch_size\n",
    "                    >= self.datasets.cumulative_sizes[turn]\n",
    "                ):\n",
    "                    batch_size = (\n",
    "                        self.datasets.cumulative_sizes[turn] - last_accessed[turn]\n",
    "                    )\n",
    "                    end_loop[turn] = True\n",
    "\n",
    "                yield list(range(last_accessed[turn], last_accessed[turn] + batch_size))\n",
    "\n",
    "                last_accessed[turn] += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from normalization import compute_mean_std, NormalizeTransform\n",
    "\n",
    "# loader = DataLoader(datasets, batch_sampler=CustomBatchSampler(datasets, batch_size=1024))\n",
    "# mean, std = compute_mean_std(loader)\n",
    "# print(mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transform = NormalizeTransform(mean, std)\n",
    "# for dataset in train_datasets:\n",
    "#     dataset.dataset.set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler = CustomBatchSampler(datasets, batch_size=batch_size)\n",
    "dataloader = DataLoader(datasets, batch_sampler=batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        output = self.norm(output)\n",
    "        output = self.h2o(output)\n",
    "        output = torch.relu(output)\n",
    "        output = global_mean_pool(output, torch.zeros(output.size(1)).to(device).long())\n",
    "        return output\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for batch in dataloader:\n",
    "                x = batch[0]\n",
    "                x = x[0]\n",
    "                y = batch[1]\n",
    "                y = y.unsqueeze(-1)\n",
    "                out = self(x)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(out, y)\n",
    "                total_loss += loss\n",
    "                count += 1\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(\n",
    "                \"Training set | Epoch\",\n",
    "                epoch,\n",
    "                \"| MSE Loss:\",\n",
    "                round((total_loss / count).item(), 4),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleLSTM(\n",
      "  (lstm): GRU(3, 64, batch_first=True)\n",
      "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (h2o): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 13,441\n",
      "\n",
      "Training set | Epoch 1 | MSE Loss: 68.7478\n",
      "Training set | Epoch 2 | MSE Loss: 68.7478\n",
      "Training set | Epoch 3 | MSE Loss: 68.7478\n",
      "Training set | Epoch 4 | MSE Loss: 68.7467\n",
      "Training set | Epoch 5 | MSE Loss: 60.1434\n",
      "Training set | Epoch 6 | MSE Loss: 48.1817\n",
      "Training set | Epoch 7 | MSE Loss: 37.9102\n",
      "Training set | Epoch 8 | MSE Loss: 31.2793\n",
      "Training set | Epoch 9 | MSE Loss: 27.1112\n",
      "Training set | Epoch 10 | MSE Loss: 24.1874\n",
      "Training set | Epoch 11 | MSE Loss: 21.8355\n",
      "Training set | Epoch 12 | MSE Loss: 19.7158\n",
      "Training set | Epoch 13 | MSE Loss: 17.6558\n",
      "Training set | Epoch 14 | MSE Loss: 15.6163\n",
      "Training set | Epoch 15 | MSE Loss: 13.7448\n",
      "Training set | Epoch 16 | MSE Loss: 12.4157\n",
      "Training set | Epoch 17 | MSE Loss: 11.6562\n",
      "Training set | Epoch 18 | MSE Loss: 11.2343\n",
      "Training set | Epoch 19 | MSE Loss: 10.9596\n",
      "Training set | Epoch 20 | MSE Loss: 10.7915\n",
      "Training set | Epoch 21 | MSE Loss: 10.8762\n",
      "Training set | Epoch 22 | MSE Loss: 10.7373\n",
      "Training set | Epoch 23 | MSE Loss: 10.7718\n",
      "Training set | Epoch 24 | MSE Loss: 10.6525\n",
      "Training set | Epoch 25 | MSE Loss: 10.4715\n",
      "Training set | Epoch 26 | MSE Loss: 10.4083\n",
      "Training set | Epoch 27 | MSE Loss: 10.1712\n",
      "Training set | Epoch 28 | MSE Loss: 10.0922\n",
      "Training set | Epoch 29 | MSE Loss: 9.9494\n",
      "Training set | Epoch 30 | MSE Loss: 9.8086\n",
      "Training set | Epoch 31 | MSE Loss: 9.7776\n",
      "Training set | Epoch 32 | MSE Loss: 9.6572\n",
      "Training set | Epoch 33 | MSE Loss: 9.5614\n",
      "Training set | Epoch 34 | MSE Loss: 9.4833\n",
      "Training set | Epoch 35 | MSE Loss: 9.3226\n",
      "Training set | Epoch 36 | MSE Loss: 9.2073\n",
      "Training set | Epoch 37 | MSE Loss: 9.0558\n",
      "Training set | Epoch 38 | MSE Loss: 8.8954\n",
      "Training set | Epoch 39 | MSE Loss: 8.7801\n",
      "Training set | Epoch 40 | MSE Loss: 8.6309\n",
      "Training set | Epoch 41 | MSE Loss: 8.545\n",
      "Training set | Epoch 42 | MSE Loss: 8.4214\n",
      "Training set | Epoch 43 | MSE Loss: 8.3248\n",
      "Training set | Epoch 44 | MSE Loss: 8.2136\n",
      "Training set | Epoch 45 | MSE Loss: 8.0965\n",
      "Training set | Epoch 46 | MSE Loss: 7.9989\n",
      "Training set | Epoch 47 | MSE Loss: 7.8855\n",
      "Training set | Epoch 48 | MSE Loss: 7.7937\n",
      "Training set | Epoch 49 | MSE Loss: 7.6853\n",
      "Training set | Epoch 50 | MSE Loss: 7.5848\n",
      "Training set | Epoch 51 | MSE Loss: 7.4817\n",
      "Training set | Epoch 52 | MSE Loss: 7.3671\n",
      "Training set | Epoch 53 | MSE Loss: 7.2648\n",
      "Training set | Epoch 54 | MSE Loss: 7.1506\n",
      "Training set | Epoch 55 | MSE Loss: 7.0568\n",
      "Training set | Epoch 56 | MSE Loss: 6.9557\n",
      "Training set | Epoch 57 | MSE Loss: 6.8659\n",
      "Training set | Epoch 58 | MSE Loss: 6.7764\n",
      "Training set | Epoch 59 | MSE Loss: 6.6879\n",
      "Training set | Epoch 60 | MSE Loss: 6.6079\n",
      "Training set | Epoch 61 | MSE Loss: 6.5288\n",
      "Training set | Epoch 62 | MSE Loss: 6.4638\n",
      "Training set | Epoch 63 | MSE Loss: 6.4032\n",
      "Training set | Epoch 64 | MSE Loss: 6.3546\n",
      "Training set | Epoch 65 | MSE Loss: 6.3064\n",
      "Training set | Epoch 66 | MSE Loss: 6.2615\n",
      "Training set | Epoch 67 | MSE Loss: 6.2196\n",
      "Training set | Epoch 68 | MSE Loss: 6.1808\n",
      "Training set | Epoch 69 | MSE Loss: 6.147\n",
      "Training set | Epoch 70 | MSE Loss: 6.1129\n",
      "Training set | Epoch 71 | MSE Loss: 6.0806\n",
      "Training set | Epoch 72 | MSE Loss: 6.0447\n",
      "Training set | Epoch 73 | MSE Loss: 6.0087\n",
      "Training set | Epoch 74 | MSE Loss: 5.9713\n",
      "Training set | Epoch 75 | MSE Loss: 5.9347\n",
      "Training set | Epoch 76 | MSE Loss: 5.8978\n",
      "Training set | Epoch 77 | MSE Loss: 5.861\n",
      "Training set | Epoch 78 | MSE Loss: 5.8241\n",
      "Training set | Epoch 79 | MSE Loss: 5.7881\n",
      "Training set | Epoch 80 | MSE Loss: 5.7533\n",
      "Training set | Epoch 81 | MSE Loss: 5.719\n",
      "Training set | Epoch 82 | MSE Loss: 5.6849\n",
      "Training set | Epoch 83 | MSE Loss: 5.6504\n",
      "Training set | Epoch 84 | MSE Loss: 5.6169\n",
      "Training set | Epoch 85 | MSE Loss: 5.5845\n",
      "Training set | Epoch 86 | MSE Loss: 5.5512\n",
      "Training set | Epoch 87 | MSE Loss: 5.5177\n",
      "Training set | Epoch 88 | MSE Loss: 5.4839\n",
      "Training set | Epoch 89 | MSE Loss: 5.4498\n",
      "Training set | Epoch 90 | MSE Loss: 5.415\n",
      "Training set | Epoch 91 | MSE Loss: 5.3793\n",
      "Training set | Epoch 92 | MSE Loss: 5.3436\n",
      "Training set | Epoch 93 | MSE Loss: 5.3093\n",
      "Training set | Epoch 94 | MSE Loss: 5.2765\n",
      "Training set | Epoch 95 | MSE Loss: 5.2449\n",
      "Training set | Epoch 96 | MSE Loss: 5.2132\n",
      "Training set | Epoch 97 | MSE Loss: 5.1818\n",
      "Training set | Epoch 98 | MSE Loss: 5.15\n",
      "Training set | Epoch 99 | MSE Loss: 5.117\n",
      "Training set | Epoch 100 | MSE Loss: 5.083\n",
      "Training set | Epoch 101 | MSE Loss: 5.0486\n",
      "Training set | Epoch 102 | MSE Loss: 5.0126\n",
      "Training set | Epoch 103 | MSE Loss: 4.9747\n",
      "Training set | Epoch 104 | MSE Loss: 4.9351\n",
      "Training set | Epoch 105 | MSE Loss: 4.8939\n",
      "Training set | Epoch 106 | MSE Loss: 4.851\n",
      "Training set | Epoch 107 | MSE Loss: 4.8058\n",
      "Training set | Epoch 108 | MSE Loss: 4.7582\n",
      "Training set | Epoch 109 | MSE Loss: 4.7084\n",
      "Training set | Epoch 110 | MSE Loss: 4.6575\n",
      "Training set | Epoch 111 | MSE Loss: 4.6039\n",
      "Training set | Epoch 112 | MSE Loss: 4.549\n",
      "Training set | Epoch 113 | MSE Loss: 4.4922\n",
      "Training set | Epoch 114 | MSE Loss: 4.4347\n",
      "Training set | Epoch 115 | MSE Loss: 4.3778\n",
      "Training set | Epoch 116 | MSE Loss: 4.3209\n",
      "Training set | Epoch 117 | MSE Loss: 4.2661\n",
      "Training set | Epoch 118 | MSE Loss: 4.213\n",
      "Training set | Epoch 119 | MSE Loss: 4.1613\n",
      "Training set | Epoch 120 | MSE Loss: 4.1099\n",
      "Training set | Epoch 121 | MSE Loss: 4.0596\n",
      "Training set | Epoch 122 | MSE Loss: 4.0115\n",
      "Training set | Epoch 123 | MSE Loss: 3.9613\n",
      "Training set | Epoch 124 | MSE Loss: 3.9069\n",
      "Training set | Epoch 125 | MSE Loss: 3.8534\n",
      "Training set | Epoch 126 | MSE Loss: 3.7996\n",
      "Training set | Epoch 127 | MSE Loss: 3.7523\n",
      "Training set | Epoch 128 | MSE Loss: 3.7086\n",
      "Training set | Epoch 129 | MSE Loss: 3.6638\n",
      "Training set | Epoch 130 | MSE Loss: 3.6145\n",
      "Training set | Epoch 131 | MSE Loss: 3.5581\n",
      "Training set | Epoch 132 | MSE Loss: 3.507\n",
      "Training set | Epoch 133 | MSE Loss: 3.4636\n",
      "Training set | Epoch 134 | MSE Loss: 3.4377\n",
      "Training set | Epoch 135 | MSE Loss: 3.4846\n",
      "Training set | Epoch 136 | MSE Loss: 3.4836\n",
      "Training set | Epoch 137 | MSE Loss: 3.402\n",
      "Training set | Epoch 138 | MSE Loss: 3.2336\n",
      "Training set | Epoch 139 | MSE Loss: 3.3131\n",
      "Training set | Epoch 140 | MSE Loss: 3.314\n",
      "Training set | Epoch 141 | MSE Loss: 3.1104\n",
      "Training set | Epoch 142 | MSE Loss: 3.1967\n",
      "Training set | Epoch 143 | MSE Loss: 3.122\n",
      "Training set | Epoch 144 | MSE Loss: 3.011\n",
      "Training set | Epoch 145 | MSE Loss: 3.0929\n",
      "Training set | Epoch 146 | MSE Loss: 2.9476\n",
      "Training set | Epoch 147 | MSE Loss: 2.9362\n",
      "Training set | Epoch 148 | MSE Loss: 2.9122\n",
      "Training set | Epoch 149 | MSE Loss: 2.825\n",
      "Training set | Epoch 150 | MSE Loss: 2.8447\n",
      "Training set | Epoch 151 | MSE Loss: 2.7459\n",
      "Training set | Epoch 152 | MSE Loss: 2.744\n",
      "Training set | Epoch 153 | MSE Loss: 2.6963\n",
      "Training set | Epoch 154 | MSE Loss: 2.6461\n",
      "Training set | Epoch 155 | MSE Loss: 2.6343\n",
      "Training set | Epoch 156 | MSE Loss: 2.5642\n",
      "Training set | Epoch 157 | MSE Loss: 2.5436\n",
      "Training set | Epoch 158 | MSE Loss: 2.5117\n",
      "Training set | Epoch 159 | MSE Loss: 2.4516\n",
      "Training set | Epoch 160 | MSE Loss: 2.4374\n",
      "Training set | Epoch 161 | MSE Loss: 2.403\n",
      "Training set | Epoch 162 | MSE Loss: 2.3364\n",
      "Training set | Epoch 163 | MSE Loss: 2.317\n",
      "Training set | Epoch 164 | MSE Loss: 2.2933\n",
      "Training set | Epoch 165 | MSE Loss: 2.2241\n",
      "Training set | Epoch 166 | MSE Loss: 2.1896\n",
      "Training set | Epoch 167 | MSE Loss: 2.1887\n",
      "Training set | Epoch 168 | MSE Loss: 2.1342\n",
      "Training set | Epoch 169 | MSE Loss: 2.0636\n",
      "Training set | Epoch 170 | MSE Loss: 2.0403\n",
      "Training set | Epoch 171 | MSE Loss: 2.0272\n",
      "Training set | Epoch 172 | MSE Loss: 1.9795\n",
      "Training set | Epoch 173 | MSE Loss: 1.9091\n",
      "Training set | Epoch 174 | MSE Loss: 1.8754\n",
      "Training set | Epoch 175 | MSE Loss: 1.8709\n",
      "Training set | Epoch 176 | MSE Loss: 1.8319\n",
      "Training set | Epoch 177 | MSE Loss: 1.7595\n",
      "Training set | Epoch 178 | MSE Loss: 1.7143\n",
      "Training set | Epoch 179 | MSE Loss: 1.7095\n",
      "Training set | Epoch 180 | MSE Loss: 1.6888\n",
      "Training set | Epoch 181 | MSE Loss: 1.6575\n",
      "Training set | Epoch 182 | MSE Loss: 1.5799\n",
      "Training set | Epoch 183 | MSE Loss: 1.5373\n",
      "Training set | Epoch 184 | MSE Loss: 1.5495\n",
      "Training set | Epoch 185 | MSE Loss: 1.5529\n",
      "Training set | Epoch 186 | MSE Loss: 1.5411\n",
      "Training set | Epoch 187 | MSE Loss: 1.4187\n",
      "Training set | Epoch 188 | MSE Loss: 1.4043\n",
      "Training set | Epoch 189 | MSE Loss: 1.4764\n",
      "Training set | Epoch 190 | MSE Loss: 1.4034\n",
      "Training set | Epoch 191 | MSE Loss: 1.3091\n",
      "Training set | Epoch 192 | MSE Loss: 1.2752\n",
      "Training set | Epoch 193 | MSE Loss: 1.3106\n",
      "Training set | Epoch 194 | MSE Loss: 1.2924\n",
      "Training set | Epoch 195 | MSE Loss: 1.1907\n",
      "Training set | Epoch 196 | MSE Loss: 1.2014\n",
      "Training set | Epoch 197 | MSE Loss: 1.2294\n",
      "Training set | Epoch 198 | MSE Loss: 1.1114\n",
      "Training set | Epoch 199 | MSE Loss: 1.1166\n",
      "Training set | Epoch 200 | MSE Loss: 1.1316\n",
      "Training set | Epoch 201 | MSE Loss: 1.0388\n",
      "Training set | Epoch 202 | MSE Loss: 1.0369\n",
      "Training set | Epoch 203 | MSE Loss: 1.0546\n",
      "Training set | Epoch 204 | MSE Loss: 0.9765\n",
      "Training set | Epoch 205 | MSE Loss: 0.9677\n",
      "Training set | Epoch 206 | MSE Loss: 0.9825\n",
      "Training set | Epoch 207 | MSE Loss: 0.9198\n",
      "Training set | Epoch 208 | MSE Loss: 0.8925\n",
      "Training set | Epoch 209 | MSE Loss: 0.905\n",
      "Training set | Epoch 210 | MSE Loss: 0.8615\n",
      "Training set | Epoch 211 | MSE Loss: 0.8331\n",
      "Training set | Epoch 212 | MSE Loss: 0.8382\n",
      "Training set | Epoch 213 | MSE Loss: 0.8049\n",
      "Training set | Epoch 214 | MSE Loss: 0.7773\n",
      "Training set | Epoch 215 | MSE Loss: 0.7701\n",
      "Training set | Epoch 216 | MSE Loss: 0.7473\n",
      "Training set | Epoch 217 | MSE Loss: 0.7231\n",
      "Training set | Epoch 218 | MSE Loss: 0.7145\n",
      "Training set | Epoch 219 | MSE Loss: 0.7062\n",
      "Training set | Epoch 220 | MSE Loss: 0.6839\n",
      "Training set | Epoch 221 | MSE Loss: 0.6595\n",
      "Training set | Epoch 222 | MSE Loss: 0.6459\n",
      "Training set | Epoch 223 | MSE Loss: 0.6306\n",
      "Training set | Epoch 224 | MSE Loss: 0.6185\n",
      "Training set | Epoch 225 | MSE Loss: 0.6017\n",
      "Training set | Epoch 226 | MSE Loss: 0.5904\n",
      "Training set | Epoch 227 | MSE Loss: 0.5753\n",
      "Training set | Epoch 228 | MSE Loss: 0.5674\n",
      "Training set | Epoch 229 | MSE Loss: 0.5573\n",
      "Training set | Epoch 230 | MSE Loss: 0.5512\n",
      "Training set | Epoch 231 | MSE Loss: 0.555\n",
      "Training set | Epoch 232 | MSE Loss: 0.5664\n",
      "Training set | Epoch 233 | MSE Loss: 0.5724\n",
      "Training set | Epoch 234 | MSE Loss: 0.5584\n",
      "Training set | Epoch 235 | MSE Loss: 0.5062\n",
      "Training set | Epoch 236 | MSE Loss: 0.4734\n",
      "Training set | Epoch 237 | MSE Loss: 0.4904\n",
      "Training set | Epoch 238 | MSE Loss: 0.4876\n",
      "Training set | Epoch 239 | MSE Loss: 0.4766\n",
      "Training set | Epoch 240 | MSE Loss: 0.4408\n",
      "Training set | Epoch 241 | MSE Loss: 0.4315\n",
      "Training set | Epoch 242 | MSE Loss: 0.4421\n",
      "Training set | Epoch 243 | MSE Loss: 0.4398\n",
      "Training set | Epoch 244 | MSE Loss: 0.4102\n",
      "Training set | Epoch 245 | MSE Loss: 0.3963\n",
      "Training set | Epoch 246 | MSE Loss: 0.3863\n",
      "Training set | Epoch 247 | MSE Loss: 0.3872\n",
      "Training set | Epoch 248 | MSE Loss: 0.3833\n",
      "Training set | Epoch 249 | MSE Loss: 0.3629\n",
      "Training set | Epoch 250 | MSE Loss: 0.346\n",
      "Training set | Epoch 251 | MSE Loss: 0.3456\n",
      "Training set | Epoch 252 | MSE Loss: 0.3367\n",
      "Training set | Epoch 253 | MSE Loss: 0.331\n",
      "Training set | Epoch 254 | MSE Loss: 0.3214\n",
      "Training set | Epoch 255 | MSE Loss: 0.3106\n",
      "Training set | Epoch 256 | MSE Loss: 0.3054\n",
      "Training set | Epoch 257 | MSE Loss: 0.3003\n",
      "Training set | Epoch 258 | MSE Loss: 0.295\n",
      "Training set | Epoch 259 | MSE Loss: 0.2919\n",
      "Training set | Epoch 260 | MSE Loss: 0.2922\n",
      "Training set | Epoch 261 | MSE Loss: 0.2933\n",
      "Training set | Epoch 262 | MSE Loss: 0.2913\n",
      "Training set | Epoch 263 | MSE Loss: 0.289\n",
      "Training set | Epoch 264 | MSE Loss: 0.2777\n",
      "Training set | Epoch 265 | MSE Loss: 0.2625\n",
      "Training set | Epoch 266 | MSE Loss: 0.2525\n",
      "Training set | Epoch 267 | MSE Loss: 0.2486\n",
      "Training set | Epoch 268 | MSE Loss: 0.2514\n",
      "Training set | Epoch 269 | MSE Loss: 0.2514\n",
      "Training set | Epoch 270 | MSE Loss: 0.2477\n",
      "Training set | Epoch 271 | MSE Loss: 0.2414\n",
      "Training set | Epoch 272 | MSE Loss: 0.2345\n",
      "Training set | Epoch 273 | MSE Loss: 0.2276\n",
      "Training set | Epoch 274 | MSE Loss: 0.2202\n",
      "Training set | Epoch 275 | MSE Loss: 0.2126\n",
      "Training set | Epoch 276 | MSE Loss: 0.208\n",
      "Training set | Epoch 277 | MSE Loss: 0.2053\n",
      "Training set | Epoch 278 | MSE Loss: 0.2036\n",
      "Training set | Epoch 279 | MSE Loss: 0.2034\n",
      "Training set | Epoch 280 | MSE Loss: 0.2055\n",
      "Training set | Epoch 281 | MSE Loss: 0.2144\n",
      "Training set | Epoch 282 | MSE Loss: 0.2275\n",
      "Training set | Epoch 283 | MSE Loss: 0.2559\n",
      "Training set | Epoch 284 | MSE Loss: 0.2843\n",
      "Training set | Epoch 285 | MSE Loss: 0.2865\n",
      "Training set | Epoch 286 | MSE Loss: 0.2314\n",
      "Training set | Epoch 287 | MSE Loss: 0.1812\n",
      "Training set | Epoch 288 | MSE Loss: 0.2051\n",
      "Training set | Epoch 289 | MSE Loss: 0.2249\n",
      "Training set | Epoch 290 | MSE Loss: 0.1933\n",
      "Training set | Epoch 291 | MSE Loss: 0.1735\n",
      "Training set | Epoch 292 | MSE Loss: 0.1989\n",
      "Training set | Epoch 293 | MSE Loss: 0.1972\n",
      "Training set | Epoch 294 | MSE Loss: 0.1663\n",
      "Training set | Epoch 295 | MSE Loss: 0.1745\n",
      "Training set | Epoch 296 | MSE Loss: 0.1931\n",
      "Training set | Epoch 297 | MSE Loss: 0.1709\n",
      "Training set | Epoch 298 | MSE Loss: 0.1564\n",
      "Training set | Epoch 299 | MSE Loss: 0.1709\n",
      "Training set | Epoch 300 | MSE Loss: 0.1635\n",
      "Training set | Epoch 301 | MSE Loss: 0.1498\n",
      "Training set | Epoch 302 | MSE Loss: 0.1526\n",
      "Training set | Epoch 303 | MSE Loss: 0.1572\n",
      "Training set | Epoch 304 | MSE Loss: 0.1511\n",
      "Training set | Epoch 305 | MSE Loss: 0.1412\n",
      "Training set | Epoch 306 | MSE Loss: 0.1492\n",
      "Training set | Epoch 307 | MSE Loss: 0.1478\n",
      "Training set | Epoch 308 | MSE Loss: 0.14\n",
      "Training set | Epoch 309 | MSE Loss: 0.1327\n",
      "Training set | Epoch 310 | MSE Loss: 0.1365\n",
      "Training set | Epoch 311 | MSE Loss: 0.135\n",
      "Training set | Epoch 312 | MSE Loss: 0.1343\n",
      "Training set | Epoch 313 | MSE Loss: 0.1266\n",
      "Training set | Epoch 314 | MSE Loss: 0.1285\n",
      "Training set | Epoch 315 | MSE Loss: 0.1277\n",
      "Training set | Epoch 316 | MSE Loss: 0.125\n",
      "Training set | Epoch 317 | MSE Loss: 0.1215\n",
      "Training set | Epoch 318 | MSE Loss: 0.1187\n",
      "Training set | Epoch 319 | MSE Loss: 0.1208\n",
      "Training set | Epoch 320 | MSE Loss: 0.12\n",
      "Training set | Epoch 321 | MSE Loss: 0.1136\n",
      "Training set | Epoch 322 | MSE Loss: 0.1133\n",
      "Training set | Epoch 323 | MSE Loss: 0.1145\n",
      "Training set | Epoch 324 | MSE Loss: 0.1124\n",
      "Training set | Epoch 325 | MSE Loss: 0.1092\n",
      "Training set | Epoch 326 | MSE Loss: 0.107\n",
      "Training set | Epoch 327 | MSE Loss: 0.1058\n",
      "Training set | Epoch 328 | MSE Loss: 0.1109\n",
      "Training set | Epoch 329 | MSE Loss: 0.1096\n",
      "Training set | Epoch 330 | MSE Loss: 0.1121\n",
      "Training set | Epoch 331 | MSE Loss: 0.1039\n",
      "Training set | Epoch 332 | MSE Loss: 0.0998\n",
      "Training set | Epoch 333 | MSE Loss: 0.0995\n",
      "Training set | Epoch 334 | MSE Loss: 0.0962\n",
      "Training set | Epoch 335 | MSE Loss: 0.0969\n",
      "Training set | Epoch 336 | MSE Loss: 0.0933\n",
      "Training set | Epoch 337 | MSE Loss: 0.0927\n",
      "Training set | Epoch 338 | MSE Loss: 0.0909\n",
      "Training set | Epoch 339 | MSE Loss: 0.0891\n",
      "Training set | Epoch 340 | MSE Loss: 0.0898\n",
      "Training set | Epoch 341 | MSE Loss: 0.0904\n",
      "Training set | Epoch 342 | MSE Loss: 0.0873\n",
      "Training set | Epoch 343 | MSE Loss: 0.0916\n",
      "Training set | Epoch 344 | MSE Loss: 0.0867\n",
      "Training set | Epoch 345 | MSE Loss: 0.0865\n",
      "Training set | Epoch 346 | MSE Loss: 0.0865\n",
      "Training set | Epoch 347 | MSE Loss: 0.0867\n",
      "Training set | Epoch 348 | MSE Loss: 0.0885\n",
      "Training set | Epoch 349 | MSE Loss: 0.0941\n",
      "Training set | Epoch 350 | MSE Loss: 0.1021\n",
      "Training set | Epoch 351 | MSE Loss: 0.1243\n",
      "Training set | Epoch 352 | MSE Loss: 0.1441\n",
      "Training set | Epoch 353 | MSE Loss: 0.1592\n",
      "Training set | Epoch 354 | MSE Loss: 0.1417\n",
      "Training set | Epoch 355 | MSE Loss: 0.0955\n",
      "Training set | Epoch 356 | MSE Loss: 0.0753\n",
      "Training set | Epoch 357 | MSE Loss: 0.0973\n",
      "Training set | Epoch 358 | MSE Loss: 0.1086\n",
      "Training set | Epoch 359 | MSE Loss: 0.0854\n",
      "Training set | Epoch 360 | MSE Loss: 0.0718\n",
      "Training set | Epoch 361 | MSE Loss: 0.0836\n",
      "Training set | Epoch 362 | MSE Loss: 0.0934\n",
      "Training set | Epoch 363 | MSE Loss: 0.0901\n",
      "Training set | Epoch 364 | MSE Loss: 0.07\n",
      "Training set | Epoch 365 | MSE Loss: 0.0746\n",
      "Training set | Epoch 366 | MSE Loss: 0.0876\n",
      "Training set | Epoch 367 | MSE Loss: 0.0817\n",
      "Training set | Epoch 368 | MSE Loss: 0.0716\n",
      "Training set | Epoch 369 | MSE Loss: 0.0679\n",
      "Training set | Epoch 370 | MSE Loss: 0.0779\n",
      "Training set | Epoch 371 | MSE Loss: 0.0739\n",
      "Training set | Epoch 372 | MSE Loss: 0.0682\n",
      "Training set | Epoch 373 | MSE Loss: 0.0612\n",
      "Training set | Epoch 374 | MSE Loss: 0.0655\n",
      "Training set | Epoch 375 | MSE Loss: 0.062\n",
      "Training set | Epoch 376 | MSE Loss: 0.0595\n",
      "Training set | Epoch 377 | MSE Loss: 0.0594\n",
      "Training set | Epoch 378 | MSE Loss: 0.0584\n",
      "Training set | Epoch 379 | MSE Loss: 0.063\n",
      "Training set | Epoch 380 | MSE Loss: 0.0604\n",
      "Training set | Epoch 381 | MSE Loss: 0.0583\n",
      "Training set | Epoch 382 | MSE Loss: 0.0542\n",
      "Training set | Epoch 383 | MSE Loss: 0.057\n",
      "Training set | Epoch 384 | MSE Loss: 0.0612\n",
      "Training set | Epoch 385 | MSE Loss: 0.0574\n",
      "Training set | Epoch 386 | MSE Loss: 0.054\n",
      "Training set | Epoch 387 | MSE Loss: 0.0516\n",
      "Training set | Epoch 388 | MSE Loss: 0.0559\n",
      "Training set | Epoch 389 | MSE Loss: 0.0594\n",
      "Training set | Epoch 390 | MSE Loss: 0.0591\n",
      "Training set | Epoch 391 | MSE Loss: 0.0572\n",
      "Training set | Epoch 392 | MSE Loss: 0.05\n",
      "Training set | Epoch 393 | MSE Loss: 0.0517\n",
      "Training set | Epoch 394 | MSE Loss: 0.0556\n",
      "Training set | Epoch 395 | MSE Loss: 0.0576\n",
      "Training set | Epoch 396 | MSE Loss: 0.0558\n",
      "Training set | Epoch 397 | MSE Loss: 0.049\n",
      "Training set | Epoch 398 | MSE Loss: 0.0521\n",
      "Training set | Epoch 399 | MSE Loss: 0.0465\n",
      "Training set | Epoch 400 | MSE Loss: 0.0509\n",
      "Training set | Epoch 401 | MSE Loss: 0.054\n",
      "Training set | Epoch 402 | MSE Loss: 0.0494\n",
      "Training set | Epoch 403 | MSE Loss: 0.0514\n",
      "Training set | Epoch 404 | MSE Loss: 0.0444\n",
      "Training set | Epoch 405 | MSE Loss: 0.0508\n",
      "Training set | Epoch 406 | MSE Loss: 0.0557\n",
      "Training set | Epoch 407 | MSE Loss: 0.0576\n",
      "Training set | Epoch 408 | MSE Loss: 0.0739\n",
      "Training set | Epoch 409 | MSE Loss: 0.0545\n",
      "Training set | Epoch 410 | MSE Loss: 0.0589\n",
      "Training set | Epoch 411 | MSE Loss: 0.0462\n",
      "Training set | Epoch 412 | MSE Loss: 0.0545\n",
      "Training set | Epoch 413 | MSE Loss: 0.0606\n",
      "Training set | Epoch 414 | MSE Loss: 0.0531\n",
      "Training set | Epoch 415 | MSE Loss: 0.0654\n",
      "Training set | Epoch 416 | MSE Loss: 0.0417\n",
      "Training set | Epoch 417 | MSE Loss: 0.0581\n",
      "Training set | Epoch 418 | MSE Loss: 0.0536\n",
      "Training set | Epoch 419 | MSE Loss: 0.0656\n",
      "Training set | Epoch 420 | MSE Loss: 0.0561\n",
      "Training set | Epoch 421 | MSE Loss: 0.0474\n",
      "Training set | Epoch 422 | MSE Loss: 0.0552\n",
      "Training set | Epoch 423 | MSE Loss: 0.0421\n",
      "Training set | Epoch 424 | MSE Loss: 0.0597\n",
      "Training set | Epoch 425 | MSE Loss: 0.0507\n",
      "Training set | Epoch 426 | MSE Loss: 0.0637\n",
      "Training set | Epoch 427 | MSE Loss: 0.0449\n",
      "Training set | Epoch 428 | MSE Loss: 0.0495\n",
      "Training set | Epoch 429 | MSE Loss: 0.0446\n",
      "Training set | Epoch 430 | MSE Loss: 0.0454\n",
      "Training set | Epoch 431 | MSE Loss: 0.0443\n",
      "Training set | Epoch 432 | MSE Loss: 0.0408\n",
      "Training set | Epoch 433 | MSE Loss: 0.0438\n",
      "Training set | Epoch 434 | MSE Loss: 0.0378\n",
      "Training set | Epoch 435 | MSE Loss: 0.0419\n",
      "Training set | Epoch 436 | MSE Loss: 0.0357\n",
      "Training set | Epoch 437 | MSE Loss: 0.0372\n",
      "Training set | Epoch 438 | MSE Loss: 0.0339\n",
      "Training set | Epoch 439 | MSE Loss: 0.0346\n",
      "Training set | Epoch 440 | MSE Loss: 0.0358\n",
      "Training set | Epoch 441 | MSE Loss: 0.0362\n",
      "Training set | Epoch 442 | MSE Loss: 0.0367\n",
      "Training set | Epoch 443 | MSE Loss: 0.0348\n",
      "Training set | Epoch 444 | MSE Loss: 0.034\n",
      "Training set | Epoch 445 | MSE Loss: 0.0324\n",
      "Training set | Epoch 446 | MSE Loss: 0.0318\n",
      "Training set | Epoch 447 | MSE Loss: 0.032\n",
      "Training set | Epoch 448 | MSE Loss: 0.0303\n",
      "Training set | Epoch 449 | MSE Loss: 0.0317\n",
      "Training set | Epoch 450 | MSE Loss: 0.0299\n",
      "Training set | Epoch 451 | MSE Loss: 0.0304\n",
      "Training set | Epoch 452 | MSE Loss: 0.0307\n",
      "Training set | Epoch 453 | MSE Loss: 0.0292\n",
      "Training set | Epoch 454 | MSE Loss: 0.0311\n",
      "Training set | Epoch 455 | MSE Loss: 0.0292\n",
      "Training set | Epoch 456 | MSE Loss: 0.03\n",
      "Training set | Epoch 457 | MSE Loss: 0.0293\n",
      "Training set | Epoch 458 | MSE Loss: 0.0293\n",
      "Training set | Epoch 459 | MSE Loss: 0.0303\n",
      "Training set | Epoch 460 | MSE Loss: 0.0307\n",
      "Training set | Epoch 461 | MSE Loss: 0.0323\n",
      "Training set | Epoch 462 | MSE Loss: 0.0358\n",
      "Training set | Epoch 463 | MSE Loss: 0.0383\n",
      "Training set | Epoch 464 | MSE Loss: 0.0446\n",
      "Training set | Epoch 465 | MSE Loss: 0.0475\n",
      "Training set | Epoch 466 | MSE Loss: 0.058\n",
      "Training set | Epoch 467 | MSE Loss: 0.0586\n",
      "Training set | Epoch 468 | MSE Loss: 0.058\n",
      "Training set | Epoch 469 | MSE Loss: 0.0435\n",
      "Training set | Epoch 470 | MSE Loss: 0.03\n",
      "Training set | Epoch 471 | MSE Loss: 0.0281\n",
      "Training set | Epoch 472 | MSE Loss: 0.0366\n",
      "Training set | Epoch 473 | MSE Loss: 0.046\n",
      "Training set | Epoch 474 | MSE Loss: 0.0464\n",
      "Training set | Epoch 475 | MSE Loss: 0.0447\n",
      "Training set | Epoch 476 | MSE Loss: 0.0321\n",
      "Training set | Epoch 477 | MSE Loss: 0.028\n",
      "Training set | Epoch 478 | MSE Loss: 0.0328\n",
      "Training set | Epoch 479 | MSE Loss: 0.0394\n",
      "Training set | Epoch 480 | MSE Loss: 0.0378\n",
      "Training set | Epoch 481 | MSE Loss: 0.0303\n",
      "Training set | Epoch 482 | MSE Loss: 0.0278\n",
      "Training set | Epoch 483 | MSE Loss: 0.0274\n",
      "Training set | Epoch 484 | MSE Loss: 0.0321\n",
      "Training set | Epoch 485 | MSE Loss: 0.0323\n",
      "Training set | Epoch 486 | MSE Loss: 0.0294\n",
      "Training set | Epoch 487 | MSE Loss: 0.0266\n",
      "Training set | Epoch 488 | MSE Loss: 0.0268\n",
      "Training set | Epoch 489 | MSE Loss: 0.0317\n",
      "Training set | Epoch 490 | MSE Loss: 0.0307\n",
      "Training set | Epoch 491 | MSE Loss: 0.0278\n",
      "Training set | Epoch 492 | MSE Loss: 0.0257\n",
      "Training set | Epoch 493 | MSE Loss: 0.0253\n",
      "Training set | Epoch 494 | MSE Loss: 0.0276\n",
      "Training set | Epoch 495 | MSE Loss: 0.0279\n",
      "Training set | Epoch 496 | MSE Loss: 0.0271\n",
      "Training set | Epoch 497 | MSE Loss: 0.0254\n",
      "Training set | Epoch 498 | MSE Loss: 0.0243\n",
      "Training set | Epoch 499 | MSE Loss: 0.0261\n",
      "Training set | Epoch 500 | MSE Loss: 0.026\n",
      "Training set | Epoch 501 | MSE Loss: 0.0267\n",
      "Training set | Epoch 502 | MSE Loss: 0.0257\n",
      "Training set | Epoch 503 | MSE Loss: 0.0241\n",
      "Training set | Epoch 504 | MSE Loss: 0.0247\n",
      "Training set | Epoch 505 | MSE Loss: 0.0251\n",
      "Training set | Epoch 506 | MSE Loss: 0.0271\n",
      "Training set | Epoch 507 | MSE Loss: 0.0273\n",
      "Training set | Epoch 508 | MSE Loss: 0.0259\n",
      "Training set | Epoch 509 | MSE Loss: 0.0257\n",
      "Training set | Epoch 510 | MSE Loss: 0.0232\n",
      "Training set | Epoch 511 | MSE Loss: 0.0246\n",
      "Training set | Epoch 512 | MSE Loss: 0.0261\n",
      "Training set | Epoch 513 | MSE Loss: 0.0268\n",
      "Training set | Epoch 514 | MSE Loss: 0.028\n",
      "Training set | Epoch 515 | MSE Loss: 0.0248\n",
      "Training set | Epoch 516 | MSE Loss: 0.0267\n",
      "Training set | Epoch 517 | MSE Loss: 0.0236\n",
      "Training set | Epoch 518 | MSE Loss: 0.0244\n",
      "Training set | Epoch 519 | MSE Loss: 0.0281\n",
      "Training set | Epoch 520 | MSE Loss: 0.027\n",
      "Training set | Epoch 521 | MSE Loss: 0.0315\n",
      "Training set | Epoch 522 | MSE Loss: 0.028\n",
      "Training set | Epoch 523 | MSE Loss: 0.0272\n",
      "Training set | Epoch 524 | MSE Loss: 0.0251\n",
      "Training set | Epoch 525 | MSE Loss: 0.0226\n",
      "Training set | Epoch 526 | MSE Loss: 0.0237\n",
      "Training set | Epoch 527 | MSE Loss: 0.0227\n",
      "Training set | Epoch 528 | MSE Loss: 0.0233\n",
      "Training set | Epoch 529 | MSE Loss: 0.0231\n",
      "Training set | Epoch 530 | MSE Loss: 0.0224\n",
      "Training set | Epoch 531 | MSE Loss: 0.023\n",
      "Training set | Epoch 532 | MSE Loss: 0.0223\n",
      "Training set | Epoch 533 | MSE Loss: 0.0225\n",
      "Training set | Epoch 534 | MSE Loss: 0.0231\n",
      "Training set | Epoch 535 | MSE Loss: 0.0224\n",
      "Training set | Epoch 536 | MSE Loss: 0.0228\n",
      "Training set | Epoch 537 | MSE Loss: 0.0222\n",
      "Training set | Epoch 538 | MSE Loss: 0.022\n",
      "Training set | Epoch 539 | MSE Loss: 0.0222\n",
      "Training set | Epoch 540 | MSE Loss: 0.0223\n",
      "Training set | Epoch 541 | MSE Loss: 0.0221\n",
      "Training set | Epoch 542 | MSE Loss: 0.0236\n",
      "Training set | Epoch 543 | MSE Loss: 0.0231\n",
      "Training set | Epoch 544 | MSE Loss: 0.0244\n",
      "Training set | Epoch 545 | MSE Loss: 0.0242\n",
      "Training set | Epoch 546 | MSE Loss: 0.0246\n",
      "Training set | Epoch 547 | MSE Loss: 0.0252\n",
      "Training set | Epoch 548 | MSE Loss: 0.0267\n",
      "Training set | Epoch 549 | MSE Loss: 0.0278\n",
      "Training set | Epoch 550 | MSE Loss: 0.0324\n",
      "Training set | Epoch 551 | MSE Loss: 0.034\n",
      "Training set | Epoch 552 | MSE Loss: 0.0423\n",
      "Training set | Epoch 553 | MSE Loss: 0.0516\n",
      "Training set | Epoch 554 | MSE Loss: 0.0805\n",
      "Training set | Epoch 555 | MSE Loss: 0.1001\n",
      "Training set | Epoch 556 | MSE Loss: 0.1298\n",
      "Training set | Epoch 557 | MSE Loss: 0.1006\n",
      "Training set | Epoch 558 | MSE Loss: 0.0427\n",
      "Training set | Epoch 559 | MSE Loss: 0.0291\n",
      "Training set | Epoch 560 | MSE Loss: 0.0579\n",
      "Training set | Epoch 561 | MSE Loss: 0.0778\n",
      "Training set | Epoch 562 | MSE Loss: 0.0365\n",
      "Training set | Epoch 563 | MSE Loss: 0.0279\n",
      "Training set | Epoch 564 | MSE Loss: 0.0493\n",
      "Training set | Epoch 565 | MSE Loss: 0.0462\n",
      "Training set | Epoch 566 | MSE Loss: 0.0262\n",
      "Training set | Epoch 567 | MSE Loss: 0.0292\n",
      "Training set | Epoch 568 | MSE Loss: 0.0415\n",
      "Training set | Epoch 569 | MSE Loss: 0.0311\n",
      "Training set | Epoch 570 | MSE Loss: 0.0237\n",
      "Training set | Epoch 571 | MSE Loss: 0.0332\n",
      "Training set | Epoch 572 | MSE Loss: 0.0363\n",
      "Training set | Epoch 573 | MSE Loss: 0.0226\n",
      "Training set | Epoch 574 | MSE Loss: 0.029\n",
      "Training set | Epoch 575 | MSE Loss: 0.0343\n",
      "Training set | Epoch 576 | MSE Loss: 0.027\n",
      "Training set | Epoch 577 | MSE Loss: 0.023\n",
      "Training set | Epoch 578 | MSE Loss: 0.0315\n",
      "Training set | Epoch 579 | MSE Loss: 0.028\n",
      "Training set | Epoch 580 | MSE Loss: 0.0211\n",
      "Training set | Epoch 581 | MSE Loss: 0.0276\n",
      "Training set | Epoch 582 | MSE Loss: 0.0267\n",
      "Training set | Epoch 583 | MSE Loss: 0.0214\n",
      "Training set | Epoch 584 | MSE Loss: 0.0238\n",
      "Training set | Epoch 585 | MSE Loss: 0.0264\n",
      "Training set | Epoch 586 | MSE Loss: 0.0224\n",
      "Training set | Epoch 587 | MSE Loss: 0.0213\n",
      "Training set | Epoch 588 | MSE Loss: 0.0243\n",
      "Training set | Epoch 589 | MSE Loss: 0.023\n",
      "Training set | Epoch 590 | MSE Loss: 0.0208\n",
      "Training set | Epoch 591 | MSE Loss: 0.0219\n",
      "Training set | Epoch 592 | MSE Loss: 0.0226\n",
      "Training set | Epoch 593 | MSE Loss: 0.0204\n",
      "Training set | Epoch 594 | MSE Loss: 0.0208\n",
      "Training set | Epoch 595 | MSE Loss: 0.022\n",
      "Training set | Epoch 596 | MSE Loss: 0.0204\n",
      "Training set | Epoch 597 | MSE Loss: 0.0201\n",
      "Training set | Epoch 598 | MSE Loss: 0.021\n",
      "Training set | Epoch 599 | MSE Loss: 0.0208\n",
      "Training set | Epoch 600 | MSE Loss: 0.0198\n",
      "Training set | Epoch 601 | MSE Loss: 0.0197\n",
      "Training set | Epoch 602 | MSE Loss: 0.0205\n",
      "Training set | Epoch 603 | MSE Loss: 0.0204\n",
      "Training set | Epoch 604 | MSE Loss: 0.0196\n",
      "Training set | Epoch 605 | MSE Loss: 0.0197\n",
      "Training set | Epoch 606 | MSE Loss: 0.0199\n",
      "Training set | Epoch 607 | MSE Loss: 0.0198\n",
      "Training set | Epoch 608 | MSE Loss: 0.0191\n",
      "Training set | Epoch 609 | MSE Loss: 0.0196\n",
      "Training set | Epoch 610 | MSE Loss: 0.0195\n",
      "Training set | Epoch 611 | MSE Loss: 0.0193\n",
      "Training set | Epoch 612 | MSE Loss: 0.0189\n",
      "Training set | Epoch 613 | MSE Loss: 0.0193\n",
      "Training set | Epoch 614 | MSE Loss: 0.0203\n",
      "Training set | Epoch 615 | MSE Loss: 0.0192\n",
      "Training set | Epoch 616 | MSE Loss: 0.0194\n",
      "Training set | Epoch 617 | MSE Loss: 0.0191\n",
      "Training set | Epoch 618 | MSE Loss: 0.0199\n",
      "Training set | Epoch 619 | MSE Loss: 0.0197\n",
      "Training set | Epoch 620 | MSE Loss: 0.0186\n",
      "Training set | Epoch 621 | MSE Loss: 0.0195\n",
      "Training set | Epoch 622 | MSE Loss: 0.0196\n",
      "Training set | Epoch 623 | MSE Loss: 0.0192\n",
      "Training set | Epoch 624 | MSE Loss: 0.0186\n",
      "Training set | Epoch 625 | MSE Loss: 0.0186\n",
      "Training set | Epoch 626 | MSE Loss: 0.0195\n",
      "Training set | Epoch 627 | MSE Loss: 0.0197\n",
      "Training set | Epoch 628 | MSE Loss: 0.019\n",
      "Training set | Epoch 629 | MSE Loss: 0.0188\n",
      "Training set | Epoch 630 | MSE Loss: 0.0184\n",
      "Training set | Epoch 631 | MSE Loss: 0.0192\n",
      "Training set | Epoch 632 | MSE Loss: 0.019\n",
      "Training set | Epoch 633 | MSE Loss: 0.0186\n",
      "Training set | Epoch 634 | MSE Loss: 0.0193\n",
      "Training set | Epoch 635 | MSE Loss: 0.0181\n",
      "Training set | Epoch 636 | MSE Loss: 0.019\n",
      "Training set | Epoch 637 | MSE Loss: 0.02\n",
      "Training set | Epoch 638 | MSE Loss: 0.019\n",
      "Training set | Epoch 639 | MSE Loss: 0.0197\n",
      "Training set | Epoch 640 | MSE Loss: 0.0179\n",
      "Training set | Epoch 641 | MSE Loss: 0.0189\n",
      "Training set | Epoch 642 | MSE Loss: 0.0191\n",
      "Training set | Epoch 643 | MSE Loss: 0.019\n",
      "Training set | Epoch 644 | MSE Loss: 0.0211\n",
      "Training set | Epoch 645 | MSE Loss: 0.0187\n",
      "Training set | Epoch 646 | MSE Loss: 0.0195\n",
      "Training set | Epoch 647 | MSE Loss: 0.0182\n",
      "Training set | Epoch 648 | MSE Loss: 0.0184\n",
      "Training set | Epoch 649 | MSE Loss: 0.0193\n",
      "Training set | Epoch 650 | MSE Loss: 0.018\n",
      "Training set | Epoch 651 | MSE Loss: 0.0183\n",
      "Training set | Epoch 652 | MSE Loss: 0.0182\n",
      "Training set | Epoch 653 | MSE Loss: 0.0182\n",
      "Training set | Epoch 654 | MSE Loss: 0.0187\n",
      "Training set | Epoch 655 | MSE Loss: 0.0178\n",
      "Training set | Epoch 656 | MSE Loss: 0.0181\n",
      "Training set | Epoch 657 | MSE Loss: 0.0181\n",
      "Training set | Epoch 658 | MSE Loss: 0.0181\n",
      "Training set | Epoch 659 | MSE Loss: 0.0193\n",
      "Training set | Epoch 660 | MSE Loss: 0.0187\n",
      "Training set | Epoch 661 | MSE Loss: 0.019\n",
      "Training set | Epoch 662 | MSE Loss: 0.018\n",
      "Training set | Epoch 663 | MSE Loss: 0.0174\n",
      "Training set | Epoch 664 | MSE Loss: 0.0176\n",
      "Training set | Epoch 665 | MSE Loss: 0.0174\n",
      "Training set | Epoch 666 | MSE Loss: 0.0175\n",
      "Training set | Epoch 667 | MSE Loss: 0.0174\n",
      "Training set | Epoch 668 | MSE Loss: 0.0172\n",
      "Training set | Epoch 669 | MSE Loss: 0.0173\n",
      "Training set | Epoch 670 | MSE Loss: 0.0174\n",
      "Training set | Epoch 671 | MSE Loss: 0.017\n",
      "Training set | Epoch 672 | MSE Loss: 0.0173\n",
      "Training set | Epoch 673 | MSE Loss: 0.0169\n",
      "Training set | Epoch 674 | MSE Loss: 0.017\n",
      "Training set | Epoch 675 | MSE Loss: 0.0169\n",
      "Training set | Epoch 676 | MSE Loss: 0.0168\n",
      "Training set | Epoch 677 | MSE Loss: 0.0168\n",
      "Training set | Epoch 678 | MSE Loss: 0.017\n",
      "Training set | Epoch 679 | MSE Loss: 0.0168\n",
      "Training set | Epoch 680 | MSE Loss: 0.017\n",
      "Training set | Epoch 681 | MSE Loss: 0.0172\n",
      "Training set | Epoch 682 | MSE Loss: 0.0184\n",
      "Training set | Epoch 683 | MSE Loss: 0.0196\n",
      "Training set | Epoch 684 | MSE Loss: 0.0228\n",
      "Training set | Epoch 685 | MSE Loss: 0.0252\n",
      "Training set | Epoch 686 | MSE Loss: 0.0292\n",
      "Training set | Epoch 687 | MSE Loss: 0.0333\n",
      "Training set | Epoch 688 | MSE Loss: 0.0488\n",
      "Training set | Epoch 689 | MSE Loss: 0.0602\n",
      "Training set | Epoch 690 | MSE Loss: 0.0882\n",
      "Training set | Epoch 691 | MSE Loss: 0.1043\n",
      "Training set | Epoch 692 | MSE Loss: 0.1189\n",
      "Training set | Epoch 693 | MSE Loss: 0.0879\n",
      "Training set | Epoch 694 | MSE Loss: 0.0422\n",
      "Training set | Epoch 695 | MSE Loss: 0.0206\n",
      "Training set | Epoch 696 | MSE Loss: 0.046\n",
      "Training set | Epoch 697 | MSE Loss: 0.0548\n",
      "Training set | Epoch 698 | MSE Loss: 0.0277\n",
      "Training set | Epoch 699 | MSE Loss: 0.0217\n",
      "Training set | Epoch 700 | MSE Loss: 0.0397\n",
      "Training set | Epoch 701 | MSE Loss: 0.035\n",
      "Training set | Epoch 702 | MSE Loss: 0.0193\n",
      "Training set | Epoch 703 | MSE Loss: 0.0258\n",
      "Training set | Epoch 704 | MSE Loss: 0.0329\n",
      "Training set | Epoch 705 | MSE Loss: 0.0201\n",
      "Training set | Epoch 706 | MSE Loss: 0.0211\n",
      "Training set | Epoch 707 | MSE Loss: 0.028\n",
      "Training set | Epoch 708 | MSE Loss: 0.0229\n",
      "Training set | Epoch 709 | MSE Loss: 0.0182\n",
      "Training set | Epoch 710 | MSE Loss: 0.025\n",
      "Training set | Epoch 711 | MSE Loss: 0.0237\n",
      "Training set | Epoch 712 | MSE Loss: 0.0172\n",
      "Training set | Epoch 713 | MSE Loss: 0.0224\n",
      "Training set | Epoch 714 | MSE Loss: 0.0221\n",
      "Training set | Epoch 715 | MSE Loss: 0.0181\n",
      "Training set | Epoch 716 | MSE Loss: 0.019\n",
      "Training set | Epoch 717 | MSE Loss: 0.0212\n",
      "Training set | Epoch 718 | MSE Loss: 0.0184\n",
      "Training set | Epoch 719 | MSE Loss: 0.0171\n",
      "Training set | Epoch 720 | MSE Loss: 0.0197\n",
      "Training set | Epoch 721 | MSE Loss: 0.018\n",
      "Training set | Epoch 722 | MSE Loss: 0.0167\n",
      "Training set | Epoch 723 | MSE Loss: 0.0183\n",
      "Training set | Epoch 724 | MSE Loss: 0.0178\n",
      "Training set | Epoch 725 | MSE Loss: 0.0164\n",
      "Training set | Epoch 726 | MSE Loss: 0.0171\n",
      "Training set | Epoch 727 | MSE Loss: 0.0177\n",
      "Training set | Epoch 728 | MSE Loss: 0.0164\n",
      "Training set | Epoch 729 | MSE Loss: 0.0163\n",
      "Training set | Epoch 730 | MSE Loss: 0.017\n",
      "Training set | Epoch 731 | MSE Loss: 0.0166\n",
      "Training set | Epoch 732 | MSE Loss: 0.016\n",
      "Training set | Epoch 733 | MSE Loss: 0.0164\n",
      "Training set | Epoch 734 | MSE Loss: 0.0165\n",
      "Training set | Epoch 735 | MSE Loss: 0.0159\n",
      "Training set | Epoch 736 | MSE Loss: 0.0159\n",
      "Training set | Epoch 737 | MSE Loss: 0.0163\n",
      "Training set | Epoch 738 | MSE Loss: 0.0159\n",
      "Training set | Epoch 739 | MSE Loss: 0.0158\n",
      "Training set | Epoch 740 | MSE Loss: 0.0161\n",
      "Training set | Epoch 741 | MSE Loss: 0.016\n",
      "Training set | Epoch 742 | MSE Loss: 0.0157\n",
      "Training set | Epoch 743 | MSE Loss: 0.0158\n",
      "Training set | Epoch 744 | MSE Loss: 0.016\n",
      "Training set | Epoch 745 | MSE Loss: 0.0162\n",
      "Training set | Epoch 746 | MSE Loss: 0.0157\n",
      "Training set | Epoch 747 | MSE Loss: 0.0159\n",
      "Training set | Epoch 748 | MSE Loss: 0.0161\n",
      "Training set | Epoch 749 | MSE Loss: 0.0158\n",
      "Training set | Epoch 750 | MSE Loss: 0.0158\n",
      "Training set | Epoch 751 | MSE Loss: 0.0156\n",
      "Training set | Epoch 752 | MSE Loss: 0.016\n",
      "Training set | Epoch 753 | MSE Loss: 0.0159\n",
      "Training set | Epoch 754 | MSE Loss: 0.0155\n",
      "Training set | Epoch 755 | MSE Loss: 0.0157\n",
      "Training set | Epoch 756 | MSE Loss: 0.0157\n",
      "Training set | Epoch 757 | MSE Loss: 0.0159\n",
      "Training set | Epoch 758 | MSE Loss: 0.0163\n",
      "Training set | Epoch 759 | MSE Loss: 0.0154\n",
      "Training set | Epoch 760 | MSE Loss: 0.0159\n",
      "Training set | Epoch 761 | MSE Loss: 0.0166\n",
      "Training set | Epoch 762 | MSE Loss: 0.0158\n",
      "Training set | Epoch 763 | MSE Loss: 0.016\n",
      "Training set | Epoch 764 | MSE Loss: 0.0155\n",
      "Training set | Epoch 765 | MSE Loss: 0.0158\n",
      "Training set | Epoch 766 | MSE Loss: 0.016\n",
      "Training set | Epoch 767 | MSE Loss: 0.0153\n",
      "Training set | Epoch 768 | MSE Loss: 0.0157\n",
      "Training set | Epoch 769 | MSE Loss: 0.0156\n",
      "Training set | Epoch 770 | MSE Loss: 0.0158\n",
      "Training set | Epoch 771 | MSE Loss: 0.0158\n",
      "Training set | Epoch 772 | MSE Loss: 0.0152\n",
      "Training set | Epoch 773 | MSE Loss: 0.0157\n",
      "Training set | Epoch 774 | MSE Loss: 0.0163\n",
      "Training set | Epoch 775 | MSE Loss: 0.0158\n",
      "Training set | Epoch 776 | MSE Loss: 0.0164\n",
      "Training set | Epoch 777 | MSE Loss: 0.0153\n",
      "Training set | Epoch 778 | MSE Loss: 0.0155\n",
      "Training set | Epoch 779 | MSE Loss: 0.0159\n",
      "Training set | Epoch 780 | MSE Loss: 0.0153\n",
      "Training set | Epoch 781 | MSE Loss: 0.0153\n",
      "Training set | Epoch 782 | MSE Loss: 0.0152\n",
      "Training set | Epoch 783 | MSE Loss: 0.0151\n",
      "Training set | Epoch 784 | MSE Loss: 0.0154\n",
      "Training set | Epoch 785 | MSE Loss: 0.0151\n",
      "Training set | Epoch 786 | MSE Loss: 0.015\n",
      "Training set | Epoch 787 | MSE Loss: 0.015\n",
      "Training set | Epoch 788 | MSE Loss: 0.0149\n",
      "Training set | Epoch 789 | MSE Loss: 0.015\n",
      "Training set | Epoch 790 | MSE Loss: 0.0149\n",
      "Training set | Epoch 791 | MSE Loss: 0.0149\n",
      "Training set | Epoch 792 | MSE Loss: 0.0149\n",
      "Training set | Epoch 793 | MSE Loss: 0.0149\n",
      "Training set | Epoch 794 | MSE Loss: 0.0148\n",
      "Training set | Epoch 795 | MSE Loss: 0.0149\n",
      "Training set | Epoch 796 | MSE Loss: 0.0148\n",
      "Training set | Epoch 797 | MSE Loss: 0.0148\n",
      "Training set | Epoch 798 | MSE Loss: 0.0149\n",
      "Training set | Epoch 799 | MSE Loss: 0.015\n",
      "Training set | Epoch 800 | MSE Loss: 0.0148\n",
      "Training set | Epoch 801 | MSE Loss: 0.015\n",
      "Training set | Epoch 802 | MSE Loss: 0.015\n",
      "Training set | Epoch 803 | MSE Loss: 0.015\n",
      "Training set | Epoch 804 | MSE Loss: 0.015\n",
      "Training set | Epoch 805 | MSE Loss: 0.0153\n",
      "Training set | Epoch 806 | MSE Loss: 0.015\n",
      "Training set | Epoch 807 | MSE Loss: 0.0149\n",
      "Training set | Epoch 808 | MSE Loss: 0.015\n",
      "Training set | Epoch 809 | MSE Loss: 0.0148\n",
      "Training set | Epoch 810 | MSE Loss: 0.0147\n",
      "Training set | Epoch 811 | MSE Loss: 0.0149\n",
      "Training set | Epoch 812 | MSE Loss: 0.0146\n",
      "Training set | Epoch 813 | MSE Loss: 0.0147\n",
      "Training set | Epoch 814 | MSE Loss: 0.0147\n",
      "Training set | Epoch 815 | MSE Loss: 0.0146\n",
      "Training set | Epoch 816 | MSE Loss: 0.0146\n",
      "Training set | Epoch 817 | MSE Loss: 0.0146\n",
      "Training set | Epoch 818 | MSE Loss: 0.0146\n",
      "Training set | Epoch 819 | MSE Loss: 0.0146\n",
      "Training set | Epoch 820 | MSE Loss: 0.0149\n",
      "Training set | Epoch 821 | MSE Loss: 0.0148\n",
      "Training set | Epoch 822 | MSE Loss: 0.0147\n",
      "Training set | Epoch 823 | MSE Loss: 0.015\n",
      "Training set | Epoch 824 | MSE Loss: 0.0151\n",
      "Training set | Epoch 825 | MSE Loss: 0.0152\n",
      "Training set | Epoch 826 | MSE Loss: 0.0172\n",
      "Training set | Epoch 827 | MSE Loss: 0.0186\n",
      "Training set | Epoch 828 | MSE Loss: 0.0245\n",
      "Training set | Epoch 829 | MSE Loss: 0.0314\n",
      "Training set | Epoch 830 | MSE Loss: 0.0446\n",
      "Training set | Epoch 831 | MSE Loss: 0.0583\n",
      "Training set | Epoch 832 | MSE Loss: 0.0884\n",
      "Training set | Epoch 833 | MSE Loss: 0.1203\n",
      "Training set | Epoch 834 | MSE Loss: 0.192\n",
      "Training set | Epoch 835 | MSE Loss: 0.1569\n",
      "Training set | Epoch 836 | MSE Loss: 0.077\n",
      "Training set | Epoch 837 | MSE Loss: 0.0349\n",
      "Training set | Epoch 838 | MSE Loss: 0.0705\n",
      "Training set | Epoch 839 | MSE Loss: 0.096\n",
      "Training set | Epoch 840 | MSE Loss: 0.0807\n",
      "Training set | Epoch 841 | MSE Loss: 0.2469\n",
      "Training set | Epoch 842 | MSE Loss: 1.0577\n",
      "Training set | Epoch 843 | MSE Loss: 0.8299\n",
      "Training set | Epoch 844 | MSE Loss: 0.932\n",
      "Training set | Epoch 845 | MSE Loss: 0.6241\n",
      "Training set | Epoch 846 | MSE Loss: 0.4052\n",
      "Training set | Epoch 847 | MSE Loss: 0.5145\n",
      "Training set | Epoch 848 | MSE Loss: 0.4274\n",
      "Training set | Epoch 849 | MSE Loss: 0.3192\n",
      "Training set | Epoch 850 | MSE Loss: 0.3254\n",
      "Training set | Epoch 851 | MSE Loss: 0.3534\n",
      "Training set | Epoch 852 | MSE Loss: 0.2452\n",
      "Training set | Epoch 853 | MSE Loss: 0.2401\n",
      "Training set | Epoch 854 | MSE Loss: 0.2064\n",
      "Training set | Epoch 855 | MSE Loss: 0.1779\n",
      "Training set | Epoch 856 | MSE Loss: 0.2027\n",
      "Training set | Epoch 857 | MSE Loss: 0.1662\n",
      "Training set | Epoch 858 | MSE Loss: 0.1415\n",
      "Training set | Epoch 859 | MSE Loss: 0.1304\n",
      "Training set | Epoch 860 | MSE Loss: 0.1115\n",
      "Training set | Epoch 861 | MSE Loss: 0.105\n",
      "Training set | Epoch 862 | MSE Loss: 0.0956\n",
      "Training set | Epoch 863 | MSE Loss: 0.1007\n",
      "Training set | Epoch 864 | MSE Loss: 0.0896\n",
      "Training set | Epoch 865 | MSE Loss: 0.0747\n",
      "Training set | Epoch 866 | MSE Loss: 0.0731\n",
      "Training set | Epoch 867 | MSE Loss: 0.0635\n",
      "Training set | Epoch 868 | MSE Loss: 0.0649\n",
      "Training set | Epoch 869 | MSE Loss: 0.0624\n",
      "Training set | Epoch 870 | MSE Loss: 0.0584\n",
      "Training set | Epoch 871 | MSE Loss: 0.055\n",
      "Training set | Epoch 872 | MSE Loss: 0.0457\n",
      "Training set | Epoch 873 | MSE Loss: 0.0451\n",
      "Training set | Epoch 874 | MSE Loss: 0.0382\n",
      "Training set | Epoch 875 | MSE Loss: 0.0388\n",
      "Training set | Epoch 876 | MSE Loss: 0.042\n",
      "Training set | Epoch 877 | MSE Loss: 0.0385\n",
      "Training set | Epoch 878 | MSE Loss: 0.0383\n",
      "Training set | Epoch 879 | MSE Loss: 0.0339\n",
      "Training set | Epoch 880 | MSE Loss: 0.0306\n",
      "Training set | Epoch 881 | MSE Loss: 0.0293\n",
      "Training set | Epoch 882 | MSE Loss: 0.0276\n",
      "Training set | Epoch 883 | MSE Loss: 0.0293\n",
      "Training set | Epoch 884 | MSE Loss: 0.0284\n",
      "Training set | Epoch 885 | MSE Loss: 0.0268\n",
      "Training set | Epoch 886 | MSE Loss: 0.0251\n",
      "Training set | Epoch 887 | MSE Loss: 0.0239\n",
      "Training set | Epoch 888 | MSE Loss: 0.0237\n",
      "Training set | Epoch 889 | MSE Loss: 0.023\n",
      "Training set | Epoch 890 | MSE Loss: 0.0229\n",
      "Training set | Epoch 891 | MSE Loss: 0.0221\n",
      "Training set | Epoch 892 | MSE Loss: 0.0215\n",
      "Training set | Epoch 893 | MSE Loss: 0.021\n",
      "Training set | Epoch 894 | MSE Loss: 0.0202\n",
      "Training set | Epoch 895 | MSE Loss: 0.02\n",
      "Training set | Epoch 896 | MSE Loss: 0.0198\n",
      "Training set | Epoch 897 | MSE Loss: 0.0195\n",
      "Training set | Epoch 898 | MSE Loss: 0.0193\n",
      "Training set | Epoch 899 | MSE Loss: 0.0188\n",
      "Training set | Epoch 900 | MSE Loss: 0.0187\n",
      "Training set | Epoch 901 | MSE Loss: 0.0183\n",
      "Training set | Epoch 902 | MSE Loss: 0.018\n",
      "Training set | Epoch 903 | MSE Loss: 0.0181\n",
      "Training set | Epoch 904 | MSE Loss: 0.0176\n",
      "Training set | Epoch 905 | MSE Loss: 0.0177\n",
      "Training set | Epoch 906 | MSE Loss: 0.0175\n",
      "Training set | Epoch 907 | MSE Loss: 0.0172\n",
      "Training set | Epoch 908 | MSE Loss: 0.0172\n",
      "Training set | Epoch 909 | MSE Loss: 0.0169\n",
      "Training set | Epoch 910 | MSE Loss: 0.0168\n",
      "Training set | Epoch 911 | MSE Loss: 0.0168\n",
      "Training set | Epoch 912 | MSE Loss: 0.0166\n",
      "Training set | Epoch 913 | MSE Loss: 0.0165\n",
      "Training set | Epoch 914 | MSE Loss: 0.0165\n",
      "Training set | Epoch 915 | MSE Loss: 0.0163\n",
      "Training set | Epoch 916 | MSE Loss: 0.0163\n",
      "Training set | Epoch 917 | MSE Loss: 0.0162\n",
      "Training set | Epoch 918 | MSE Loss: 0.0161\n",
      "Training set | Epoch 919 | MSE Loss: 0.016\n",
      "Training set | Epoch 920 | MSE Loss: 0.016\n",
      "Training set | Epoch 921 | MSE Loss: 0.0159\n",
      "Training set | Epoch 922 | MSE Loss: 0.0158\n",
      "Training set | Epoch 923 | MSE Loss: 0.0158\n",
      "Training set | Epoch 924 | MSE Loss: 0.0158\n",
      "Training set | Epoch 925 | MSE Loss: 0.0157\n",
      "Training set | Epoch 926 | MSE Loss: 0.0157\n",
      "Training set | Epoch 927 | MSE Loss: 0.0156\n",
      "Training set | Epoch 928 | MSE Loss: 0.0155\n",
      "Training set | Epoch 929 | MSE Loss: 0.0155\n",
      "Training set | Epoch 930 | MSE Loss: 0.0154\n",
      "Training set | Epoch 931 | MSE Loss: 0.0154\n",
      "Training set | Epoch 932 | MSE Loss: 0.0154\n",
      "Training set | Epoch 933 | MSE Loss: 0.0153\n",
      "Training set | Epoch 934 | MSE Loss: 0.0153\n",
      "Training set | Epoch 935 | MSE Loss: 0.0152\n",
      "Training set | Epoch 936 | MSE Loss: 0.0152\n",
      "Training set | Epoch 937 | MSE Loss: 0.0152\n",
      "Training set | Epoch 938 | MSE Loss: 0.0151\n",
      "Training set | Epoch 939 | MSE Loss: 0.0151\n",
      "Training set | Epoch 940 | MSE Loss: 0.0151\n",
      "Training set | Epoch 941 | MSE Loss: 0.0151\n",
      "Training set | Epoch 942 | MSE Loss: 0.0151\n",
      "Training set | Epoch 943 | MSE Loss: 0.015\n",
      "Training set | Epoch 944 | MSE Loss: 0.015\n",
      "Training set | Epoch 945 | MSE Loss: 0.015\n",
      "Training set | Epoch 946 | MSE Loss: 0.0149\n",
      "Training set | Epoch 947 | MSE Loss: 0.0149\n",
      "Training set | Epoch 948 | MSE Loss: 0.0149\n",
      "Training set | Epoch 949 | MSE Loss: 0.0149\n",
      "Training set | Epoch 950 | MSE Loss: 0.0148\n",
      "Training set | Epoch 951 | MSE Loss: 0.0148\n",
      "Training set | Epoch 952 | MSE Loss: 0.0148\n",
      "Training set | Epoch 953 | MSE Loss: 0.0148\n",
      "Training set | Epoch 954 | MSE Loss: 0.0147\n",
      "Training set | Epoch 955 | MSE Loss: 0.0147\n",
      "Training set | Epoch 956 | MSE Loss: 0.0147\n",
      "Training set | Epoch 957 | MSE Loss: 0.0147\n",
      "Training set | Epoch 958 | MSE Loss: 0.0146\n",
      "Training set | Epoch 959 | MSE Loss: 0.0146\n",
      "Training set | Epoch 960 | MSE Loss: 0.0146\n",
      "Training set | Epoch 961 | MSE Loss: 0.0146\n",
      "Training set | Epoch 962 | MSE Loss: 0.0146\n",
      "Training set | Epoch 963 | MSE Loss: 0.0145\n",
      "Training set | Epoch 964 | MSE Loss: 0.0145\n",
      "Training set | Epoch 965 | MSE Loss: 0.0145\n",
      "Training set | Epoch 966 | MSE Loss: 0.0145\n",
      "Training set | Epoch 967 | MSE Loss: 0.0145\n",
      "Training set | Epoch 968 | MSE Loss: 0.0145\n",
      "Training set | Epoch 969 | MSE Loss: 0.0145\n",
      "Training set | Epoch 970 | MSE Loss: 0.0144\n",
      "Training set | Epoch 971 | MSE Loss: 0.0144\n",
      "Training set | Epoch 972 | MSE Loss: 0.0144\n",
      "Training set | Epoch 973 | MSE Loss: 0.0144\n",
      "Training set | Epoch 974 | MSE Loss: 0.0144\n",
      "Training set | Epoch 975 | MSE Loss: 0.0143\n",
      "Training set | Epoch 976 | MSE Loss: 0.0143\n",
      "Training set | Epoch 977 | MSE Loss: 0.0143\n",
      "Training set | Epoch 978 | MSE Loss: 0.0143\n",
      "Training set | Epoch 979 | MSE Loss: 0.0143\n",
      "Training set | Epoch 980 | MSE Loss: 0.0142\n",
      "Training set | Epoch 981 | MSE Loss: 0.0142\n",
      "Training set | Epoch 982 | MSE Loss: 0.0142\n",
      "Training set | Epoch 983 | MSE Loss: 0.0142\n",
      "Training set | Epoch 984 | MSE Loss: 0.0142\n",
      "Training set | Epoch 985 | MSE Loss: 0.0142\n",
      "Training set | Epoch 986 | MSE Loss: 0.0141\n",
      "Training set | Epoch 987 | MSE Loss: 0.0142\n",
      "Training set | Epoch 988 | MSE Loss: 0.0142\n",
      "Training set | Epoch 989 | MSE Loss: 0.0141\n",
      "Training set | Epoch 990 | MSE Loss: 0.0141\n",
      "Training set | Epoch 991 | MSE Loss: 0.0141\n",
      "Training set | Epoch 992 | MSE Loss: 0.0141\n",
      "Training set | Epoch 993 | MSE Loss: 0.0141\n",
      "Training set | Epoch 994 | MSE Loss: 0.0141\n",
      "Training set | Epoch 995 | MSE Loss: 0.0142\n",
      "Training set | Epoch 996 | MSE Loss: 0.0141\n",
      "Training set | Epoch 997 | MSE Loss: 0.0141\n",
      "Training set | Epoch 998 | MSE Loss: 0.0142\n",
      "Training set | Epoch 999 | MSE Loss: 0.0141\n",
      "Training set | Epoch 1000 | MSE Loss: 0.014\n"
     ]
    }
   ],
   "source": [
    "D = 3\n",
    "H = 64\n",
    "\n",
    "model = SimpleLSTM(D, H, 1).to(device)\n",
    "print(model)\n",
    "print()\n",
    "print(\"Total parameters:\", f\"{sum(p.numel() for p in model.parameters()):,}\")\n",
    "print()\n",
    "model.fit(epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | MSE loss: 6.4735 | Total matched 9 out of 13 (Accuracy: 69.23%)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "\n",
    "f = open(f\"test_results/test_result_w_succ_diff_nodes_lstm_{datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")}.csv\", \"w\", newline=\"\")\n",
    "csv_writer = csv.writer(f)\n",
    "csv_writer.writerow([\"Dataset\", \"Actual\", \"Predicted\"])\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_concat_datasets = ConcatDataset(test_datasets)\n",
    "    test_batch_sampler = CustomBatchSampler(test_concat_datasets, batch_size=10240)\n",
    "    test_dataloader = DataLoader(test_concat_datasets, batch_sampler=test_batch_sampler)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_matched = 0\n",
    "    count = 0\n",
    "    for batch in test_dataloader:\n",
    "        x = batch[0]\n",
    "        x = x[0]\n",
    "        y = batch[1]\n",
    "        y = y.unsqueeze(-1)\n",
    "        out = model(x)\n",
    "        csv_writer.writerows(\n",
    "            (i, j.item(), k.item())\n",
    "            for (i, j, k) in zip(\n",
    "                 batch[0][1], y.detach().cpu().numpy(), out.detach().cpu().numpy()\n",
    "            )\n",
    "        )\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss\n",
    "        out = torch.round(out)\n",
    "        matched = (out == y).sum().item()\n",
    "        total_matched += matched\n",
    "        count += 1\n",
    "\n",
    "    print(\n",
    "        \"Test set\",\n",
    "        \"| MSE loss:\",\n",
    "        round((total_loss / count).item(), 4),\n",
    "        \"| Total matched\",\n",
    "        total_matched,\n",
    "        \"out of\",\n",
    "        len(test_concat_datasets),\n",
    "        f\"(Accuracy: {round(total_matched/len(test_concat_datasets) * 100, 2)}%)\",\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Untrained Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | star_graph_n13 | MSE loss: 6.7323 | Total matched 18,052 out of 53,248 (Accuracy: 33.9%)\n"
     ]
    }
   ],
   "source": [
    "dataset_s_n13 = CVFConfigForGCNWSuccLSTMDataset(\n",
    "    device,\n",
    "    \"star_graph_n13_config_rank_dataset.csv\",\n",
    "    \"star_graph_n13_edge_index.json\",\n",
    ")\n",
    "\n",
    "\n",
    "dataset = dataset_s_n13\n",
    "\n",
    "# dataset.set_transform(transform)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_dataloader = DataLoader(dataset, batch_size=10240)\n",
    "    total_loss = 0\n",
    "    total_matched = 0\n",
    "    count = 0\n",
    "    for batch in test_dataloader:\n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "        y = y.unsqueeze(-1)\n",
    "        out = model(x[0])\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss\n",
    "        out = torch.round(out)\n",
    "        matched = (out == y).sum().item()\n",
    "        total_matched += matched\n",
    "        count += 1\n",
    "\n",
    "    print(\n",
    "        \"Test set\",\n",
    "        f\"| {dataset.dataset_name}\",\n",
    "        \"| MSE loss:\",\n",
    "        round((total_loss / count).item(), 4),\n",
    "        \"| Total matched\",\n",
    "        f\"{total_matched:,}\",\n",
    "        \"out of\",\n",
    "        f\"{len(dataset):,}\",\n",
    "        f\"(Accuracy: {round(total_matched/len(dataset) * 100, 2)}%)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
