{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07edcb7c-e6c3-456c-9a5a-ce756ca68597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a8453c-852b-4dab-a887-e83a057456ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([\n",
    "    [0, 1],\n",
    "    [0, 2],\n",
    "    [0, 3],\n",
    "    [1, 0],\n",
    "    [2, 0],\n",
    "    [3, 0]\n",
    "], dtype=torch.long)\n",
    "\n",
    "x = torch.tensor([[0, 1], [0, 1], [0, 1], [0, 1]], dtype=torch.float) # configs: [0, 0, 0, 0], [1, 1, 1, 1]\n",
    "\n",
    "y = torch.tensor([[3.0], [1.0]]) # ranks\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "046f11a8-3fcd-4d0e-9bdc-f95b33e16f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6, 2, False, False, False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.num_nodes, data.num_edges, data.num_node_features, data.has_isolated_nodes(), data.has_self_loops(), data.is_directed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c28d9580-a55f-431d-91db-982af72d5ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y', 'x', 'edge_index']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5300d021-4bdf-49cc-913a-72c8072585e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = to_dense_adj(edge_index.t().contiguous())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b78390-3477-4a57-adc4-d8a89f220b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b570064-4ece-42a0-b252-08135943581b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85b44b15-1091-4337-ac96-56630cd0ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(data.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "        self.out = torch.nn.Linear(16, data.num_node_features)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = gap(x, None)\n",
    "\n",
    "        # print(x)\n",
    "        # return F.log_softmax(x, dim=1)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bf2fcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1209/3130078632.py:12: UserWarning: Using a target size (torch.Size([2, 1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(out, data.y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(4.8378, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.3658, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.8922, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.8940, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.9189, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.5488, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.4361, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.3193, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.3842, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.1622, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.6732, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.7890, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.2588, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1236, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.5756, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0346, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0296, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1503, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0527, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.4495, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.4061, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0168, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.6196, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.3550, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1903, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.5859, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0752, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1088, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0675, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2185, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2359, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.4867, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0183, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.5517, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2611, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2317, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0798, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1073, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2615, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0587, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2007, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0049, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0010, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1004, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0550, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0052, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0009, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0013, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0304, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0007, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1642, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0005, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0057, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0077, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2471, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1880, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1671, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1064, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1824, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1175, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0040, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0024, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2546, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1163, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0619, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0876, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1722, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1755, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1362, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0922, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0228, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0356, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0218, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1522, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.5526, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0048, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0079, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0003, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1432, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0625, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0175, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0593, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2467, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0283, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0411, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0742, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0089, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0648, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0542, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0051, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.4237, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0010, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0301, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1320, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.5101, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0180, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0326, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0291, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0010, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0950, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.3058, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0040, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0259, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2410, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0031, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0351, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0593, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0019, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0136, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0220, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0289, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2247, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0014, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1402, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0263, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1129, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0045, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0351, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1254, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0103, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0015, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0304, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0452, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0032, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1769, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0316, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0519, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1721, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1112, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0162, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0014, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1174, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1142, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0011, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1302, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0238, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1777, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0205, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0032, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0022, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0656, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0017, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0012, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2531, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2002, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0087, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0041, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0090, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0087, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0401, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0625, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1094, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0003, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2648, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0251, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0413, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0188, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.3825, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1096, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0156, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0520, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1476, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0003, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0249, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0359, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0703, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0108, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0742, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0255, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0971, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0413, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0875, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0018, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0156, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0030, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0345, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0037, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0114, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0137, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0453, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2529, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0010, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0061, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0992, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0346, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0018, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0150, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0013, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0215, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0293, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0778, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0497, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0220, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0562, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0118, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0229, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = data.to(device)\n",
    "# data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    # loss = F.nll_loss(out, data.y)\n",
    "    loss = F.mse_loss(out, data.y)\n",
    "    print(\"Loss:\", loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afa7270e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6490, 1.6121]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor([\n",
    "    [0, 1],\n",
    "    [0, 2],\n",
    "    [0, 3],\n",
    "    [1, 0],\n",
    "    [2, 0],\n",
    "    [3, 0]\n",
    "], dtype=torch.long)\n",
    "\n",
    "x = torch.tensor([[0, 1], [0, 1], [0, 1], [0, 1]], dtype=torch.float) # configs: [0, 0, 0, 0], [1, 1, 1, 1]\n",
    "\n",
    "test_data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "\n",
    "model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9df5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
