{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07edcb7c-e6c3-456c-9a5a-ce756ca68597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d3a8453c-852b-4dab-a887-e83a057456ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([\n",
    "    [0, 1],\n",
    "    [0, 2],\n",
    "    [0, 3],\n",
    "    [1, 0],\n",
    "    [2, 0],\n",
    "    [3, 0]\n",
    "], dtype=torch.long)\n",
    "\n",
    "x = torch.tensor([[0], [0], [0], [0]], dtype=torch.float) # configs: [0, 0, 0, 0], [1, 1, 1, 1]\n",
    "\n",
    "y = torch.tensor([[3.0]]) # ranks\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "046f11a8-3fcd-4d0e-9bdc-f95b33e16f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6, 1, False, False, False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.num_nodes, data.num_edges, data.num_node_features, data.has_isolated_nodes(), data.has_self_loops(), data.is_directed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c28d9580-a55f-431d-91db-982af72d5ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y', 'x', 'edge_index']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5300d021-4bdf-49cc-913a-72c8072585e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = to_dense_adj(edge_index.t().contiguous())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "34b78390-3477-4a57-adc4-d8a89f220b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8b570064-4ece-42a0-b252-08135943581b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "85b44b15-1091-4337-ac96-56630cd0ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(1, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "        self.out = torch.nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = gap(x, None)\n",
    "\n",
    "        # print(x)\n",
    "        # return F.log_softmax(x, dim=1)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2bf2fcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(8.2198, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(8.1626, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(8.1055, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(8.0487, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.9921, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.9357, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.8795, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.8236, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.7678, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.7123, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.6571, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.6020, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.5472, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.4927, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.4383, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.3843, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.3304, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.2768, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.2235, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.1704, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.1176, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.0650, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.0127, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.9607, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.9089, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.8574, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.8061, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.7551, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.7044, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.6539, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.6037, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.5538, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.5041, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.4547, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.4056, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.3567, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.3081, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.2598, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.2117, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.1640, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.1164, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.0692, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.0222, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.9755, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.9291, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.8829, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.8370, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.7914, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.7460, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.7009, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.6560, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.6115, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.5672, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.5231, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.4793, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.4358, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.3926, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.3496, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.3068, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.2643, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.2221, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.1802, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.1385, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.0970, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.0558, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.0149, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.9742, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.9338, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.8936, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.8537, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.8140, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.7745, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.7354, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.6964, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.6577, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.6193, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.5811, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.5431, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.5054, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.4680, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.4307, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.3938, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.3570, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.3205, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.2842, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.2482, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.2124, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.1768, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.1415, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.1064, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.0715, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.0369, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.0024, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.9683, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.9343, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.9006, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.8671, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.8338, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.8007, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.7679, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.7353, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.7029, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.6707, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.6388, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.6071, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.5755, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.5442, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.5132, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.4823, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.4516, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.4212, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.3909, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.3609, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.3311, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.3015, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.2721, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.2429, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.2139, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.1851, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.1566, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.1282, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.1000, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.0720, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.0443, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.0167, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.9893, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.9621, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.9351, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.9084, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.8818, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.8554, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.8292, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.8031, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.7773, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.7517, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.7262, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.7010, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.6759, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.6510, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.6263, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.6018, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.5774, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.5533, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.5293, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.5055, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.4819, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.4585, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.4352, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.4121, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.3892, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.3665, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.3439, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.3215, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.2993, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.2773, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.2554, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.2337, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.2122, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.1908, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.1696, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.1486, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.1277, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.1070, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.0864, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.0661, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.0458, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.0258, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.0059, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.9861, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.9665, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.9471, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.9278, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.9087, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.8898, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.8710, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.8523, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.8338, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.8154, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.7972, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.7792, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.7613, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.7435, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.7259, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.7084, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.6911, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.6739, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.6569, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.6400, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.6232, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.6066, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.5901, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.5738, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.5576, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.5415, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.5256, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.5098, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.4941, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.4786, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.4632, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.4480, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = data.to(device)\n",
    "# data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    # loss = F.nll_loss(out, data.y)\n",
    "    loss = F.mse_loss(out, data.y)\n",
    "    print(\"Loss:\", loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa7270e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
