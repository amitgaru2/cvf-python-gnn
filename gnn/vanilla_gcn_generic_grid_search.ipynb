{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "from torch.utils.data import ConcatDataset, Sampler, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import CVFConfigForGCNGridSearchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_rr_n4 = CVFConfigForGCNGridSearchDataset(\n",
    "    device,\n",
    "    \"graph_random_regular_graph_n4_d3_config_rank_dataset.csv\",\n",
    "    \"graph_random_regular_graph_n4_d3_edge_index.json\",\n",
    ")\n",
    "\n",
    "dataset_rr_n5 = CVFConfigForGCNGridSearchDataset(\n",
    "    device,\n",
    "    \"graph_random_regular_graph_n5_d4_config_rank_dataset.csv\",\n",
    "    \"graph_random_regular_graph_n5_d4_edge_index.json\",\n",
    ")\n",
    "\n",
    "dataset_rr_n6 = CVFConfigForGCNGridSearchDataset(\n",
    "    device,\n",
    "    \"graph_random_regular_graph_n6_d3_config_rank_dataset.csv\",\n",
    "    \"graph_random_regular_graph_n6_d3_edge_index.json\",\n",
    ")\n",
    "\n",
    "dataset_rr_n7 = CVFConfigForGCNGridSearchDataset(\n",
    "    device,\n",
    "    \"graph_random_regular_graph_n7_d4_config_rank_dataset.csv\",\n",
    "    \"graph_random_regular_graph_n7_d4_edge_index.json\",\n",
    ")\n",
    "\n",
    "dataset_rr_n8 = CVFConfigForGCNGridSearchDataset(\n",
    "    device,\n",
    "    \"graph_random_regular_graph_n8_d4_config_rank_dataset.csv\",\n",
    "    \"graph_random_regular_graph_n8_d4_edge_index.json\",\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dataset_coll = [\n",
    "    dataset_rr_n4,\n",
    "    dataset_rr_n5,\n",
    "    dataset_rr_n6,\n",
    "    dataset_rr_n7,\n",
    "    dataset_rr_n8,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(0.9 * len(datasets))\n",
    "# test_size = len(datasets) - train_size\n",
    "# train_dataset, test_dataset = random_split(datasets, [train_size, test_size])\n",
    "\n",
    "train_sizes = [int(0.9 * len(ds)) for ds in dataset_coll]\n",
    "test_sizes = [len(ds) - trs for ds, trs in zip(dataset_coll, train_sizes)]\n",
    "\n",
    "train_test_datasets = [\n",
    "    random_split(ds, [tr_s, ts])\n",
    "    for ds, tr_s, ts in zip(dataset_coll, train_sizes, test_sizes)\n",
    "]\n",
    "\n",
    "train_datasets = [ds[0] for ds in train_test_datasets]\n",
    "test_datasets = [ds[1] for ds in train_test_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428602\n"
     ]
    }
   ],
   "source": [
    "datasets = ConcatDataset(train_datasets)\n",
    "print(len(datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBatchSampler(Sampler):\n",
    "    def __init__(self, datasets: ConcatDataset, batch_size: int):\n",
    "        self.datasets = datasets\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    @property\n",
    "    def module(self):\n",
    "        return self._module\n",
    "\n",
    "    @module.setter\n",
    "    def module(self, val):\n",
    "        self._module = val\n",
    "\n",
    "    # def __len__(self):\n",
    "    #     return len(self.datasets)\n",
    "\n",
    "    def __iter__(self):\n",
    "        last_accessed = [0] + self.datasets.cumulative_sizes[:]\n",
    "        end_loop = [False for _ in range(len(self.datasets.datasets))]\n",
    "\n",
    "        while not all(end_loop):\n",
    "            for turn in range(len(self.datasets.datasets)):\n",
    "                if end_loop[turn]:\n",
    "                    continue\n",
    "\n",
    "                batch_size = self.batch_size\n",
    "                if (\n",
    "                    last_accessed[turn] + batch_size\n",
    "                    >= self.datasets.cumulative_sizes[turn]\n",
    "                ):\n",
    "                    batch_size = (\n",
    "                        self.datasets.cumulative_sizes[turn] - last_accessed[turn]\n",
    "                    )\n",
    "                    end_loop[turn] = True\n",
    "\n",
    "                # currently explicitly setting edge index before yielding\n",
    "                # TODO: find a better way to do it\n",
    "                self.module.edge_index = self.datasets.datasets[turn].dataset.edge_index\n",
    "\n",
    "                yield list(range(last_accessed[turn], last_accessed[turn] + batch_size))\n",
    "\n",
    "                last_accessed[turn] += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler = CustomBatchSampler(datasets, batch_size=batch_size)\n",
    "dataloader = DataLoader(datasets, batch_sampler=batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[230, 3042, 6728, 77040, 428602]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sampler.datasets.cumulative_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(dataloaders):\n",
    "#     print(i, len(batch[0]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaGNN(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNConv(dim_in, dim_h)\n",
    "        self.gcn2 = GCNConv(dim_h, dim_h)\n",
    "        self.out = torch.nn.Linear(dim_h, dim_out)\n",
    "\n",
    "    @property\n",
    "    def edge_index(self):\n",
    "        return self._edge_index\n",
    "\n",
    "    @edge_index.setter\n",
    "    def edge_index(self, val):\n",
    "        self._edge_index = val\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.gcn1(x, self.edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = self.gcn2(h, self.edge_index)\n",
    "        h = torch.relu(h)\n",
    "        h = self.out(h)\n",
    "        h = torch.relu(h)\n",
    "        h = global_mean_pool(h, torch.zeros(h.size(1)).to(device).long())\n",
    "        return h\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        dataloader.batch_sampler.module = self\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=0.001)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for batch in dataloader:\n",
    "                x = batch[0]\n",
    "                y = batch[1]\n",
    "                optimizer.zero_grad()\n",
    "                out = self(x)\n",
    "                loss = criterion(out, y)\n",
    "                total_loss += loss\n",
    "                count += 1\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(\n",
    "                \"Training set | Epoch\",\n",
    "                epoch,\n",
    "                \"| Loss:\",\n",
    "                round((total_loss / count).item(), 4),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn = VanillaGNN(1, 64, 1).to(device)\n",
    "# print(gnn)\n",
    "\n",
    "# gnn.fit(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "# import csv\n",
    "\n",
    "# torch.no_grad()\n",
    "# torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "# f = open(\"test_result.csv\", \"w\", newline=\"\")\n",
    "# csv_writer = csv.writer(f)\n",
    "# csv_writer.writerow([\"Actual\", \"Predicted\"])\n",
    "\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# total_loss = 0\n",
    "# total_matched = 0\n",
    "\n",
    "test_concat_datasets = ConcatDataset(test_datasets)\n",
    "batch_sampler = CustomBatchSampler(test_concat_datasets, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_concat_datasets, batch_sampler=batch_sampler)\n",
    "\n",
    "# test_dataloader.batch_sampler.module = gnn\n",
    "\n",
    "# count = 0\n",
    "# for batch in test_dataloader:\n",
    "# \tx = batch[0]\n",
    "# \ty = batch[1]\n",
    "# \tout = gnn(x)\n",
    "# \t# print(y.shape, out.shape)\n",
    "# \tcsv_writer.writerows(zip(y.detach().cpu().numpy(), out.detach().cpu().numpy()))\n",
    "# \tloss = criterion(out, y)\n",
    "# \ttotal_loss += loss\n",
    "# \tout = torch.round(out)\n",
    "# \tmatched = (out == y).sum().item()\n",
    "# \ttotal_matched += matched\n",
    "# \tcount += 1\n",
    "\n",
    "# print(\n",
    "# \t\"Test loss:\",\n",
    "# \ttotal_loss.detach() / count,\n",
    "# \t\"Total matched\",\n",
    "# \ttotal_matched,\n",
    "# \t\"out of\",\n",
    "# \tlen(test_concat_datasets),\n",
    "# \tf\"({round(total_matched/len(test_concat_datasets) * 100, 2)}%)\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loss(model):\n",
    "    torch.no_grad()\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    # test_dataloader.batch_sampler.module = gnn\n",
    "\n",
    "    count = 0\n",
    "    total_loss = 0\n",
    "    for batch in test_dataloader:\n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "        out = torch.FloatTensor(model.predict(x)).to(device)\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss\n",
    "        # out = torch.round(out)\n",
    "        # matched = (out == y).sum().item()\n",
    "        # total_matched += matched\n",
    "        count += 1\n",
    "\n",
    "    loss = total_loss / count\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNeuralNet(NeuralNet):\n",
    "    def fit_loop(self, X, y=None, epochs=None, **fit_params):\n",
    "        # super().fit_loop()\n",
    "        \"\"\"The proper fit loop.\n",
    "\n",
    "        Contains the logic of what actually happens during the fit\n",
    "        loop.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : input data, compatible with skorch.dataset.Dataset\n",
    "          By default, you should be able to pass:\n",
    "\n",
    "            * numpy arrays\n",
    "            * torch tensors\n",
    "            * pandas DataFrame or Series\n",
    "            * scipy sparse CSR matrices\n",
    "            * a dictionary of the former three\n",
    "            * a list/tuple of the former three\n",
    "            * a Dataset\n",
    "\n",
    "          If this doesn't work with your data, you have to pass a\n",
    "          ``Dataset`` that can deal with the data.\n",
    "\n",
    "        y : target data, compatible with skorch.dataset.Dataset\n",
    "          The same data types as for ``X`` are supported. If your X is\n",
    "          a Dataset that contains the target, ``y`` may be set to\n",
    "          None.\n",
    "\n",
    "        epochs : int or None (default=None)\n",
    "          If int, train for this number of epochs; if None, use\n",
    "          ``self.max_epochs``.\n",
    "\n",
    "        **fit_params : dict\n",
    "          Additional parameters passed to the ``forward`` method of\n",
    "          the module and to the ``self.train_split`` call.\n",
    "\n",
    "        \"\"\"\n",
    "        self.check_data(X, y)\n",
    "        self.check_training_readiness()\n",
    "        epochs = epochs if epochs is not None else self.max_epochs\n",
    "\n",
    "        on_epoch_kwargs = {\n",
    "            \"dataset_train\": datasets,\n",
    "            \"dataset_valid\": test_concat_datasets,\n",
    "        }\n",
    "        dataloader.batch_sampler.module = self.module_\n",
    "        iterator_train = dataloader\n",
    "        test_dataloader.batch_sampler.module = self.module_\n",
    "        iterator_valid = test_dataloader\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            self.notify(\"on_epoch_begin\", **on_epoch_kwargs)\n",
    "\n",
    "            self.run_single_epoch(\n",
    "                iterator_train,\n",
    "                training=True,\n",
    "                prefix=\"train\",\n",
    "                step_fn=self.train_step,\n",
    "                **fit_params\n",
    "            )\n",
    "\n",
    "            self.run_single_epoch(\n",
    "                iterator_valid,\n",
    "                training=False,\n",
    "                prefix=\"valid\",\n",
    "                step_fn=self.validation_step,\n",
    "                **fit_params\n",
    "            )\n",
    "\n",
    "            self.notify(\"on_epoch_end\", **on_epoch_kwargs)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset_rr_n4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = CustomNeuralNet(\n",
    "#     VanillaGNN,\n",
    "#     train_split=None,\n",
    "#     device=device,\n",
    "#     lr=0.01,\n",
    "#     batch_size=32,\n",
    "#     max_epochs=10,\n",
    "#     criterion=torch.nn.MSELoss,\n",
    "#     optimizer=torch.optim.Adam,\n",
    "#     optimizer__weight_decay=0.01,\n",
    "#     module__dim_in=1,\n",
    "#     module__dim_h=32,\n",
    "#     module__dim_out=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.fit(datasets, y=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.history.to_file('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# params = {\n",
    "#     \"lr\": [0.01],\n",
    "#     \"max_epochs\": [5, 10],\n",
    "#     \"module__dim_in\": [1],\n",
    "#     \"module__dim_h\": [32],\n",
    "#     \"module__dim_out\": [1],\n",
    "# }\n",
    "\n",
    "# gs = GridSearchCV(net, params, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "# gs.fit(datasets, y=None)\n",
    "\n",
    "# gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_test_loss(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"lr\": [0.01],\n",
    "    \"batch_size\": [32],\n",
    "    \"max_epochs\": [10],\n",
    "    \"optimizer\": [torch.optim.SGD, torch.optim.Adam],\n",
    "    \"module__dim_h\": [16, 32, 64],\n",
    "    \"optimizer__weight_decay\": [0.01],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lr': 0.01,\n",
       "  'batch_size': 32,\n",
       "  'max_epochs': 10,\n",
       "  'optimizer': torch.optim.sgd.SGD,\n",
       "  'module__dim_h': 16,\n",
       "  'optimizer__weight_decay': 0.01},\n",
       " {'lr': 0.01,\n",
       "  'batch_size': 32,\n",
       "  'max_epochs': 10,\n",
       "  'optimizer': torch.optim.sgd.SGD,\n",
       "  'module__dim_h': 32,\n",
       "  'optimizer__weight_decay': 0.01},\n",
       " {'lr': 0.01,\n",
       "  'batch_size': 32,\n",
       "  'max_epochs': 10,\n",
       "  'optimizer': torch.optim.sgd.SGD,\n",
       "  'module__dim_h': 64,\n",
       "  'optimizer__weight_decay': 0.01},\n",
       " {'lr': 0.01,\n",
       "  'batch_size': 32,\n",
       "  'max_epochs': 10,\n",
       "  'optimizer': torch.optim.adam.Adam,\n",
       "  'module__dim_h': 16,\n",
       "  'optimizer__weight_decay': 0.01},\n",
       " {'lr': 0.01,\n",
       "  'batch_size': 32,\n",
       "  'max_epochs': 10,\n",
       "  'optimizer': torch.optim.adam.Adam,\n",
       "  'module__dim_h': 32,\n",
       "  'optimizer__weight_decay': 0.01},\n",
       " {'lr': 0.01,\n",
       "  'batch_size': 32,\n",
       "  'max_epochs': 10,\n",
       "  'optimizer': torch.optim.adam.Adam,\n",
       "  'module__dim_h': 64,\n",
       "  'optimizer__weight_decay': 0.01}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "param_combinations = list(itertools.product(*params.values()))\n",
    "\n",
    "param_combinations_dict = [dict(zip(params.keys(), combination)) for combination in param_combinations]\n",
    "\n",
    "# param_combinations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m1.2959\u001b[0m        \u001b[32m1.2630\u001b[0m  29.8525\n",
      "      2        \u001b[36m1.2121\u001b[0m        \u001b[32m1.2458\u001b[0m  29.5186\n",
      "      3        \u001b[36m1.2078\u001b[0m        \u001b[32m1.2412\u001b[0m  29.4248\n",
      "      4        \u001b[36m1.2067\u001b[0m        \u001b[32m1.2397\u001b[0m  29.4076\n",
      "      5        \u001b[36m1.2061\u001b[0m        \u001b[32m1.2387\u001b[0m  29.3968\n",
      "      6        \u001b[36m1.2057\u001b[0m        \u001b[32m1.2379\u001b[0m  29.4178\n",
      "      7        \u001b[36m1.2054\u001b[0m        \u001b[32m1.2364\u001b[0m  29.4383\n",
      "      8        \u001b[36m1.2051\u001b[0m        \u001b[32m1.2360\u001b[0m  29.4640\n",
      "      9        \u001b[36m1.2049\u001b[0m        \u001b[32m1.2355\u001b[0m  29.4576\n",
      "     10        \u001b[36m1.2048\u001b[0m        \u001b[32m1.2354\u001b[0m  29.4676\n",
      "{'lr': 0.01, 'batch_size': 32, 'max_epochs': 10, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'module__dim_h': 16, 'optimizer__weight_decay': 0.01} tensor(1.2394, device='cuda:0')\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m9.1376\u001b[0m        \u001b[32m9.1603\u001b[0m  29.4322\n",
      "      2        9.1376        9.1603  29.4452\n",
      "      3        9.1376        9.1603  29.4370\n",
      "      4        9.1376        9.1603  29.4166\n",
      "      5        9.1376        9.1603  29.4091\n",
      "      6        9.1376        9.1603  29.4072\n",
      "      7        9.1376        9.1603  29.3592\n",
      "      8        9.1376        9.1603  29.4090\n",
      "      9        9.1376        9.1603  29.1980\n",
      "     10        9.1376        9.1603  29.1803\n",
      "{'lr': 0.01, 'batch_size': 32, 'max_epochs': 10, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'module__dim_h': 32, 'optimizer__weight_decay': 0.01} tensor(9.1428, device='cuda:0')\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m1.2734\u001b[0m        \u001b[32m1.2528\u001b[0m  29.7100\n",
      "      2        \u001b[36m1.2065\u001b[0m        \u001b[32m1.2407\u001b[0m  29.7136\n",
      "      3        \u001b[36m1.2028\u001b[0m        \u001b[32m1.2393\u001b[0m  29.2104\n",
      "      4        \u001b[36m1.2020\u001b[0m        1.2483  29.4384\n",
      "      5        \u001b[36m1.2017\u001b[0m        1.2401  28.9737\n",
      "      6        \u001b[36m1.2015\u001b[0m        1.2487  29.6558\n",
      "      7        1.2015        1.2491  29.5176\n",
      "      8        \u001b[36m1.2014\u001b[0m        1.2493  29.9205\n",
      "      9        \u001b[36m1.2013\u001b[0m        1.2433  29.7732\n",
      "     10        1.2013        1.2506  29.5045\n",
      "{'lr': 0.01, 'batch_size': 32, 'max_epochs': 10, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'module__dim_h': 64, 'optimizer__weight_decay': 0.01} tensor(1.2548, device='cuda:0')\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m1.2267\u001b[0m        \u001b[32m1.2247\u001b[0m  29.5170\n",
      "      2        \u001b[36m1.2037\u001b[0m        \u001b[32m1.2220\u001b[0m  30.2495\n",
      "      3        \u001b[36m1.2025\u001b[0m        \u001b[32m1.2178\u001b[0m  29.9413\n",
      "      4        \u001b[36m1.2014\u001b[0m        1.2307  29.7231\n",
      "      5        \u001b[36m1.2012\u001b[0m        1.2200  30.2214\n",
      "      6        \u001b[36m1.2009\u001b[0m        1.2207  29.9909\n",
      "      7        \u001b[36m1.2006\u001b[0m        1.2200  29.9132\n",
      "      8        1.2006        1.2200  29.4495\n",
      "      9        1.2006        1.2200  30.3278\n",
      "     10        \u001b[36m1.2006\u001b[0m        1.2196  36.4966\n",
      "{'lr': 0.01, 'batch_size': 32, 'max_epochs': 10, 'optimizer': <class 'torch.optim.adam.Adam'>, 'module__dim_h': 16, 'optimizer__weight_decay': 0.01} tensor(1.2231, device='cuda:0')\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m1.2320\u001b[0m        \u001b[32m1.2162\u001b[0m  39.3703\n",
      "      2        \u001b[36m1.1995\u001b[0m        \u001b[32m1.2025\u001b[0m  39.5541\n"
     ]
    }
   ],
   "source": [
    "avg_for = 1\n",
    "for params in param_combinations_dict:\n",
    "    avg_loss = 0.0\n",
    "    for i in range(avg_for):\n",
    "        net = CustomNeuralNet(\n",
    "            VanillaGNN,\n",
    "            train_split=None,\n",
    "            device=device,\n",
    "            criterion=torch.nn.MSELoss,\n",
    "            # optimizer=torch.optim.Adam,\n",
    "            # optimizer__weight_decay=0.01,\n",
    "            module__dim_in=1,\n",
    "            # module__dim_h=32,\n",
    "            module__dim_out=1,\n",
    "            **params\n",
    "        )\n",
    "        net.fit(datasets, y=None)\n",
    "        avg_loss += get_test_loss(net)\n",
    "    print(params, avg_loss / avg_for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
