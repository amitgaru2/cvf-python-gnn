{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x before log softmax tensor([[-0.5952,  0.7045],\n",
      "        [-0.7346,  0.8649],\n",
      "        [-0.6142,  0.6207],\n",
      "        [-0.4748,  0.4603]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-1.5408, -0.2411],\n",
      "        [-1.7834, -0.1840],\n",
      "        [-1.4902, -0.2553],\n",
      "        [-1.2663, -0.3311]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 1, Loss: 0.8865\n",
      "x before log softmax tensor([[-0.5281,  0.5923],\n",
      "        [-0.6581,  0.7390],\n",
      "        [-0.5389,  0.4968],\n",
      "        [-0.4089,  0.3500]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-1.4027, -0.2823],\n",
      "        [-1.6181, -0.2210],\n",
      "        [-1.3395, -0.3038],\n",
      "        [-1.1430, -0.3840]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 2, Loss: 0.8368\n",
      "x before log softmax tensor([[-0.4642,  0.4868],\n",
      "        [-0.5849,  0.6203],\n",
      "        [-0.4659,  0.3773],\n",
      "        [-0.3452,  0.2438]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-1.2777, -0.3267],\n",
      "        [-1.4672, -0.2621],\n",
      "        [-1.2011, -0.3579],\n",
      "        [-1.0304, -0.4414]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 3, Loss: 0.7956\n",
      "x before log softmax tensor([[-0.3991,  0.3885],\n",
      "        [-0.5104,  0.5096],\n",
      "        [-0.3925,  0.2704],\n",
      "        [-0.2811,  0.1494]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-1.1626, -0.3749],\n",
      "        [-1.3279, -0.3079],\n",
      "        [-1.0785, -0.4157],\n",
      "        [-0.9314, -0.5009]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 4, Loss: 0.7625\n",
      "x before log softmax tensor([[-0.3342,  0.3031],\n",
      "        [-0.4339,  0.4081],\n",
      "        [-0.3195,  0.1729],\n",
      "        [-0.2197,  0.0678]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-1.0617, -0.4245],\n",
      "        [-1.2003, -0.3583],\n",
      "        [-0.9693, -0.4770],\n",
      "        [-0.8472, -0.5597]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 5, Loss: 0.7372\n",
      "x before log softmax tensor([[-0.2694,  0.2263],\n",
      "        [-0.3572,  0.3150],\n",
      "        [-0.2511,  0.0861],\n",
      "        [-0.1633, -0.0025]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.9715, -0.4757],\n",
      "        [-1.0847, -0.4125],\n",
      "        [-0.8759, -0.5387],\n",
      "        [-0.7767, -0.6160]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 6, Loss: 0.7190\n",
      "x before log softmax tensor([[-0.2059,  0.1576],\n",
      "        [-0.2823,  0.2310],\n",
      "        [-0.1872,  0.0081],\n",
      "        [-0.1108, -0.0654]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.8913, -0.5278],\n",
      "        [-0.9824, -0.4691],\n",
      "        [-0.7955, -0.6003],\n",
      "        [-0.7161, -0.6707]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 7, Loss: 0.7067\n",
      "x before log softmax tensor([[-0.1488,  0.1030],\n",
      "        [-0.2144,  0.1626],\n",
      "        [-0.1267, -0.0638],\n",
      "        [-0.0610, -0.1234]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.8270, -0.5752],\n",
      "        [-0.8993, -0.5223],\n",
      "        [-0.7251, -0.6622],\n",
      "        [-0.6624, -0.7248]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 8, Loss: 0.6998\n",
      "x before log softmax tensor([[-0.0947,  0.0537],\n",
      "        [-0.1505,  0.1008],\n",
      "        [-0.0713, -0.1273],\n",
      "        [-0.0155, -0.1744]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7701, -0.6217],\n",
      "        [-0.8267, -0.5754],\n",
      "        [-0.6656, -0.7215],\n",
      "        [-0.6169, -0.7757]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 9, Loss: 0.6967\n",
      "x before log softmax tensor([[-0.0450,  0.0095],\n",
      "        [-0.0919,  0.0455],\n",
      "        [-0.0224, -0.1809],\n",
      "        [ 0.0245, -0.2169]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7207, -0.6663],\n",
      "        [-0.7642, -0.6268],\n",
      "        [-0.6170, -0.7755],\n",
      "        [-0.5797, -0.8211]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 10, Loss: 0.6964\n",
      "x before log softmax tensor([[-0.0011, -0.0293],\n",
      "        [-0.0402, -0.0030],\n",
      "        [ 0.0213, -0.2269],\n",
      "        [ 0.0604, -0.2532]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.6792, -0.7073],\n",
      "        [-0.7119, -0.6747],\n",
      "        [-0.5767, -0.8249],\n",
      "        [-0.5486, -0.8622]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 11, Loss: 0.6982\n",
      "x before log softmax tensor([[ 0.0337, -0.0635],\n",
      "        [ 0.0011, -0.0457],\n",
      "        [ 0.0572, -0.2657],\n",
      "        [ 0.0898, -0.2835]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.6457, -0.7429],\n",
      "        [-0.6700, -0.7168],\n",
      "        [-0.5447, -0.8676],\n",
      "        [-0.5238, -0.8971]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 12, Loss: 0.7011\n",
      "x before log softmax tensor([[ 0.0584, -0.0928],\n",
      "        [ 0.0299, -0.0797],\n",
      "        [ 0.0806, -0.2963],\n",
      "        [ 0.1092, -0.3095]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.6204, -0.7716],\n",
      "        [-0.6399, -0.7494],\n",
      "        [-0.5223, -0.8993],\n",
      "        [-0.5056, -0.9242]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 13, Loss: 0.7041\n",
      "x before log softmax tensor([[ 0.0734, -0.1166],\n",
      "        [ 0.0474, -0.1067],\n",
      "        [ 0.0939, -0.3198],\n",
      "        [ 0.1199, -0.3296]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.6026, -0.7927],\n",
      "        [-0.6190, -0.7732],\n",
      "        [-0.5075, -0.9212],\n",
      "        [-0.4934, -0.9430]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 14, Loss: 0.7066\n",
      "x before log softmax tensor([[ 0.0805, -0.1340],\n",
      "        [ 0.0561, -0.1262],\n",
      "        [ 0.1017, -0.3340],\n",
      "        [ 0.1261, -0.3419]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.5916, -0.8061],\n",
      "        [-0.6062, -0.7884],\n",
      "        [-0.4988, -0.9346],\n",
      "        [-0.4863, -0.9543]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 15, Loss: 0.7083\n",
      "x before log softmax tensor([[ 0.0803, -0.1456],\n",
      "        [ 0.0565, -0.1390],\n",
      "        [ 0.1027, -0.3414],\n",
      "        [ 0.1265, -0.3480]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.5866, -0.8124],\n",
      "        [-0.6002, -0.7956],\n",
      "        [-0.4956, -0.9396],\n",
      "        [-0.4838, -0.9582]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 16, Loss: 0.7090\n",
      "x before log softmax tensor([[ 0.0737, -0.1519],\n",
      "        [ 0.0498, -0.1459],\n",
      "        [ 0.0977, -0.3427],\n",
      "        [ 0.1217, -0.3488]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.5867, -0.8123],\n",
      "        [-0.6001, -0.7957],\n",
      "        [-0.4970, -0.9374],\n",
      "        [-0.4853, -0.9558]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 17, Loss: 0.7088\n",
      "x before log softmax tensor([[ 0.0620, -0.1538],\n",
      "        [ 0.0370, -0.1477],\n",
      "        [ 0.0877, -0.3391],\n",
      "        [ 0.1126, -0.3451]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.5911, -0.8068],\n",
      "        [-0.6050, -0.7898],\n",
      "        [-0.5024, -0.9291],\n",
      "        [-0.4903, -0.9480]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 18, Loss: 0.7078\n",
      "x before log softmax tensor([[ 0.0459, -0.1520],\n",
      "        [ 0.0194, -0.1455],\n",
      "        [ 0.0736, -0.3313],\n",
      "        [ 0.1001, -0.3378]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.5991, -0.7970],\n",
      "        [-0.6141, -0.7790],\n",
      "        [-0.5111, -0.9159],\n",
      "        [-0.4980, -0.9359]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 19, Loss: 0.7062\n",
      "x before log softmax tensor([[ 0.0266, -0.1472],\n",
      "        [-0.0020, -0.1399],\n",
      "        [ 0.0564, -0.3202],\n",
      "        [ 0.0849, -0.3276]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.6100, -0.7838],\n",
      "        [-0.6266, -0.7645],\n",
      "        [-0.5225, -0.8991],\n",
      "        [-0.5080, -0.9205]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 20, Loss: 0.7044\n",
      "x before log softmax tensor([[ 0.0048, -0.1403],\n",
      "        [-0.0261, -0.1319],\n",
      "        [ 0.0369, -0.3068],\n",
      "        [ 0.0678, -0.3152]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.6232, -0.7683],\n",
      "        [-0.6417, -0.7474],\n",
      "        [-0.5360, -0.8797],\n",
      "        [-0.5198, -0.9029]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 21, Loss: 0.7024\n",
      "x before log softmax tensor([[-0.0185, -0.1318],\n",
      "        [-0.0521, -0.1221],\n",
      "        [ 0.0159, -0.2917],\n",
      "        [ 0.0494, -0.3015]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.6381, -0.7514],\n",
      "        [-0.6588, -0.7287],\n",
      "        [-0.5511, -0.8587],\n",
      "        [-0.5330, -0.8839]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 22, Loss: 0.7005\n",
      "x before log softmax tensor([[-0.0426, -0.1223],\n",
      "        [-0.0790, -0.1111],\n",
      "        [-0.0060, -0.2757],\n",
      "        [ 0.0304, -0.2868]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.6541, -0.7338],\n",
      "        [-0.6772, -0.7093],\n",
      "        [-0.5674, -0.8371],\n",
      "        [-0.5471, -0.8643]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 23, Loss: 0.6988\n",
      "x before log softmax tensor([[-0.0669, -0.1122],\n",
      "        [-0.1061, -0.0996],\n",
      "        [-0.0280, -0.2592],\n",
      "        [ 0.0112, -0.2718]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.6707, -0.7161],\n",
      "        [-0.6964, -0.6899],\n",
      "        [-0.5842, -0.8154],\n",
      "        [-0.5616, -0.8447]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 24, Loss: 0.6974\n",
      "x before log softmax tensor([[-0.0864, -0.0992],\n",
      "        [-0.1327, -0.0880],\n",
      "        [-0.0496, -0.2428],\n",
      "        [-0.0033, -0.2540]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.6868, -0.6996],\n",
      "        [-0.7157, -0.6710],\n",
      "        [-0.6012, -0.7944],\n",
      "        [-0.5756, -0.8263]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 25, Loss: 0.6963\n",
      "x before log softmax tensor([[-0.1038, -0.0856],\n",
      "        [-0.1579, -0.0764],\n",
      "        [-0.0702, -0.2268],\n",
      "        [-0.0161, -0.2359]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7023, -0.6841],\n",
      "        [-0.7347, -0.6533],\n",
      "        [-0.6179, -0.7745],\n",
      "        [-0.5893, -0.8091]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 26, Loss: 0.6956\n",
      "x before log softmax tensor([[-0.1199, -0.0727],\n",
      "        [-0.1812, -0.0653],\n",
      "        [-0.0894, -0.2115],\n",
      "        [-0.0282, -0.2188]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7171, -0.6698],\n",
      "        [-0.7527, -0.6369],\n",
      "        [-0.6340, -0.7561],\n",
      "        [-0.6024, -0.7930]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 27, Loss: 0.6952\n",
      "x before log softmax tensor([[-0.1347, -0.0606],\n",
      "        [-0.2022, -0.0549],\n",
      "        [-0.1069, -0.1972],\n",
      "        [-0.0393, -0.2030]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7308, -0.6568],\n",
      "        [-0.7695, -0.6222],\n",
      "        [-0.6490, -0.7393],\n",
      "        [-0.6147, -0.7783]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 28, Loss: 0.6951\n",
      "x before log softmax tensor([[-0.1477, -0.0498],\n",
      "        [-0.2208, -0.0454],\n",
      "        [-0.1225, -0.1842],\n",
      "        [-0.0493, -0.1886]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7433, -0.6454],\n",
      "        [-0.7847, -0.6093],\n",
      "        [-0.6628, -0.7245],\n",
      "        [-0.6260, -0.7652]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 29, Loss: 0.6951\n",
      "x before log softmax tensor([[-0.1588, -0.0402],\n",
      "        [-0.2367, -0.0370],\n",
      "        [-0.1360, -0.1726],\n",
      "        [-0.0582, -0.1758]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7542, -0.6356],\n",
      "        [-0.7980, -0.5983],\n",
      "        [-0.6750, -0.7116],\n",
      "        [-0.6361, -0.7537]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 30, Loss: 0.6953\n",
      "x before log softmax tensor([[-0.1680, -0.0320],\n",
      "        [-0.2498, -0.0298],\n",
      "        [-0.1474, -0.1625],\n",
      "        [-0.0657, -0.1648]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7634, -0.6275],\n",
      "        [-0.8092, -0.5892],\n",
      "        [-0.6856, -0.7007],\n",
      "        [-0.6448, -0.7439]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 31, Loss: 0.6955\n",
      "x before log softmax tensor([[-0.1752, -0.0254],\n",
      "        [-0.2599, -0.0239],\n",
      "        [-0.1566, -0.1540],\n",
      "        [-0.0719, -0.1555]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7709, -0.6210],\n",
      "        [-0.8181, -0.5821],\n",
      "        [-0.6944, -0.6919],\n",
      "        [-0.6522, -0.7359]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 32, Loss: 0.6958\n",
      "x before log softmax tensor([[-0.1804, -0.0202],\n",
      "        [-0.2672, -0.0192],\n",
      "        [-0.1636, -0.1471],\n",
      "        [-0.0767, -0.1481]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7764, -0.6163],\n",
      "        [-0.8248, -0.5768],\n",
      "        [-0.7014, -0.6850],\n",
      "        [-0.6581, -0.7295]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 33, Loss: 0.6960\n",
      "x before log softmax tensor([[-0.1835, -0.0165],\n",
      "        [-0.2717, -0.0159],\n",
      "        [-0.1684, -0.1418],\n",
      "        [-0.0802, -0.1424]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7801, -0.6131],\n",
      "        [-0.8292, -0.5734],\n",
      "        [-0.7065, -0.6800],\n",
      "        [-0.6625, -0.7247]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 34, Loss: 0.6962\n",
      "x before log softmax tensor([[-0.1848, -0.0143],\n",
      "        [-0.2735, -0.0138],\n",
      "        [-0.1712, -0.1380],\n",
      "        [-0.0825, -0.1385]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7821, -0.6115],\n",
      "        [-0.8314, -0.5717],\n",
      "        [-0.7099, -0.6767],\n",
      "        [-0.6656, -0.7215]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 35, Loss: 0.6963\n",
      "x before log softmax tensor([[-0.1844, -0.0134],\n",
      "        [-0.2727, -0.0130],\n",
      "        [-0.1720, -0.1357],\n",
      "        [-0.0836, -0.1361]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7823, -0.6113],\n",
      "        [-0.8314, -0.5717],\n",
      "        [-0.7115, -0.6751],\n",
      "        [-0.6672, -0.7197]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 36, Loss: 0.6963\n",
      "x before log softmax tensor([[-0.1822, -0.0138],\n",
      "        [-0.2697, -0.0132],\n",
      "        [-0.1711, -0.1346],\n",
      "        [-0.0836, -0.1352]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7809, -0.6125],\n",
      "        [-0.8296, -0.5731],\n",
      "        [-0.7115, -0.6751],\n",
      "        [-0.6677, -0.7193]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 37, Loss: 0.6962\n",
      "x before log softmax tensor([[-0.1787, -0.0153],\n",
      "        [-0.2645, -0.0144],\n",
      "        [-0.1686, -0.1348],\n",
      "        [-0.0827, -0.1356]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7781, -0.6148],\n",
      "        [-0.8260, -0.5759],\n",
      "        [-0.7102, -0.6764],\n",
      "        [-0.6670, -0.7200]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 38, Loss: 0.6961\n",
      "x before log softmax tensor([[-0.1738, -0.0178],\n",
      "        [-0.2576, -0.0165],\n",
      "        [-0.1646, -0.1359],\n",
      "        [-0.0809, -0.1372]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7742, -0.6182],\n",
      "        [-0.8209, -0.5799],\n",
      "        [-0.7076, -0.6789],\n",
      "        [-0.6654, -0.7217]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 39, Loss: 0.6958\n",
      "x before log softmax tensor([[-0.1678, -0.0212],\n",
      "        [-0.2490, -0.0194],\n",
      "        [-0.1595, -0.1380],\n",
      "        [-0.0784, -0.1398]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7692, -0.6225],\n",
      "        [-0.8146, -0.5849],\n",
      "        [-0.7040, -0.6824],\n",
      "        [-0.6629, -0.7243]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 40, Loss: 0.6956\n",
      "x before log softmax tensor([[-0.1610, -0.0252],\n",
      "        [-0.2392, -0.0228],\n",
      "        [-0.1535, -0.1408],\n",
      "        [-0.0753, -0.1432]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7633, -0.6276],\n",
      "        [-0.8072, -0.5908],\n",
      "        [-0.6995, -0.6868],\n",
      "        [-0.6598, -0.7277]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 41, Loss: 0.6953\n",
      "x before log softmax tensor([[-0.1534, -0.0297],\n",
      "        [-0.2283, -0.0266],\n",
      "        [-0.1466, -0.1441],\n",
      "        [-0.0718, -0.1472]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7569, -0.6332],\n",
      "        [-0.7991, -0.5974],\n",
      "        [-0.6944, -0.6919],\n",
      "        [-0.6561, -0.7316]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 42, Loss: 0.6951\n",
      "x before log softmax tensor([[-0.1454, -0.0346],\n",
      "        [-0.2167, -0.0308],\n",
      "        [-0.1393, -0.1478],\n",
      "        [-0.0679, -0.1516]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7501, -0.6393],\n",
      "        [-0.7904, -0.6045],\n",
      "        [-0.6889, -0.6974],\n",
      "        [-0.6522, -0.7359]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 43, Loss: 0.6948\n",
      "x before log softmax tensor([[-0.1371, -0.0396],\n",
      "        [-0.2047, -0.0350],\n",
      "        [-0.1316, -0.1517],\n",
      "        [-0.0640, -0.1563]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7431, -0.6456],\n",
      "        [-0.7816, -0.6119],\n",
      "        [-0.6831, -0.7033],\n",
      "        [-0.6481, -0.7404]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 44, Loss: 0.6946\n",
      "x before log softmax tensor([[-0.1287, -0.0446],\n",
      "        [-0.1924, -0.0392],\n",
      "        [-0.1237, -0.1556],\n",
      "        [-0.0600, -0.1610]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7361, -0.6520],\n",
      "        [-0.7727, -0.6195],\n",
      "        [-0.6774, -0.7092],\n",
      "        [-0.6439, -0.7449]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 45, Loss: 0.6945\n",
      "x before log softmax tensor([[-0.1204, -0.0495],\n",
      "        [-0.1802, -0.0433],\n",
      "        [-0.1160, -0.1594],\n",
      "        [-0.0561, -0.1656]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7292, -0.6584],\n",
      "        [-0.7640, -0.6270],\n",
      "        [-0.6717, -0.7151],\n",
      "        [-0.6399, -0.7494]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 46, Loss: 0.6943\n",
      "x before log softmax tensor([[-0.1123, -0.0542],\n",
      "        [-0.1683, -0.0471],\n",
      "        [-0.1085, -0.1629],\n",
      "        [-0.0525, -0.1700]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7226, -0.6645],\n",
      "        [-0.7556, -0.6343],\n",
      "        [-0.6663, -0.7207],\n",
      "        [-0.6361, -0.7536]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 47, Loss: 0.6942\n",
      "x before log softmax tensor([[-0.1046, -0.0584],\n",
      "        [-0.1569, -0.0504],\n",
      "        [-0.1014, -0.1660],\n",
      "        [-0.0491, -0.1739]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7165, -0.6703],\n",
      "        [-0.7478, -0.6413],\n",
      "        [-0.6614, -0.7260],\n",
      "        [-0.6327, -0.7575]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 48, Loss: 0.6942\n",
      "x before log softmax tensor([[-0.0975, -0.0621],\n",
      "        [-0.1462, -0.0533],\n",
      "        [-0.0948, -0.1685],\n",
      "        [-0.0461, -0.1774]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7110, -0.6756],\n",
      "        [-0.7407, -0.6478],\n",
      "        [-0.6569, -0.7307],\n",
      "        [-0.6297, -0.7609]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 49, Loss: 0.6942\n",
      "x before log softmax tensor([[-0.0910, -0.0652],\n",
      "        [-0.1362, -0.0556],\n",
      "        [-0.0889, -0.1705],\n",
      "        [-0.0436, -0.1802]], grad_fn=<AddBackward0>)\n",
      "out tensor([[-0.7061, -0.6803],\n",
      "        [-0.7343, -0.6536],\n",
      "        [-0.6531, -0.7348],\n",
      "        [-0.6272, -0.7638]], grad_fn=<LogSoftmaxBackward0>) y tensor([0, 1, 0, 1])\n",
      "Epoch 50, Loss: 0.6942\n",
      "x before log softmax tensor([[-0.0852, -0.0677],\n",
      "        [-0.1273, -0.0572],\n",
      "        [-0.0837, -0.1718],\n",
      "        [-0.0416, -0.1823]], grad_fn=<AddBackward0>)\n",
      "Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Define the GNN model\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        print(\"x before log softmax\", x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Create a toy graph\n",
    "# Number of nodes: 4\n",
    "# Edges: (0 -> 1), (1 -> 2), (2 -> 3), (3 -> 0)\n",
    "edge_index = torch.tensor([\n",
    "    [0, 1, 2, 3],\n",
    "    [1, 2, 3, 0]\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Node features (4 nodes, each with 3 features)\n",
    "x = torch.tensor([\n",
    "    [1.0, 0.0, 1.0],\n",
    "    [0.0, 1.0, 1.0],\n",
    "    [1.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0]\n",
    "], dtype=torch.float)\n",
    "\n",
    "# Labels (binary classification)\n",
    "y = torch.tensor([0, 1, 0, 1], dtype=torch.long)\n",
    "\n",
    "# Define the dataset\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = GNN(input_dim=3, hidden_dim=16, output_dim=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    print(\"out\", out, \"y\", data.y)\n",
    "    loss = loss_fn(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "_, pred = model(data.x, data.edge_index).max(dim=1)\n",
    "accuracy = (pred == data.y).sum().item() / data.y.size(0)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
