{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8f4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertForMaskedLM, BertModel\n",
    "\n",
    "class TokenVectorBERT(nn.Module):\n",
    "    def __init__(self, input_dim=3, vocab_dim=64, bert_hidden=64, max_seq_len=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_proj = nn.Linear(input_dim, vocab_dim)  # turn [0, 0, 2] into an embedding\n",
    "        self.config = BertConfig(\n",
    "            vocab_size=1,  # dummy, unused\n",
    "            hidden_size=bert_hidden,\n",
    "            num_hidden_layers=2,\n",
    "            num_attention_heads=2,\n",
    "            intermediate_size=bert_hidden * 2,\n",
    "            max_position_embeddings=max_seq_len,\n",
    "            pad_token_id=0\n",
    "        )\n",
    "        self.bert = BertModel(self.config)\n",
    "        self.mlm_head = nn.Linear(bert_hidden, vocab_dim)\n",
    "\n",
    "    def forward(self, input_vecs, attention_mask=None):\n",
    "        # input_vecs: (batch_size, seq_len, input_dim) like (2, 4, 3)\n",
    "        x = self.token_proj(input_vecs)  # (batch_size, seq_len, vocab_dim)\n",
    "        outputs = self.bert(inputs_embeds=x, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        return self.mlm_head(sequence_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99a1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Dummy Data -----\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "token_dim = 3\n",
    "\n",
    "# Simulate tokens: each token is a 3D vector with values 0â€“5\n",
    "torch.manual_seed(42)\n",
    "inputs = torch.randint(0, 6, (batch_size, seq_len, token_dim)).float()\n",
    "\n",
    "# Attention mask: 1s for real tokens\n",
    "attention_mask = torch.ones(batch_size, seq_len).long()\n",
    "\n",
    "# Learnable MASK token (for masking positions)\n",
    "mask_vector = nn.Parameter(torch.zeros(token_dim))\n",
    "\n",
    "# ----- Masking Function -----\n",
    "def mask_input_tokens(inputs, mask_token, mask_prob=0.3):\n",
    "    inputs = inputs.clone()\n",
    "    labels = inputs.clone()\n",
    "    mask = torch.rand(inputs[:, :, 0].shape) < mask_prob  # shape: (B, T)\n",
    "\n",
    "    for i in range(inputs.size(0)):\n",
    "        for j in range(inputs.size(1)):\n",
    "            if mask[i, j]:\n",
    "                inputs[i, j] = mask_token\n",
    "\n",
    "    return inputs, labels, mask  # No NaNs here!\n",
    "\n",
    "def masked_mse_loss(pred, target, mask):\n",
    "    print(\"pred\", pred)\n",
    "    print()\n",
    "    print(\"target\", target)\n",
    "    loss = (pred - target) ** 2\n",
    "    loss = loss.mean(dim=-1)  # (B, T)\n",
    "    loss = loss * mask.float()\n",
    "    \n",
    "    valid_tokens = mask.sum()\n",
    "    return loss.sum() / (valid_tokens + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d42a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs tensor([[[0., 5., 4.],\n",
      "         [4., 0., 5.],\n",
      "         [4., 2., 4.]]])\n",
      "mask_vector Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n",
      "\n",
      "masked tensor([[[0., 0., 0.],\n",
      "         [4., 0., 5.],\n",
      "         [4., 2., 4.]]], grad_fn=<CopySlices>)\n",
      "target_labels tensor([[[0., 5., 4.],\n",
      "         [4., 0., 5.],\n",
      "         [4., 2., 4.]]])\n",
      "loss_mask tensor([[ True, False, False]])\n",
      "\n",
      "pred tensor([[[-0.4393, -0.4414,  0.1026,  1.0424, -0.8489, -1.0016,  1.1614,\n",
      "           0.1247, -0.5056, -0.2427, -0.1999, -0.0664,  0.2215, -0.0149,\n",
      "           0.2927,  0.4635, -0.3558,  0.0516,  0.6538, -0.5757, -0.0094,\n",
      "          -0.7010, -0.1859, -1.2894,  0.3111, -0.4648,  0.7936, -0.7858,\n",
      "           0.5232,  0.2594,  0.7620, -0.3715,  0.3781,  0.4227, -0.5386,\n",
      "          -0.0127, -0.0249,  0.0633, -0.2753,  0.0828,  0.1584, -0.3863,\n",
      "           0.4227, -0.3966,  0.4425, -0.3326, -0.8230, -0.4211, -1.0250,\n",
      "           0.1551, -1.0032,  0.4018, -0.0580,  0.3002,  0.0034,  0.5586,\n",
      "          -0.2463,  0.1408, -0.7887, -0.8310,  0.3055,  0.5960,  0.9230,\n",
      "          -0.1249],\n",
      "         [-0.1013,  0.0820, -0.5901,  0.9623, -1.1616, -0.1734,  1.2603,\n",
      "          -0.1470, -0.2266, -0.6973, -0.6588,  1.1650, -0.0751, -0.1841,\n",
      "          -0.0887,  1.6292, -0.0897, -0.5814,  0.5944,  0.1689,  0.3412,\n",
      "          -0.8591,  0.6111,  0.1080, -0.2435,  0.2992,  0.1361, -0.6344,\n",
      "           0.4851, -0.1643,  1.3864, -0.2244, -0.3258, -0.2688, -1.2975,\n",
      "          -0.4706, -1.4676,  0.0077,  0.3739, -0.2951,  0.0048, -0.2054,\n",
      "          -0.3421, -0.3880, -0.6574,  1.2904, -1.0892,  1.0499, -0.4827,\n",
      "           0.5437, -0.4309, -0.9851, -1.3706, -0.0106, -0.3560,  0.0477,\n",
      "           0.4882, -0.1780, -1.0128, -1.7896,  0.0540, -0.1283,  0.9761,\n",
      "          -1.3139],\n",
      "         [ 0.0503,  0.1354, -0.0149,  1.0678, -1.2659,  0.1906,  0.9609,\n",
      "           0.3805, -0.0706, -0.6595, -1.0797,  0.7482,  0.1833, -0.1057,\n",
      "          -0.0787,  1.6413, -0.0942, -0.1877,  0.6249, -0.1087,  0.2778,\n",
      "          -1.0069,  0.5184, -0.3270,  0.2551,  0.6641,  0.0219, -0.5181,\n",
      "           0.9600,  0.0974,  1.5551, -0.4179, -0.1685, -0.0787, -0.7674,\n",
      "          -0.8563, -1.1851,  0.3201, -0.0871, -0.2841, -0.2008,  0.2208,\n",
      "          -0.7358, -0.1959, -0.9821,  1.2674, -0.6884,  0.3230, -0.6676,\n",
      "           0.4360, -1.1118, -1.0754, -1.0543,  0.5856,  0.1875,  0.4888,\n",
      "           0.0833,  0.0131, -0.9858, -1.4443,  0.8385,  0.1355,  1.2137,\n",
      "          -0.9953]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "target tensor([[[ 2.7283,  1.9907,  2.7916, -0.3569,  0.0114, -2.3362, -2.5257,\n",
      "          -0.8575,  0.6847,  2.9530,  0.4159,  3.4020,  1.7324, -4.1367,\n",
      "           0.2392,  2.0096,  0.4688, -1.0194,  0.1835, -3.4620,  3.7875,\n",
      "          -4.9355, -1.1222,  0.6317,  0.9846,  1.0735, -0.5423,  0.2380,\n",
      "          -3.8961, -2.2617, -0.6457, -1.7641, -3.2350,  0.7728, -1.1182,\n",
      "          -0.6054, -3.0655, -1.7289, -3.8688, -0.1057,  2.9199,  1.2461,\n",
      "          -5.0244,  0.1633,  0.0416,  1.4179,  0.9901, -1.6663,  1.2909,\n",
      "           1.5798,  2.7079, -1.6764,  0.3813, -2.9427,  0.4721, -2.3908,\n",
      "          -0.3417,  2.2002,  2.9823, -0.7063, -1.6410,  2.6675, -0.3687,\n",
      "          -1.0743],\n",
      "         [-1.3670,  3.5844,  0.3255, -2.4369,  1.2961, -2.9913, -4.7569,\n",
      "           4.1261,  2.3632,  1.0436, -1.2446,  2.1303, -0.8403, -1.7759,\n",
      "           2.0349,  1.6617, -2.2408,  1.2744, -1.6166, -0.1799,  0.0060,\n",
      "          -0.9036, -0.7533, -1.2382, -2.3548,  3.4904, -0.7038,  1.8064,\n",
      "          -0.5410,  0.6223, -1.0829,  0.0401, -2.6813,  1.1669, -2.1101,\n",
      "          -1.0710, -4.1994, -0.6883, -3.7891,  1.8663,  0.1577, -2.8661,\n",
      "          -4.0874,  1.6794, -4.4206,  0.7143, -0.2331,  0.0933, -3.4909,\n",
      "           1.9871, -1.0377,  1.4070,  1.8532, -1.1229, -1.3976, -3.1513,\n",
      "          -2.2961,  1.3500,  0.9566,  0.5116, -1.0561,  1.6056, -0.9624,\n",
      "          -1.8525],\n",
      "         [-0.4714,  3.4624,  1.1303, -1.8765,  0.4441, -3.3606, -4.0777,\n",
      "           2.6995,  1.6315,  1.9135, -0.7778,  2.8275, -0.2770, -1.9082,\n",
      "           0.6755,  1.9574, -0.9269,  0.3082, -1.1425, -0.5200,  0.8522,\n",
      "          -1.2839, -1.7639,  0.0194, -1.5088,  2.8861, -1.4626,  1.1678,\n",
      "          -1.0701,  0.4374, -1.8884, -0.8116, -3.3430, -0.1455, -1.2158,\n",
      "          -1.5456, -4.3513, -1.0780, -4.2448,  0.4708,  0.9713, -1.9081,\n",
      "          -4.4719,  1.2024, -3.1395,  0.6574,  1.2766, -0.3401, -2.0154,\n",
      "           2.2053, -0.1150,  0.1471,  0.8283, -1.4695, -0.6053, -3.1154,\n",
      "          -0.8587,  1.8418,  1.2559,  0.4616, -1.0880,  2.6551, -0.9111,\n",
      "          -1.0045]]], grad_fn=<ViewBackward0>)\n",
      "Epoch 1/1 | Loss: 5.1958\n"
     ]
    }
   ],
   "source": [
    "model = TokenVectorBERT(input_dim=3, vocab_dim=64)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "print(\"inputs\", inputs)\n",
    "print(\"mask_vector\", mask_vector)\n",
    "print()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    masked_inputs, target_labels, loss_mask = mask_input_tokens(inputs, mask_vector)\n",
    "\n",
    "    print(\"masked\", masked_inputs)\n",
    "    print(\"target_labels\", target_labels)\n",
    "    print(\"loss_mask\", loss_mask)\n",
    "    print()\n",
    "\n",
    "    logits = model(masked_inputs, attention_mask)\n",
    "\n",
    "    # Compute loss only on masked positions\n",
    "    loss = masked_mse_loss(logits, model.token_proj(target_labels), loss_mask)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29310f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
