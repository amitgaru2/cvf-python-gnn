{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_dense_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaGNNLayer(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(dim_in, dim_out, bias=False)\n",
    "\n",
    "    def forward(self, x, adjacency):\n",
    "        x = self.linear(x)\n",
    "        x = torch.sparse.mm(adjacency, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([\n",
    "    [0, 1],\n",
    "    [0, 2],\n",
    "    [0, 3],\n",
    "    [1, 0],\n",
    "    [2, 0],\n",
    "    [3, 0]\n",
    "], dtype=torch.long)\n",
    "\n",
    "# x = torch.tensor([[0], [0], [0], [0]], dtype=torch.float) # configs: [0, 0, 0, 0], [1, 1, 1, 1]\n",
    "x = torch.tensor([[0, 1], [0, 1], [0, 1], [0, 1]], dtype=torch.float) # configs: [0, 0, 0, 0], [1, 1, 1, 1]\n",
    "\n",
    "y = torch.tensor([[3.0, 1.0]]) # ranks\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "data.train_mask = torch.tensor([1 for _ in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 0., 1., 0.],\n",
       "        [1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency = to_dense_adj(edge_index.t().contiguous())[0]\n",
    "adjacency += torch.eye(len(adjacency))\n",
    "adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaGNN(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.gnn1 = VanillaGNNLayer(dim_in, dim_h)\n",
    "        self.gnn2 = VanillaGNNLayer(dim_h, dim_h)\n",
    "        self.out = torch.nn.Linear(dim_h, dim_out)\n",
    "    \n",
    "    def forward(self, x, adjacency):\n",
    "        h = self.gnn1(x, adjacency)\n",
    "        h = torch.relu(h)\n",
    "        h = self.gnn2(h, adjacency)\n",
    "        h = torch.relu(h)\n",
    "        h = self.out(h)\n",
    "        return torch.sum(h, dim=0)\n",
    "    \n",
    "    def fit(self, data, epochs):\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "        self.train()\n",
    "        for epoch in range(epochs+1):\n",
    "            optimizer.zero_grad()\n",
    "            out = self(data.x, adjacency)\n",
    "            loss = criterion(out, data.y)\n",
    "            print(\"Loss:\", loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanillaGNN(\n",
      "  (gnn1): VanillaGNNLayer(\n",
      "    (linear): Linear(in_features=2, out_features=9, bias=False)\n",
      "  )\n",
      "  (gnn2): VanillaGNNLayer(\n",
      "    (linear): Linear(in_features=9, out_features=9, bias=False)\n",
      "  )\n",
      "  (out): Linear(in_features=9, out_features=2, bias=True)\n",
      ")\n",
      "Loss: tensor(14.1569, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(11.5662, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(9.4132, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.6275, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.1314, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.8576, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.7629, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.8233, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.0265, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.3677, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.8463, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.4639, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.2198, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.1074, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.1092, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.2035, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.3349, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.4773, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.5870, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.6405, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.6322, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.5708, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.4731, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.3587, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.2459, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.1481, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0736, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0269, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0339, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0584, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0762, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0624, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0420, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0229, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0102, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0146, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0176, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0149, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0096, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.0658e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(8.3532e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.4688e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.7494e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.8817e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.1491e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.6851e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.6594e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.3924e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.2068e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.1982e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.0646e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.1450e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.8787e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.2401e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.7269e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.7096e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.4583e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.4272e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.7281e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(9.4193e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.9867e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.8952e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1726e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.3103e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0299e-05, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.2769e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.6577e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.6681e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.5726e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.5812e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(8.1761e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.9349e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.5523e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.9797e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.9186e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.8792e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.4170e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.6581e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.5969e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2047e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.8600e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.9802e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.3235e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.6517e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.3926e-06, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(8.2014e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.2881e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.6331e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.5065e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(9.1761e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.5137e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.7010e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.9391e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(8.5884e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.2945e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.6408e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.8373e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.8301e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.3121e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.4395e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.6976e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.8138e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.4861e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.0789e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1316e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.2784e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.4462e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1274e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.3687e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0985e-07, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.8271e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.6583e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.2986e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.6760e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.4583e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.4088e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.3813e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.6581e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.7041e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.3902e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.7390e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.7200e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.3545e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.0326e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.8602e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(8.8394e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.4231e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2243e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.3355e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.0519e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.8651e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.3638e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.5480e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1772e-08, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.1028e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.0340e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.4236e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.7218e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.1431e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.7695e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2284e-10, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1444e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.1834e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.5944e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.5237e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.1260e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.4683e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0866e-10, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.3509e-10, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.6630e-10, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.9895e-10, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(7.5051e-10, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.2725e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.2645e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.1037e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.0966e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.2933e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1959e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.0306e-10, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1122e-10, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(5.1337e-11, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(2.3492e-11, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0016e-10, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(4.6305e-10, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.0929e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.7174e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.9566e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.6909e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(1.1455e-09, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(6.5089e-10, grad_fn=<MseLossBackward0>)\n",
      "Loss: tensor(3.1353e-10, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gnn = VanillaGNN(data.num_features, 16, data.num_features)\n",
    "print(gnn)\n",
    "gnn.fit(data, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.0000, 1.0000], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out = gnn(data.x, adjacency)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
