{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256\n",
    "D = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(output, target, target_mask):\n",
    "    distance = torch.zeros((N, N))\n",
    "    for r in range(N):\n",
    "        for c in range(N):\n",
    "            distance[r, c] = torch.cdist(output[r].unsqueeze(0), output[c].unsqueeze(0))\n",
    "\n",
    "    loss = torch.mean(torch.square(target - distance))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = torch.FloatTensor([[0, 0, 1, 1], [0, 0, 1, 2], [0, 0, 0, 1], [0, 0, 0, 0]])\n",
    "# target = [\n",
    "#     [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "#     [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "#     [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "#     [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "#     [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "#     [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
    "#     [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
    "#     [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "#     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "#     [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "#     [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "#     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "# ]\n",
    "\n",
    "target = pickle.load(open(\"matrix3.dump\", \"rb\"))\n",
    "target = torch.FloatTensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask = target == hd\n",
    "# target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target.masked_fill(target_mask, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(embedding.parameters(), lr=0.01, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = torch.nn.CosineEmbeddingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    loss = custom_loss(embedding.weight, target, target_mask)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Epoch:\", epoch, \"| Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: tensor(8.2941, grad_fn=<MeanBackward0>)\n",
      "Epoch: 1 | Loss: tensor(8.1671, grad_fn=<MeanBackward0>)\n",
      "Epoch: 2 | Loss: tensor(8.0419, grad_fn=<MeanBackward0>)\n",
      "Epoch: 3 | Loss: tensor(7.9182, grad_fn=<MeanBackward0>)\n",
      "Epoch: 4 | Loss: tensor(7.7962, grad_fn=<MeanBackward0>)\n",
      "Epoch: 5 | Loss: tensor(7.6758, grad_fn=<MeanBackward0>)\n",
      "Epoch: 6 | Loss: tensor(7.5571, grad_fn=<MeanBackward0>)\n",
      "Epoch: 7 | Loss: tensor(7.4400, grad_fn=<MeanBackward0>)\n",
      "Epoch: 8 | Loss: tensor(7.3245, grad_fn=<MeanBackward0>)\n",
      "Epoch: 9 | Loss: tensor(7.2107, grad_fn=<MeanBackward0>)\n",
      "Epoch: 10 | Loss: tensor(7.0985, grad_fn=<MeanBackward0>)\n",
      "Epoch: 11 | Loss: tensor(6.9880, grad_fn=<MeanBackward0>)\n",
      "Epoch: 12 | Loss: tensor(6.8791, grad_fn=<MeanBackward0>)\n",
      "Epoch: 13 | Loss: tensor(6.7718, grad_fn=<MeanBackward0>)\n",
      "Epoch: 14 | Loss: tensor(6.6660, grad_fn=<MeanBackward0>)\n",
      "Epoch: 15 | Loss: tensor(6.5619, grad_fn=<MeanBackward0>)\n",
      "Epoch: 16 | Loss: tensor(6.4594, grad_fn=<MeanBackward0>)\n",
      "Epoch: 17 | Loss: tensor(6.3584, grad_fn=<MeanBackward0>)\n",
      "Epoch: 18 | Loss: tensor(6.2590, grad_fn=<MeanBackward0>)\n",
      "Epoch: 19 | Loss: tensor(6.1612, grad_fn=<MeanBackward0>)\n",
      "Epoch: 20 | Loss: tensor(6.0648, grad_fn=<MeanBackward0>)\n",
      "Epoch: 21 | Loss: tensor(5.9700, grad_fn=<MeanBackward0>)\n",
      "Epoch: 22 | Loss: tensor(5.8766, grad_fn=<MeanBackward0>)\n",
      "Epoch: 23 | Loss: tensor(5.7847, grad_fn=<MeanBackward0>)\n",
      "Epoch: 24 | Loss: tensor(5.6943, grad_fn=<MeanBackward0>)\n",
      "Epoch: 25 | Loss: tensor(5.6053, grad_fn=<MeanBackward0>)\n",
      "Epoch: 26 | Loss: tensor(5.5177, grad_fn=<MeanBackward0>)\n",
      "Epoch: 27 | Loss: tensor(5.4315, grad_fn=<MeanBackward0>)\n",
      "Epoch: 28 | Loss: tensor(5.3466, grad_fn=<MeanBackward0>)\n",
      "Epoch: 29 | Loss: tensor(5.2632, grad_fn=<MeanBackward0>)\n",
      "Epoch: 30 | Loss: tensor(5.1810, grad_fn=<MeanBackward0>)\n",
      "Epoch: 31 | Loss: tensor(5.1002, grad_fn=<MeanBackward0>)\n",
      "Epoch: 32 | Loss: tensor(5.0207, grad_fn=<MeanBackward0>)\n",
      "Epoch: 33 | Loss: tensor(4.9425, grad_fn=<MeanBackward0>)\n",
      "Epoch: 34 | Loss: tensor(4.8655, grad_fn=<MeanBackward0>)\n",
      "Epoch: 35 | Loss: tensor(4.7898, grad_fn=<MeanBackward0>)\n",
      "Epoch: 36 | Loss: tensor(4.7153, grad_fn=<MeanBackward0>)\n",
      "Epoch: 37 | Loss: tensor(4.6420, grad_fn=<MeanBackward0>)\n",
      "Epoch: 38 | Loss: tensor(4.5699, grad_fn=<MeanBackward0>)\n",
      "Epoch: 39 | Loss: tensor(4.4990, grad_fn=<MeanBackward0>)\n",
      "Epoch: 40 | Loss: tensor(4.4292, grad_fn=<MeanBackward0>)\n",
      "Epoch: 41 | Loss: tensor(4.3606, grad_fn=<MeanBackward0>)\n",
      "Epoch: 42 | Loss: tensor(4.2930, grad_fn=<MeanBackward0>)\n",
      "Epoch: 43 | Loss: tensor(4.2266, grad_fn=<MeanBackward0>)\n",
      "Epoch: 44 | Loss: tensor(4.1612, grad_fn=<MeanBackward0>)\n",
      "Epoch: 45 | Loss: tensor(4.0969, grad_fn=<MeanBackward0>)\n",
      "Epoch: 46 | Loss: tensor(4.0336, grad_fn=<MeanBackward0>)\n",
      "Epoch: 47 | Loss: tensor(3.9714, grad_fn=<MeanBackward0>)\n",
      "Epoch: 48 | Loss: tensor(3.9101, grad_fn=<MeanBackward0>)\n",
      "Epoch: 49 | Loss: tensor(3.8499, grad_fn=<MeanBackward0>)\n",
      "Epoch: 50 | Loss: tensor(3.7906, grad_fn=<MeanBackward0>)\n",
      "Epoch: 51 | Loss: tensor(3.7322, grad_fn=<MeanBackward0>)\n",
      "Epoch: 52 | Loss: tensor(3.6748, grad_fn=<MeanBackward0>)\n",
      "Epoch: 53 | Loss: tensor(3.6183, grad_fn=<MeanBackward0>)\n",
      "Epoch: 54 | Loss: tensor(3.5628, grad_fn=<MeanBackward0>)\n",
      "Epoch: 55 | Loss: tensor(3.5081, grad_fn=<MeanBackward0>)\n",
      "Epoch: 56 | Loss: tensor(3.4542, grad_fn=<MeanBackward0>)\n",
      "Epoch: 57 | Loss: tensor(3.4013, grad_fn=<MeanBackward0>)\n",
      "Epoch: 58 | Loss: tensor(3.3492, grad_fn=<MeanBackward0>)\n",
      "Epoch: 59 | Loss: tensor(3.2979, grad_fn=<MeanBackward0>)\n",
      "Epoch: 60 | Loss: tensor(3.2474, grad_fn=<MeanBackward0>)\n",
      "Epoch: 61 | Loss: tensor(3.1977, grad_fn=<MeanBackward0>)\n",
      "Epoch: 62 | Loss: tensor(3.1488, grad_fn=<MeanBackward0>)\n",
      "Epoch: 63 | Loss: tensor(3.1007, grad_fn=<MeanBackward0>)\n",
      "Epoch: 64 | Loss: tensor(3.0534, grad_fn=<MeanBackward0>)\n",
      "Epoch: 65 | Loss: tensor(3.0068, grad_fn=<MeanBackward0>)\n",
      "Epoch: 66 | Loss: tensor(2.9609, grad_fn=<MeanBackward0>)\n",
      "Epoch: 67 | Loss: tensor(2.9157, grad_fn=<MeanBackward0>)\n",
      "Epoch: 68 | Loss: tensor(2.8713, grad_fn=<MeanBackward0>)\n",
      "Epoch: 69 | Loss: tensor(2.8276, grad_fn=<MeanBackward0>)\n",
      "Epoch: 70 | Loss: tensor(2.7845, grad_fn=<MeanBackward0>)\n",
      "Epoch: 71 | Loss: tensor(2.7421, grad_fn=<MeanBackward0>)\n",
      "Epoch: 72 | Loss: tensor(2.7004, grad_fn=<MeanBackward0>)\n",
      "Epoch: 73 | Loss: tensor(2.6594, grad_fn=<MeanBackward0>)\n",
      "Epoch: 74 | Loss: tensor(2.6190, grad_fn=<MeanBackward0>)\n",
      "Epoch: 75 | Loss: tensor(2.5792, grad_fn=<MeanBackward0>)\n",
      "Epoch: 76 | Loss: tensor(2.5400, grad_fn=<MeanBackward0>)\n",
      "Epoch: 77 | Loss: tensor(2.5015, grad_fn=<MeanBackward0>)\n",
      "Epoch: 78 | Loss: tensor(2.4635, grad_fn=<MeanBackward0>)\n",
      "Epoch: 79 | Loss: tensor(2.4262, grad_fn=<MeanBackward0>)\n",
      "Epoch: 80 | Loss: tensor(2.3894, grad_fn=<MeanBackward0>)\n",
      "Epoch: 81 | Loss: tensor(2.3532, grad_fn=<MeanBackward0>)\n",
      "Epoch: 82 | Loss: tensor(2.3176, grad_fn=<MeanBackward0>)\n",
      "Epoch: 83 | Loss: tensor(2.2825, grad_fn=<MeanBackward0>)\n",
      "Epoch: 84 | Loss: tensor(2.2479, grad_fn=<MeanBackward0>)\n",
      "Epoch: 85 | Loss: tensor(2.2139, grad_fn=<MeanBackward0>)\n",
      "Epoch: 86 | Loss: tensor(2.1805, grad_fn=<MeanBackward0>)\n",
      "Epoch: 87 | Loss: tensor(2.1475, grad_fn=<MeanBackward0>)\n",
      "Epoch: 88 | Loss: tensor(2.1151, grad_fn=<MeanBackward0>)\n",
      "Epoch: 89 | Loss: tensor(2.0831, grad_fn=<MeanBackward0>)\n",
      "Epoch: 90 | Loss: tensor(2.0517, grad_fn=<MeanBackward0>)\n",
      "Epoch: 91 | Loss: tensor(2.0207, grad_fn=<MeanBackward0>)\n",
      "Epoch: 92 | Loss: tensor(1.9902, grad_fn=<MeanBackward0>)\n",
      "Epoch: 93 | Loss: tensor(1.9602, grad_fn=<MeanBackward0>)\n",
      "Epoch: 94 | Loss: tensor(1.9306, grad_fn=<MeanBackward0>)\n",
      "Epoch: 95 | Loss: tensor(1.9016, grad_fn=<MeanBackward0>)\n",
      "Epoch: 96 | Loss: tensor(1.8729, grad_fn=<MeanBackward0>)\n",
      "Epoch: 97 | Loss: tensor(1.8447, grad_fn=<MeanBackward0>)\n",
      "Epoch: 98 | Loss: tensor(1.8169, grad_fn=<MeanBackward0>)\n",
      "Epoch: 99 | Loss: tensor(1.7896, grad_fn=<MeanBackward0>)\n",
      "Epoch: 100 | Loss: tensor(1.7626, grad_fn=<MeanBackward0>)\n",
      "Epoch: 101 | Loss: tensor(1.7361, grad_fn=<MeanBackward0>)\n",
      "Epoch: 102 | Loss: tensor(1.7100, grad_fn=<MeanBackward0>)\n",
      "Epoch: 103 | Loss: tensor(1.6843, grad_fn=<MeanBackward0>)\n",
      "Epoch: 104 | Loss: tensor(1.6590, grad_fn=<MeanBackward0>)\n",
      "Epoch: 105 | Loss: tensor(1.6341, grad_fn=<MeanBackward0>)\n",
      "Epoch: 106 | Loss: tensor(1.6095, grad_fn=<MeanBackward0>)\n",
      "Epoch: 107 | Loss: tensor(1.5854, grad_fn=<MeanBackward0>)\n",
      "Epoch: 108 | Loss: tensor(1.5616, grad_fn=<MeanBackward0>)\n",
      "Epoch: 109 | Loss: tensor(1.5382, grad_fn=<MeanBackward0>)\n",
      "Epoch: 110 | Loss: tensor(1.5151, grad_fn=<MeanBackward0>)\n",
      "Epoch: 111 | Loss: tensor(1.4924, grad_fn=<MeanBackward0>)\n",
      "Epoch: 112 | Loss: tensor(1.4700, grad_fn=<MeanBackward0>)\n",
      "Epoch: 113 | Loss: tensor(1.4480, grad_fn=<MeanBackward0>)\n",
      "Epoch: 114 | Loss: tensor(1.4263, grad_fn=<MeanBackward0>)\n",
      "Epoch: 115 | Loss: tensor(1.4049, grad_fn=<MeanBackward0>)\n",
      "Epoch: 116 | Loss: tensor(1.3839, grad_fn=<MeanBackward0>)\n",
      "Epoch: 117 | Loss: tensor(1.3632, grad_fn=<MeanBackward0>)\n",
      "Epoch: 118 | Loss: tensor(1.3428, grad_fn=<MeanBackward0>)\n",
      "Epoch: 119 | Loss: tensor(1.3227, grad_fn=<MeanBackward0>)\n",
      "Epoch: 120 | Loss: tensor(1.3029, grad_fn=<MeanBackward0>)\n",
      "Epoch: 121 | Loss: tensor(1.2835, grad_fn=<MeanBackward0>)\n",
      "Epoch: 122 | Loss: tensor(1.2643, grad_fn=<MeanBackward0>)\n",
      "Epoch: 123 | Loss: tensor(1.2454, grad_fn=<MeanBackward0>)\n",
      "Epoch: 124 | Loss: tensor(1.2268, grad_fn=<MeanBackward0>)\n",
      "Epoch: 125 | Loss: tensor(1.2085, grad_fn=<MeanBackward0>)\n",
      "Epoch: 126 | Loss: tensor(1.1904, grad_fn=<MeanBackward0>)\n",
      "Epoch: 127 | Loss: tensor(1.1727, grad_fn=<MeanBackward0>)\n",
      "Epoch: 128 | Loss: tensor(1.1552, grad_fn=<MeanBackward0>)\n",
      "Epoch: 129 | Loss: tensor(1.1380, grad_fn=<MeanBackward0>)\n",
      "Epoch: 130 | Loss: tensor(1.1210, grad_fn=<MeanBackward0>)\n",
      "Epoch: 131 | Loss: tensor(1.1043, grad_fn=<MeanBackward0>)\n",
      "Epoch: 132 | Loss: tensor(1.0879, grad_fn=<MeanBackward0>)\n",
      "Epoch: 133 | Loss: tensor(1.0717, grad_fn=<MeanBackward0>)\n",
      "Epoch: 134 | Loss: tensor(1.0557, grad_fn=<MeanBackward0>)\n",
      "Epoch: 135 | Loss: tensor(1.0400, grad_fn=<MeanBackward0>)\n",
      "Epoch: 136 | Loss: tensor(1.0245, grad_fn=<MeanBackward0>)\n",
      "Epoch: 137 | Loss: tensor(1.0093, grad_fn=<MeanBackward0>)\n",
      "Epoch: 138 | Loss: tensor(0.9943, grad_fn=<MeanBackward0>)\n",
      "Epoch: 139 | Loss: tensor(0.9795, grad_fn=<MeanBackward0>)\n",
      "Epoch: 140 | Loss: tensor(0.9649, grad_fn=<MeanBackward0>)\n",
      "Epoch: 141 | Loss: tensor(0.9506, grad_fn=<MeanBackward0>)\n",
      "Epoch: 142 | Loss: tensor(0.9365, grad_fn=<MeanBackward0>)\n",
      "Epoch: 143 | Loss: tensor(0.9226, grad_fn=<MeanBackward0>)\n",
      "Epoch: 144 | Loss: tensor(0.9089, grad_fn=<MeanBackward0>)\n",
      "Epoch: 145 | Loss: tensor(0.8954, grad_fn=<MeanBackward0>)\n",
      "Epoch: 146 | Loss: tensor(0.8821, grad_fn=<MeanBackward0>)\n",
      "Epoch: 147 | Loss: tensor(0.8691, grad_fn=<MeanBackward0>)\n",
      "Epoch: 148 | Loss: tensor(0.8562, grad_fn=<MeanBackward0>)\n",
      "Epoch: 149 | Loss: tensor(0.8435, grad_fn=<MeanBackward0>)\n",
      "Epoch: 150 | Loss: tensor(0.8310, grad_fn=<MeanBackward0>)\n",
      "Epoch: 151 | Loss: tensor(0.8187, grad_fn=<MeanBackward0>)\n",
      "Epoch: 152 | Loss: tensor(0.8066, grad_fn=<MeanBackward0>)\n",
      "Epoch: 153 | Loss: tensor(0.7947, grad_fn=<MeanBackward0>)\n",
      "Epoch: 154 | Loss: tensor(0.7829, grad_fn=<MeanBackward0>)\n",
      "Epoch: 155 | Loss: tensor(0.7713, grad_fn=<MeanBackward0>)\n",
      "Epoch: 156 | Loss: tensor(0.7599, grad_fn=<MeanBackward0>)\n",
      "Epoch: 157 | Loss: tensor(0.7487, grad_fn=<MeanBackward0>)\n",
      "Epoch: 158 | Loss: tensor(0.7377, grad_fn=<MeanBackward0>)\n",
      "Epoch: 159 | Loss: tensor(0.7268, grad_fn=<MeanBackward0>)\n",
      "Epoch: 160 | Loss: tensor(0.7161, grad_fn=<MeanBackward0>)\n",
      "Epoch: 161 | Loss: tensor(0.7055, grad_fn=<MeanBackward0>)\n",
      "Epoch: 162 | Loss: tensor(0.6951, grad_fn=<MeanBackward0>)\n",
      "Epoch: 163 | Loss: tensor(0.6848, grad_fn=<MeanBackward0>)\n",
      "Epoch: 164 | Loss: tensor(0.6748, grad_fn=<MeanBackward0>)\n",
      "Epoch: 165 | Loss: tensor(0.6648, grad_fn=<MeanBackward0>)\n",
      "Epoch: 166 | Loss: tensor(0.6550, grad_fn=<MeanBackward0>)\n",
      "Epoch: 167 | Loss: tensor(0.6454, grad_fn=<MeanBackward0>)\n",
      "Epoch: 168 | Loss: tensor(0.6359, grad_fn=<MeanBackward0>)\n",
      "Epoch: 169 | Loss: tensor(0.6266, grad_fn=<MeanBackward0>)\n",
      "Epoch: 170 | Loss: tensor(0.6173, grad_fn=<MeanBackward0>)\n",
      "Epoch: 171 | Loss: tensor(0.6083, grad_fn=<MeanBackward0>)\n",
      "Epoch: 172 | Loss: tensor(0.5993, grad_fn=<MeanBackward0>)\n",
      "Epoch: 173 | Loss: tensor(0.5905, grad_fn=<MeanBackward0>)\n",
      "Epoch: 174 | Loss: tensor(0.5819, grad_fn=<MeanBackward0>)\n",
      "Epoch: 175 | Loss: tensor(0.5733, grad_fn=<MeanBackward0>)\n",
      "Epoch: 176 | Loss: tensor(0.5649, grad_fn=<MeanBackward0>)\n",
      "Epoch: 177 | Loss: tensor(0.5567, grad_fn=<MeanBackward0>)\n",
      "Epoch: 178 | Loss: tensor(0.5485, grad_fn=<MeanBackward0>)\n",
      "Epoch: 179 | Loss: tensor(0.5405, grad_fn=<MeanBackward0>)\n",
      "Epoch: 180 | Loss: tensor(0.5326, grad_fn=<MeanBackward0>)\n",
      "Epoch: 181 | Loss: tensor(0.5248, grad_fn=<MeanBackward0>)\n",
      "Epoch: 182 | Loss: tensor(0.5171, grad_fn=<MeanBackward0>)\n",
      "Epoch: 183 | Loss: tensor(0.5096, grad_fn=<MeanBackward0>)\n",
      "Epoch: 184 | Loss: tensor(0.5021, grad_fn=<MeanBackward0>)\n",
      "Epoch: 185 | Loss: tensor(0.4948, grad_fn=<MeanBackward0>)\n",
      "Epoch: 186 | Loss: tensor(0.4876, grad_fn=<MeanBackward0>)\n",
      "Epoch: 187 | Loss: tensor(0.4805, grad_fn=<MeanBackward0>)\n",
      "Epoch: 188 | Loss: tensor(0.4734, grad_fn=<MeanBackward0>)\n",
      "Epoch: 189 | Loss: tensor(0.4666, grad_fn=<MeanBackward0>)\n",
      "Epoch: 190 | Loss: tensor(0.4598, grad_fn=<MeanBackward0>)\n",
      "Epoch: 191 | Loss: tensor(0.4531, grad_fn=<MeanBackward0>)\n",
      "Epoch: 192 | Loss: tensor(0.4465, grad_fn=<MeanBackward0>)\n",
      "Epoch: 193 | Loss: tensor(0.4400, grad_fn=<MeanBackward0>)\n",
      "Epoch: 194 | Loss: tensor(0.4336, grad_fn=<MeanBackward0>)\n",
      "Epoch: 195 | Loss: tensor(0.4273, grad_fn=<MeanBackward0>)\n",
      "Epoch: 196 | Loss: tensor(0.4211, grad_fn=<MeanBackward0>)\n",
      "Epoch: 197 | Loss: tensor(0.4150, grad_fn=<MeanBackward0>)\n",
      "Epoch: 198 | Loss: tensor(0.4090, grad_fn=<MeanBackward0>)\n",
      "Epoch: 199 | Loss: tensor(0.4030, grad_fn=<MeanBackward0>)\n",
      "Epoch: 200 | Loss: tensor(0.3972, grad_fn=<MeanBackward0>)\n",
      "Epoch: 201 | Loss: tensor(0.3915, grad_fn=<MeanBackward0>)\n",
      "Epoch: 202 | Loss: tensor(0.3858, grad_fn=<MeanBackward0>)\n",
      "Epoch: 203 | Loss: tensor(0.3802, grad_fn=<MeanBackward0>)\n",
      "Epoch: 204 | Loss: tensor(0.3747, grad_fn=<MeanBackward0>)\n",
      "Epoch: 205 | Loss: tensor(0.3693, grad_fn=<MeanBackward0>)\n",
      "Epoch: 206 | Loss: tensor(0.3640, grad_fn=<MeanBackward0>)\n",
      "Epoch: 207 | Loss: tensor(0.3588, grad_fn=<MeanBackward0>)\n",
      "Epoch: 208 | Loss: tensor(0.3536, grad_fn=<MeanBackward0>)\n",
      "Epoch: 209 | Loss: tensor(0.3485, grad_fn=<MeanBackward0>)\n",
      "Epoch: 210 | Loss: tensor(0.3435, grad_fn=<MeanBackward0>)\n",
      "Epoch: 211 | Loss: tensor(0.3385, grad_fn=<MeanBackward0>)\n",
      "Epoch: 212 | Loss: tensor(0.3337, grad_fn=<MeanBackward0>)\n",
      "Epoch: 213 | Loss: tensor(0.3289, grad_fn=<MeanBackward0>)\n",
      "Epoch: 214 | Loss: tensor(0.3242, grad_fn=<MeanBackward0>)\n",
      "Epoch: 215 | Loss: tensor(0.3195, grad_fn=<MeanBackward0>)\n",
      "Epoch: 216 | Loss: tensor(0.3149, grad_fn=<MeanBackward0>)\n",
      "Epoch: 217 | Loss: tensor(0.3104, grad_fn=<MeanBackward0>)\n",
      "Epoch: 218 | Loss: tensor(0.3060, grad_fn=<MeanBackward0>)\n",
      "Epoch: 219 | Loss: tensor(0.3016, grad_fn=<MeanBackward0>)\n",
      "Epoch: 220 | Loss: tensor(0.2973, grad_fn=<MeanBackward0>)\n",
      "Epoch: 221 | Loss: tensor(0.2931, grad_fn=<MeanBackward0>)\n",
      "Epoch: 222 | Loss: tensor(0.2889, grad_fn=<MeanBackward0>)\n",
      "Epoch: 223 | Loss: tensor(0.2848, grad_fn=<MeanBackward0>)\n",
      "Epoch: 224 | Loss: tensor(0.2807, grad_fn=<MeanBackward0>)\n",
      "Epoch: 225 | Loss: tensor(0.2767, grad_fn=<MeanBackward0>)\n",
      "Epoch: 226 | Loss: tensor(0.2728, grad_fn=<MeanBackward0>)\n",
      "Epoch: 227 | Loss: tensor(0.2689, grad_fn=<MeanBackward0>)\n",
      "Epoch: 228 | Loss: tensor(0.2651, grad_fn=<MeanBackward0>)\n",
      "Epoch: 229 | Loss: tensor(0.2614, grad_fn=<MeanBackward0>)\n",
      "Epoch: 230 | Loss: tensor(0.2577, grad_fn=<MeanBackward0>)\n",
      "Epoch: 231 | Loss: tensor(0.2540, grad_fn=<MeanBackward0>)\n",
      "Epoch: 232 | Loss: tensor(0.2504, grad_fn=<MeanBackward0>)\n",
      "Epoch: 233 | Loss: tensor(0.2469, grad_fn=<MeanBackward0>)\n",
      "Epoch: 234 | Loss: tensor(0.2434, grad_fn=<MeanBackward0>)\n",
      "Epoch: 235 | Loss: tensor(0.2400, grad_fn=<MeanBackward0>)\n",
      "Epoch: 236 | Loss: tensor(0.2366, grad_fn=<MeanBackward0>)\n",
      "Epoch: 237 | Loss: tensor(0.2333, grad_fn=<MeanBackward0>)\n",
      "Epoch: 238 | Loss: tensor(0.2300, grad_fn=<MeanBackward0>)\n",
      "Epoch: 239 | Loss: tensor(0.2268, grad_fn=<MeanBackward0>)\n",
      "Epoch: 240 | Loss: tensor(0.2236, grad_fn=<MeanBackward0>)\n",
      "Epoch: 241 | Loss: tensor(0.2205, grad_fn=<MeanBackward0>)\n",
      "Epoch: 242 | Loss: tensor(0.2174, grad_fn=<MeanBackward0>)\n",
      "Epoch: 243 | Loss: tensor(0.2143, grad_fn=<MeanBackward0>)\n",
      "Epoch: 244 | Loss: tensor(0.2113, grad_fn=<MeanBackward0>)\n",
      "Epoch: 245 | Loss: tensor(0.2084, grad_fn=<MeanBackward0>)\n",
      "Epoch: 246 | Loss: tensor(0.2055, grad_fn=<MeanBackward0>)\n",
      "Epoch: 247 | Loss: tensor(0.2026, grad_fn=<MeanBackward0>)\n",
      "Epoch: 248 | Loss: tensor(0.1998, grad_fn=<MeanBackward0>)\n",
      "Epoch: 249 | Loss: tensor(0.1970, grad_fn=<MeanBackward0>)\n",
      "Epoch: 250 | Loss: tensor(0.1943, grad_fn=<MeanBackward0>)\n",
      "Epoch: 251 | Loss: tensor(0.1916, grad_fn=<MeanBackward0>)\n",
      "Epoch: 252 | Loss: tensor(0.1890, grad_fn=<MeanBackward0>)\n",
      "Epoch: 253 | Loss: tensor(0.1864, grad_fn=<MeanBackward0>)\n",
      "Epoch: 254 | Loss: tensor(0.1838, grad_fn=<MeanBackward0>)\n",
      "Epoch: 255 | Loss: tensor(0.1813, grad_fn=<MeanBackward0>)\n",
      "Epoch: 256 | Loss: tensor(0.1788, grad_fn=<MeanBackward0>)\n",
      "Epoch: 257 | Loss: tensor(0.1763, grad_fn=<MeanBackward0>)\n",
      "Epoch: 258 | Loss: tensor(0.1739, grad_fn=<MeanBackward0>)\n",
      "Epoch: 259 | Loss: tensor(0.1715, grad_fn=<MeanBackward0>)\n",
      "Epoch: 260 | Loss: tensor(0.1692, grad_fn=<MeanBackward0>)\n",
      "Epoch: 261 | Loss: tensor(0.1669, grad_fn=<MeanBackward0>)\n",
      "Epoch: 262 | Loss: tensor(0.1646, grad_fn=<MeanBackward0>)\n",
      "Epoch: 263 | Loss: tensor(0.1623, grad_fn=<MeanBackward0>)\n",
      "Epoch: 264 | Loss: tensor(0.1601, grad_fn=<MeanBackward0>)\n",
      "Epoch: 265 | Loss: tensor(0.1580, grad_fn=<MeanBackward0>)\n",
      "Epoch: 266 | Loss: tensor(0.1558, grad_fn=<MeanBackward0>)\n",
      "Epoch: 267 | Loss: tensor(0.1537, grad_fn=<MeanBackward0>)\n",
      "Epoch: 268 | Loss: tensor(0.1516, grad_fn=<MeanBackward0>)\n",
      "Epoch: 269 | Loss: tensor(0.1496, grad_fn=<MeanBackward0>)\n",
      "Epoch: 270 | Loss: tensor(0.1476, grad_fn=<MeanBackward0>)\n",
      "Epoch: 271 | Loss: tensor(0.1456, grad_fn=<MeanBackward0>)\n",
      "Epoch: 272 | Loss: tensor(0.1436, grad_fn=<MeanBackward0>)\n",
      "Epoch: 273 | Loss: tensor(0.1417, grad_fn=<MeanBackward0>)\n",
      "Epoch: 274 | Loss: tensor(0.1398, grad_fn=<MeanBackward0>)\n",
      "Epoch: 275 | Loss: tensor(0.1379, grad_fn=<MeanBackward0>)\n",
      "Epoch: 276 | Loss: tensor(0.1361, grad_fn=<MeanBackward0>)\n",
      "Epoch: 277 | Loss: tensor(0.1343, grad_fn=<MeanBackward0>)\n",
      "Epoch: 278 | Loss: tensor(0.1325, grad_fn=<MeanBackward0>)\n",
      "Epoch: 279 | Loss: tensor(0.1307, grad_fn=<MeanBackward0>)\n",
      "Epoch: 280 | Loss: tensor(0.1290, grad_fn=<MeanBackward0>)\n",
      "Epoch: 281 | Loss: tensor(0.1273, grad_fn=<MeanBackward0>)\n",
      "Epoch: 282 | Loss: tensor(0.1256, grad_fn=<MeanBackward0>)\n",
      "Epoch: 283 | Loss: tensor(0.1240, grad_fn=<MeanBackward0>)\n",
      "Epoch: 284 | Loss: tensor(0.1223, grad_fn=<MeanBackward0>)\n",
      "Epoch: 285 | Loss: tensor(0.1207, grad_fn=<MeanBackward0>)\n",
      "Epoch: 286 | Loss: tensor(0.1192, grad_fn=<MeanBackward0>)\n",
      "Epoch: 287 | Loss: tensor(0.1176, grad_fn=<MeanBackward0>)\n",
      "Epoch: 288 | Loss: tensor(0.1161, grad_fn=<MeanBackward0>)\n",
      "Epoch: 289 | Loss: tensor(0.1146, grad_fn=<MeanBackward0>)\n",
      "Epoch: 290 | Loss: tensor(0.1131, grad_fn=<MeanBackward0>)\n",
      "Epoch: 291 | Loss: tensor(0.1116, grad_fn=<MeanBackward0>)\n",
      "Epoch: 292 | Loss: tensor(0.1102, grad_fn=<MeanBackward0>)\n",
      "Epoch: 293 | Loss: tensor(0.1088, grad_fn=<MeanBackward0>)\n",
      "Epoch: 294 | Loss: tensor(0.1074, grad_fn=<MeanBackward0>)\n",
      "Epoch: 295 | Loss: tensor(0.1060, grad_fn=<MeanBackward0>)\n",
      "Epoch: 296 | Loss: tensor(0.1046, grad_fn=<MeanBackward0>)\n",
      "Epoch: 297 | Loss: tensor(0.1033, grad_fn=<MeanBackward0>)\n",
      "Epoch: 298 | Loss: tensor(0.1020, grad_fn=<MeanBackward0>)\n",
      "Epoch: 299 | Loss: tensor(0.1007, grad_fn=<MeanBackward0>)\n",
      "Epoch: 300 | Loss: tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "Epoch: 301 | Loss: tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "Epoch: 302 | Loss: tensor(0.0969, grad_fn=<MeanBackward0>)\n",
      "Epoch: 303 | Loss: tensor(0.0957, grad_fn=<MeanBackward0>)\n",
      "Epoch: 304 | Loss: tensor(0.0945, grad_fn=<MeanBackward0>)\n",
      "Epoch: 305 | Loss: tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "Epoch: 306 | Loss: tensor(0.0921, grad_fn=<MeanBackward0>)\n",
      "Epoch: 307 | Loss: tensor(0.0910, grad_fn=<MeanBackward0>)\n",
      "Epoch: 308 | Loss: tensor(0.0899, grad_fn=<MeanBackward0>)\n",
      "Epoch: 309 | Loss: tensor(0.0888, grad_fn=<MeanBackward0>)\n",
      "Epoch: 310 | Loss: tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "Epoch: 311 | Loss: tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "Epoch: 312 | Loss: tensor(0.0855, grad_fn=<MeanBackward0>)\n",
      "Epoch: 313 | Loss: tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "Epoch: 314 | Loss: tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "Epoch: 315 | Loss: tensor(0.0824, grad_fn=<MeanBackward0>)\n",
      "Epoch: 316 | Loss: tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "Epoch: 317 | Loss: tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "Epoch: 318 | Loss: tensor(0.0795, grad_fn=<MeanBackward0>)\n",
      "Epoch: 319 | Loss: tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "Epoch: 320 | Loss: tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "Epoch: 321 | Loss: tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "Epoch: 322 | Loss: tensor(0.0757, grad_fn=<MeanBackward0>)\n",
      "Epoch: 323 | Loss: tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "Epoch: 324 | Loss: tensor(0.0740, grad_fn=<MeanBackward0>)\n",
      "Epoch: 325 | Loss: tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "Epoch: 326 | Loss: tensor(0.0722, grad_fn=<MeanBackward0>)\n",
      "Epoch: 327 | Loss: tensor(0.0714, grad_fn=<MeanBackward0>)\n",
      "Epoch: 328 | Loss: tensor(0.0705, grad_fn=<MeanBackward0>)\n",
      "Epoch: 329 | Loss: tensor(0.0697, grad_fn=<MeanBackward0>)\n",
      "Epoch: 330 | Loss: tensor(0.0689, grad_fn=<MeanBackward0>)\n",
      "Epoch: 331 | Loss: tensor(0.0681, grad_fn=<MeanBackward0>)\n",
      "Epoch: 332 | Loss: tensor(0.0673, grad_fn=<MeanBackward0>)\n",
      "Epoch: 333 | Loss: tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "Epoch: 334 | Loss: tensor(0.0658, grad_fn=<MeanBackward0>)\n",
      "Epoch: 335 | Loss: tensor(0.0651, grad_fn=<MeanBackward0>)\n",
      "Epoch: 336 | Loss: tensor(0.0643, grad_fn=<MeanBackward0>)\n",
      "Epoch: 337 | Loss: tensor(0.0636, grad_fn=<MeanBackward0>)\n",
      "Epoch: 338 | Loss: tensor(0.0629, grad_fn=<MeanBackward0>)\n",
      "Epoch: 339 | Loss: tensor(0.0622, grad_fn=<MeanBackward0>)\n",
      "Epoch: 340 | Loss: tensor(0.0615, grad_fn=<MeanBackward0>)\n",
      "Epoch: 341 | Loss: tensor(0.0608, grad_fn=<MeanBackward0>)\n",
      "Epoch: 342 | Loss: tensor(0.0601, grad_fn=<MeanBackward0>)\n",
      "Epoch: 343 | Loss: tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "Epoch: 344 | Loss: tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "Epoch: 345 | Loss: tensor(0.0582, grad_fn=<MeanBackward0>)\n",
      "Epoch: 346 | Loss: tensor(0.0576, grad_fn=<MeanBackward0>)\n",
      "Epoch: 347 | Loss: tensor(0.0569, grad_fn=<MeanBackward0>)\n",
      "Epoch: 348 | Loss: tensor(0.0563, grad_fn=<MeanBackward0>)\n",
      "Epoch: 349 | Loss: tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "Epoch: 350 | Loss: tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "Epoch: 351 | Loss: tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "Epoch: 352 | Loss: tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "Epoch: 353 | Loss: tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "Epoch: 354 | Loss: tensor(0.0528, grad_fn=<MeanBackward0>)\n",
      "Epoch: 355 | Loss: tensor(0.0523, grad_fn=<MeanBackward0>)\n",
      "Epoch: 356 | Loss: tensor(0.0517, grad_fn=<MeanBackward0>)\n",
      "Epoch: 357 | Loss: tensor(0.0512, grad_fn=<MeanBackward0>)\n",
      "Epoch: 358 | Loss: tensor(0.0507, grad_fn=<MeanBackward0>)\n",
      "Epoch: 359 | Loss: tensor(0.0502, grad_fn=<MeanBackward0>)\n",
      "Epoch: 360 | Loss: tensor(0.0497, grad_fn=<MeanBackward0>)\n",
      "Epoch: 361 | Loss: tensor(0.0492, grad_fn=<MeanBackward0>)\n",
      "Epoch: 362 | Loss: tensor(0.0487, grad_fn=<MeanBackward0>)\n",
      "Epoch: 363 | Loss: tensor(0.0482, grad_fn=<MeanBackward0>)\n",
      "Epoch: 364 | Loss: tensor(0.0477, grad_fn=<MeanBackward0>)\n",
      "Epoch: 365 | Loss: tensor(0.0472, grad_fn=<MeanBackward0>)\n",
      "Epoch: 366 | Loss: tensor(0.0468, grad_fn=<MeanBackward0>)\n",
      "Epoch: 367 | Loss: tensor(0.0463, grad_fn=<MeanBackward0>)\n",
      "Epoch: 368 | Loss: tensor(0.0459, grad_fn=<MeanBackward0>)\n",
      "Epoch: 369 | Loss: tensor(0.0454, grad_fn=<MeanBackward0>)\n",
      "Epoch: 370 | Loss: tensor(0.0450, grad_fn=<MeanBackward0>)\n",
      "Epoch: 371 | Loss: tensor(0.0445, grad_fn=<MeanBackward0>)\n",
      "Epoch: 372 | Loss: tensor(0.0441, grad_fn=<MeanBackward0>)\n",
      "Epoch: 373 | Loss: tensor(0.0437, grad_fn=<MeanBackward0>)\n",
      "Epoch: 374 | Loss: tensor(0.0433, grad_fn=<MeanBackward0>)\n",
      "Epoch: 375 | Loss: tensor(0.0429, grad_fn=<MeanBackward0>)\n",
      "Epoch: 376 | Loss: tensor(0.0425, grad_fn=<MeanBackward0>)\n",
      "Epoch: 377 | Loss: tensor(0.0421, grad_fn=<MeanBackward0>)\n",
      "Epoch: 378 | Loss: tensor(0.0417, grad_fn=<MeanBackward0>)\n",
      "Epoch: 379 | Loss: tensor(0.0413, grad_fn=<MeanBackward0>)\n",
      "Epoch: 380 | Loss: tensor(0.0410, grad_fn=<MeanBackward0>)\n",
      "Epoch: 381 | Loss: tensor(0.0406, grad_fn=<MeanBackward0>)\n",
      "Epoch: 382 | Loss: tensor(0.0402, grad_fn=<MeanBackward0>)\n",
      "Epoch: 383 | Loss: tensor(0.0399, grad_fn=<MeanBackward0>)\n",
      "Epoch: 384 | Loss: tensor(0.0395, grad_fn=<MeanBackward0>)\n",
      "Epoch: 385 | Loss: tensor(0.0392, grad_fn=<MeanBackward0>)\n",
      "Epoch: 386 | Loss: tensor(0.0388, grad_fn=<MeanBackward0>)\n",
      "Epoch: 387 | Loss: tensor(0.0385, grad_fn=<MeanBackward0>)\n",
      "Epoch: 388 | Loss: tensor(0.0382, grad_fn=<MeanBackward0>)\n",
      "Epoch: 389 | Loss: tensor(0.0378, grad_fn=<MeanBackward0>)\n",
      "Epoch: 390 | Loss: tensor(0.0375, grad_fn=<MeanBackward0>)\n",
      "Epoch: 391 | Loss: tensor(0.0372, grad_fn=<MeanBackward0>)\n",
      "Epoch: 392 | Loss: tensor(0.0369, grad_fn=<MeanBackward0>)\n",
      "Epoch: 393 | Loss: tensor(0.0366, grad_fn=<MeanBackward0>)\n",
      "Epoch: 394 | Loss: tensor(0.0363, grad_fn=<MeanBackward0>)\n",
      "Epoch: 395 | Loss: tensor(0.0360, grad_fn=<MeanBackward0>)\n",
      "Epoch: 396 | Loss: tensor(0.0357, grad_fn=<MeanBackward0>)\n",
      "Epoch: 397 | Loss: tensor(0.0354, grad_fn=<MeanBackward0>)\n",
      "Epoch: 398 | Loss: tensor(0.0351, grad_fn=<MeanBackward0>)\n",
      "Epoch: 399 | Loss: tensor(0.0349, grad_fn=<MeanBackward0>)\n",
      "Epoch: 400 | Loss: tensor(0.0346, grad_fn=<MeanBackward0>)\n",
      "Epoch: 401 | Loss: tensor(0.0343, grad_fn=<MeanBackward0>)\n",
      "Epoch: 402 | Loss: tensor(0.0340, grad_fn=<MeanBackward0>)\n",
      "Epoch: 403 | Loss: tensor(0.0338, grad_fn=<MeanBackward0>)\n",
      "Epoch: 404 | Loss: tensor(0.0335, grad_fn=<MeanBackward0>)\n",
      "Epoch: 405 | Loss: tensor(0.0333, grad_fn=<MeanBackward0>)\n",
      "Epoch: 406 | Loss: tensor(0.0330, grad_fn=<MeanBackward0>)\n",
      "Epoch: 407 | Loss: tensor(0.0328, grad_fn=<MeanBackward0>)\n",
      "Epoch: 408 | Loss: tensor(0.0325, grad_fn=<MeanBackward0>)\n",
      "Epoch: 409 | Loss: tensor(0.0323, grad_fn=<MeanBackward0>)\n",
      "Epoch: 410 | Loss: tensor(0.0321, grad_fn=<MeanBackward0>)\n",
      "Epoch: 411 | Loss: tensor(0.0318, grad_fn=<MeanBackward0>)\n",
      "Epoch: 412 | Loss: tensor(0.0316, grad_fn=<MeanBackward0>)\n",
      "Epoch: 413 | Loss: tensor(0.0314, grad_fn=<MeanBackward0>)\n",
      "Epoch: 414 | Loss: tensor(0.0311, grad_fn=<MeanBackward0>)\n",
      "Epoch: 415 | Loss: tensor(0.0309, grad_fn=<MeanBackward0>)\n",
      "Epoch: 416 | Loss: tensor(0.0307, grad_fn=<MeanBackward0>)\n",
      "Epoch: 417 | Loss: tensor(0.0305, grad_fn=<MeanBackward0>)\n",
      "Epoch: 418 | Loss: tensor(0.0303, grad_fn=<MeanBackward0>)\n",
      "Epoch: 419 | Loss: tensor(0.0301, grad_fn=<MeanBackward0>)\n",
      "Epoch: 420 | Loss: tensor(0.0299, grad_fn=<MeanBackward0>)\n",
      "Epoch: 421 | Loss: tensor(0.0297, grad_fn=<MeanBackward0>)\n",
      "Epoch: 422 | Loss: tensor(0.0295, grad_fn=<MeanBackward0>)\n",
      "Epoch: 423 | Loss: tensor(0.0293, grad_fn=<MeanBackward0>)\n",
      "Epoch: 424 | Loss: tensor(0.0291, grad_fn=<MeanBackward0>)\n",
      "Epoch: 425 | Loss: tensor(0.0289, grad_fn=<MeanBackward0>)\n",
      "Epoch: 426 | Loss: tensor(0.0288, grad_fn=<MeanBackward0>)\n",
      "Epoch: 427 | Loss: tensor(0.0286, grad_fn=<MeanBackward0>)\n",
      "Epoch: 428 | Loss: tensor(0.0284, grad_fn=<MeanBackward0>)\n",
      "Epoch: 429 | Loss: tensor(0.0282, grad_fn=<MeanBackward0>)\n",
      "Epoch: 430 | Loss: tensor(0.0280, grad_fn=<MeanBackward0>)\n",
      "Epoch: 431 | Loss: tensor(0.0279, grad_fn=<MeanBackward0>)\n",
      "Epoch: 432 | Loss: tensor(0.0277, grad_fn=<MeanBackward0>)\n",
      "Epoch: 433 | Loss: tensor(0.0275, grad_fn=<MeanBackward0>)\n",
      "Epoch: 434 | Loss: tensor(0.0274, grad_fn=<MeanBackward0>)\n",
      "Epoch: 435 | Loss: tensor(0.0272, grad_fn=<MeanBackward0>)\n",
      "Epoch: 436 | Loss: tensor(0.0271, grad_fn=<MeanBackward0>)\n",
      "Epoch: 437 | Loss: tensor(0.0269, grad_fn=<MeanBackward0>)\n",
      "Epoch: 438 | Loss: tensor(0.0268, grad_fn=<MeanBackward0>)\n",
      "Epoch: 439 | Loss: tensor(0.0266, grad_fn=<MeanBackward0>)\n",
      "Epoch: 440 | Loss: tensor(0.0265, grad_fn=<MeanBackward0>)\n",
      "Epoch: 441 | Loss: tensor(0.0263, grad_fn=<MeanBackward0>)\n",
      "Epoch: 442 | Loss: tensor(0.0262, grad_fn=<MeanBackward0>)\n",
      "Epoch: 443 | Loss: tensor(0.0260, grad_fn=<MeanBackward0>)\n",
      "Epoch: 444 | Loss: tensor(0.0259, grad_fn=<MeanBackward0>)\n",
      "Epoch: 445 | Loss: tensor(0.0258, grad_fn=<MeanBackward0>)\n",
      "Epoch: 446 | Loss: tensor(0.0256, grad_fn=<MeanBackward0>)\n",
      "Epoch: 447 | Loss: tensor(0.0255, grad_fn=<MeanBackward0>)\n",
      "Epoch: 448 | Loss: tensor(0.0254, grad_fn=<MeanBackward0>)\n",
      "Epoch: 449 | Loss: tensor(0.0252, grad_fn=<MeanBackward0>)\n",
      "Epoch: 450 | Loss: tensor(0.0251, grad_fn=<MeanBackward0>)\n",
      "Epoch: 451 | Loss: tensor(0.0250, grad_fn=<MeanBackward0>)\n",
      "Epoch: 452 | Loss: tensor(0.0249, grad_fn=<MeanBackward0>)\n",
      "Epoch: 453 | Loss: tensor(0.0248, grad_fn=<MeanBackward0>)\n",
      "Epoch: 454 | Loss: tensor(0.0246, grad_fn=<MeanBackward0>)\n",
      "Epoch: 455 | Loss: tensor(0.0245, grad_fn=<MeanBackward0>)\n",
      "Epoch: 456 | Loss: tensor(0.0244, grad_fn=<MeanBackward0>)\n",
      "Epoch: 457 | Loss: tensor(0.0243, grad_fn=<MeanBackward0>)\n",
      "Epoch: 458 | Loss: tensor(0.0242, grad_fn=<MeanBackward0>)\n",
      "Epoch: 459 | Loss: tensor(0.0241, grad_fn=<MeanBackward0>)\n",
      "Epoch: 460 | Loss: tensor(0.0240, grad_fn=<MeanBackward0>)\n",
      "Epoch: 461 | Loss: tensor(0.0239, grad_fn=<MeanBackward0>)\n",
      "Epoch: 462 | Loss: tensor(0.0238, grad_fn=<MeanBackward0>)\n",
      "Epoch: 463 | Loss: tensor(0.0237, grad_fn=<MeanBackward0>)\n",
      "Epoch: 464 | Loss: tensor(0.0236, grad_fn=<MeanBackward0>)\n",
      "Epoch: 465 | Loss: tensor(0.0235, grad_fn=<MeanBackward0>)\n",
      "Epoch: 466 | Loss: tensor(0.0234, grad_fn=<MeanBackward0>)\n",
      "Epoch: 467 | Loss: tensor(0.0233, grad_fn=<MeanBackward0>)\n",
      "Epoch: 468 | Loss: tensor(0.0232, grad_fn=<MeanBackward0>)\n",
      "Epoch: 469 | Loss: tensor(0.0231, grad_fn=<MeanBackward0>)\n",
      "Epoch: 470 | Loss: tensor(0.0230, grad_fn=<MeanBackward0>)\n",
      "Epoch: 471 | Loss: tensor(0.0229, grad_fn=<MeanBackward0>)\n",
      "Epoch: 472 | Loss: tensor(0.0228, grad_fn=<MeanBackward0>)\n",
      "Epoch: 473 | Loss: tensor(0.0227, grad_fn=<MeanBackward0>)\n",
      "Epoch: 474 | Loss: tensor(0.0226, grad_fn=<MeanBackward0>)\n",
      "Epoch: 475 | Loss: tensor(0.0225, grad_fn=<MeanBackward0>)\n",
      "Epoch: 476 | Loss: tensor(0.0225, grad_fn=<MeanBackward0>)\n",
      "Epoch: 477 | Loss: tensor(0.0224, grad_fn=<MeanBackward0>)\n",
      "Epoch: 478 | Loss: tensor(0.0223, grad_fn=<MeanBackward0>)\n",
      "Epoch: 479 | Loss: tensor(0.0222, grad_fn=<MeanBackward0>)\n",
      "Epoch: 480 | Loss: tensor(0.0221, grad_fn=<MeanBackward0>)\n",
      "Epoch: 481 | Loss: tensor(0.0221, grad_fn=<MeanBackward0>)\n",
      "Epoch: 482 | Loss: tensor(0.0220, grad_fn=<MeanBackward0>)\n",
      "Epoch: 483 | Loss: tensor(0.0219, grad_fn=<MeanBackward0>)\n",
      "Epoch: 484 | Loss: tensor(0.0218, grad_fn=<MeanBackward0>)\n",
      "Epoch: 485 | Loss: tensor(0.0218, grad_fn=<MeanBackward0>)\n",
      "Epoch: 486 | Loss: tensor(0.0217, grad_fn=<MeanBackward0>)\n",
      "Epoch: 487 | Loss: tensor(0.0216, grad_fn=<MeanBackward0>)\n",
      "Epoch: 488 | Loss: tensor(0.0216, grad_fn=<MeanBackward0>)\n",
      "Epoch: 489 | Loss: tensor(0.0215, grad_fn=<MeanBackward0>)\n",
      "Epoch: 490 | Loss: tensor(0.0214, grad_fn=<MeanBackward0>)\n",
      "Epoch: 491 | Loss: tensor(0.0214, grad_fn=<MeanBackward0>)\n",
      "Epoch: 492 | Loss: tensor(0.0213, grad_fn=<MeanBackward0>)\n",
      "Epoch: 493 | Loss: tensor(0.0212, grad_fn=<MeanBackward0>)\n",
      "Epoch: 494 | Loss: tensor(0.0212, grad_fn=<MeanBackward0>)\n",
      "Epoch: 495 | Loss: tensor(0.0211, grad_fn=<MeanBackward0>)\n",
      "Epoch: 496 | Loss: tensor(0.0210, grad_fn=<MeanBackward0>)\n",
      "Epoch: 497 | Loss: tensor(0.0210, grad_fn=<MeanBackward0>)\n",
      "Epoch: 498 | Loss: tensor(0.0209, grad_fn=<MeanBackward0>)\n",
      "Epoch: 499 | Loss: tensor(0.0209, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    train(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0142]], grad_fn=<CdistBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = embedding(torch.LongTensor([1, 33]))\n",
    "torch.cdist(embed[0].unsqueeze(0), embed[1].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0006, -0.0027, -0.0002,  0.0339],\n",
       "        [-0.0005, -0.0047,  0.0046,  0.0013],\n",
       "        [ 0.0048, -0.0040,  0.0106,  0.0042],\n",
       "        ...,\n",
       "        [-0.0330, -0.0006,  0.0033,  0.0026],\n",
       "        [ 0.0060, -0.0061,  0.0051,  0.0023],\n",
       "        [ 0.0026, -0.0093,  0.0064,  0.0060]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0],\n",
       "        [  9],\n",
       "        [ 33],\n",
       "        [129]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(target[1]).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(embedding.weight.detach().cpu(), open(\"embedding.dump\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "wt = pickle.load(open(\"embedding.dump\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
