Starting with 50 epochs and 1024 batch size.
Total configs: 448.
Total configs: 78,125.
Total configs: 27,000.
Total configs: 243.
Total configs: 2,187.
Train Datasets: ['implicit_graph_n7']
Train Dataset size: 77,076,249, Subset size: 50000
Max sequence length: 47
Spectral embedding dim: 2
CausalTransformer(
  (embedding): EmbeddingProjectionModel(
    (projection): Linear(in_features=7, out_features=8, bias=True)
  )
  (transformer): TransformerDecoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (linear1): Linear(in_features=8, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=8, bias=True)
        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_head): Linear(in_features=8, out_features=1, bias=True)
)
Total parameters: 70,969
Training set | Epoch 1 | MSE Loss: 2.0294 | Time taken: 454.0229s
Training set | Epoch 2 | MSE Loss: 0.1076 | Time taken: 460.3646s
Training set | Epoch 3 | MSE Loss: 0.0902 | Time taken: 449.3856s
Training set | Epoch 4 | MSE Loss: 0.1067 | Time taken: 406.811s
Training set | Epoch 5 | MSE Loss: 0.0899 | Time taken: 406.8706s
Training set | Epoch 6 | MSE Loss: 0.0927 | Time taken: 406.8048s
Training set | Epoch 7 | MSE Loss: 0.0892 | Time taken: 412.5976s
Training set | Epoch 8 | MSE Loss: 0.0856 | Time taken: 406.9902s
Training set | Epoch 9 | MSE Loss: 0.0875 | Time taken: 407.0132s
Training set | Epoch 10 | MSE Loss: 0.0948 | Time taken: 407.0071s
Training set | Epoch 11 | MSE Loss: 0.0861 | Time taken: 407.0019s
Training set | Epoch 12 | MSE Loss: 0.1016 | Time taken: 407.0764s
Training set | Epoch 13 | MSE Loss: 0.1008 | Time taken: 407.0979s
Training set | Epoch 14 | MSE Loss: 0.1123 | Time taken: 406.9834s
Training set | Epoch 15 | MSE Loss: 0.1074 | Time taken: 407.0462s
Training set | Epoch 16 | MSE Loss: 0.1209 | Time taken: 407.0873s
Training set | Epoch 17 | MSE Loss: 0.1217 | Time taken: 407.0089s
Training set | Epoch 18 | MSE Loss: 0.1237 | Time taken: 407.0082s
Training set | Epoch 19 | MSE Loss: 0.1205 | Time taken: 407.0352s
Training set | Epoch 20 | MSE Loss: 0.1187 | Time taken: 416.4642s
Training set | Epoch 21 | MSE Loss: 0.1268 | Time taken: 430.3616s
Training set | Epoch 22 | MSE Loss: 0.1193 | Time taken: 406.8868s
Training set | Epoch 23 | MSE Loss: 0.1404 | Time taken: 410.17s
Training set | Epoch 24 | MSE Loss: 0.1404 | Time taken: 406.8861s
Training set | Epoch 25 | MSE Loss: 0.1463 | Time taken: 406.8797s
Training set | Epoch 26 | MSE Loss: 0.1435 | Time taken: 406.8747s
Training set | Epoch 27 | MSE Loss: 0.1368 | Time taken: 406.8269s
Training set | Epoch 28 | MSE Loss: 0.1566 | Time taken: 406.8136s
Training set | Epoch 29 | MSE Loss: 0.1689 | Time taken: 406.869s
Training set | Epoch 30 | MSE Loss: 0.1936 | Time taken: 406.8669s
Training set | Epoch 31 | MSE Loss: 0.1942 | Time taken: 406.7585s
Training set | Epoch 32 | MSE Loss: 0.1888 | Time taken: 446.4679s
Training set | Epoch 33 | MSE Loss: 0.2129 | Time taken: 457.3686s
Training set | Epoch 34 | MSE Loss: 0.1719 | Time taken: 421.4762s
Training set | Epoch 35 | MSE Loss: 0.2212 | Time taken: 407.6224s
Training set | Epoch 36 | MSE Loss: 0.2053 | Time taken: 455.2192s
Training set | Epoch 37 | MSE Loss: 0.1865 | Time taken: 457.8441s
Training set | Epoch 38 | MSE Loss: 0.249 | Time taken: 458.0901s
Training set | Epoch 39 | MSE Loss: 0.199 | Time taken: 445.7514s
Training set | Epoch 40 | MSE Loss: 0.1924 | Time taken: 436.6664s
Training set | Epoch 41 | MSE Loss: 0.1834 | Time taken: 407.1746s
Training set | Epoch 42 | MSE Loss: 0.1869 | Time taken: 407.1542s
Training set | Epoch 43 | MSE Loss: 0.1925 | Time taken: 407.2888s
Training set | Epoch 44 | MSE Loss: 0.2032 | Time taken: 407.1634s
Training set | Epoch 45 | MSE Loss: 0.2106 | Time taken: 407.2466s
Training set | Epoch 46 | MSE Loss: 0.2326 | Time taken: 407.1882s
Training set | Epoch 47 | MSE Loss: 0.2444 | Time taken: 407.2468s
Training set | Epoch 48 | MSE Loss: 0.2465 | Time taken: 407.1161s
Training set | Epoch 49 | MSE Loss: 0.35 | Time taken: 407.1875s
Training set | Epoch 50 | MSE Loss: 0.5064 | Time taken: 407.2124s


End Training | Total training time taken 20862.0175s


Saving model trained_models/transformer_trained_at_2025_04_28_21_27.pt
Done!
./nohup_commands.sh: line 100: cd: gnn: No such file or directory
