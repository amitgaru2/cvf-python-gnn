Starting with 50 epochs and 1024 batch size.
Total configs: 448.
Total configs: 78,125.
Total configs: 27,000.
Total configs: 243.
Total configs: 2,187.
Train Datasets: ['implicit_graph_n7']
Train Dataset size: 77,076,249, Subset size: 100000
Max sequence length: 47
Spectral embedding dim: 2
CausalTransformer(
  (embedding): EmbeddingProjectionModel(
    (projection): Linear(in_features=7, out_features=8, bias=True)
  )
  (transformer): TransformerDecoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (linear1): Linear(in_features=8, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=8, bias=True)
        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_head): Linear(in_features=8, out_features=1, bias=True)
)
Total parameters: 70,969
Training set | Epoch 1 | MSE Loss: 1.195 | Time taken: 916.9388s
Training set | Epoch 2 | MSE Loss: 0.1682 | Time taken: 923.7766s
Training set | Epoch 3 | MSE Loss: 0.1572 | Time taken: 905.6413s
Training set | Epoch 4 | MSE Loss: 0.1694 | Time taken: 852.6173s
Training set | Epoch 5 | MSE Loss: 0.1653 | Time taken: 826.8184s
Training set | Epoch 6 | MSE Loss: 0.1533 | Time taken: 823.9167s
Training set | Epoch 7 | MSE Loss: 0.1489 | Time taken: 864.3476s
Training set | Epoch 8 | MSE Loss: 0.1402 | Time taken: 825.9644s
Training set | Epoch 9 | MSE Loss: 0.135 | Time taken: 921.9992s
Training set | Epoch 10 | MSE Loss: 0.1423 | Time taken: 920.0994s
Training set | Epoch 11 | MSE Loss: 0.1409 | Time taken: 919.3458s
Training set | Epoch 12 | MSE Loss: 0.1376 | Time taken: 824.5071s
Training set | Epoch 13 | MSE Loss: 0.1517 | Time taken: 837.2013s
Training set | Epoch 14 | MSE Loss: 0.168 | Time taken: 922.1101s
Training set | Epoch 15 | MSE Loss: 0.1795 | Time taken: 918.4797s
Training set | Epoch 16 | MSE Loss: 0.193 | Time taken: 817.7704s
Training set | Epoch 17 | MSE Loss: 0.186 | Time taken: 819.0889s
Training set | Epoch 18 | MSE Loss: 0.1798 | Time taken: 821.9692s
Training set | Epoch 19 | MSE Loss: 0.1831 | Time taken: 822.4495s
Training set | Epoch 20 | MSE Loss: 0.1678 | Time taken: 821.1336s
Training set | Epoch 21 | MSE Loss: 0.1556 | Time taken: 818.6641s
Training set | Epoch 22 | MSE Loss: 0.153 | Time taken: 817.4834s
Training set | Epoch 23 | MSE Loss: 0.1502 | Time taken: 818.7302s
Training set | Epoch 24 | MSE Loss: 0.1533 | Time taken: 817.5701s
Training set | Epoch 25 | MSE Loss: 0.1476 | Time taken: 818.3043s
Training set | Epoch 26 | MSE Loss: 0.1473 | Time taken: 819.1563s
Training set | Epoch 27 | MSE Loss: 0.1455 | Time taken: 821.9924s
Training set | Epoch 28 | MSE Loss: 0.1416 | Time taken: 816.3282s
Training set | Epoch 29 | MSE Loss: 0.1462 | Time taken: 820.4471s
Training set | Epoch 30 | MSE Loss: 0.1435 | Time taken: 819.5601s
Training set | Epoch 31 | MSE Loss: 0.1399 | Time taken: 838.8155s
Training set | Epoch 32 | MSE Loss: 0.1402 | Time taken: 920.1346s
Training set | Epoch 33 | MSE Loss: 0.134 | Time taken: 899.4759s
Training set | Epoch 34 | MSE Loss: 0.1294 | Time taken: 821.7039s
Training set | Epoch 35 | MSE Loss: 0.1311 | Time taken: 824.7029s
Training set | Epoch 36 | MSE Loss: 0.1341 | Time taken: 827.32s
Training set | Epoch 37 | MSE Loss: 0.1401 | Time taken: 826.4307s
Training set | Epoch 38 | MSE Loss: 0.1385 | Time taken: 825.1196s
Training set | Epoch 39 | MSE Loss: 0.1484 | Time taken: 916.3328s
Training set | Epoch 40 | MSE Loss: 0.1323 | Time taken: 913.9743s
Training set | Epoch 41 | MSE Loss: 0.1671 | Time taken: 908.0743s
Training set | Epoch 42 | MSE Loss: 0.1555 | Time taken: 894.5346s
Training set | Epoch 43 | MSE Loss: 0.1461 | Time taken: 814.7174s
Training set | Epoch 44 | MSE Loss: 0.1463 | Time taken: 814.3221s
Training set | Epoch 45 | MSE Loss: 0.1485 | Time taken: 815.8065s
Training set | Epoch 46 | MSE Loss: 0.1579 | Time taken: 814.2498s
Training set | Epoch 47 | MSE Loss: 0.1626 | Time taken: 814.0203s
Training set | Epoch 48 | MSE Loss: 0.1619 | Time taken: 813.9793s
Training set | Epoch 49 | MSE Loss: 0.1715 | Time taken: 813.1075s
Training set | Epoch 50 | MSE Loss: 0.1544 | Time taken: 813.0686s


End Training | Total training time taken 42431.0594s


Saving model trained_models/transformer_trained_at_2025_04_29_10_06.pt
Done!
